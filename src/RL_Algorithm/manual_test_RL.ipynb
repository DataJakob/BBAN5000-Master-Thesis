{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from ThesisEnvironment  import PortfolioEnvironment as PorEnv\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../Data/StockPrices.csv\")\n",
    "\n",
    "stock_data_train, stock_data_test = train_test_split(\n",
    "    df, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "esg_scores = [36.6, 35.3, 17.9, 18, \n",
    "              18, 21.2, 18.7, 20,\n",
    "              19.8, 13.8, 18.1, 19, \n",
    "              17.2, 14, 17.2, 19.5, \n",
    "              19.7, 21.2, 26.8, 19.3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = PorEnv(stock_data_train, esg_scores, max_steps=100, window_size=10, esg_threshold=27)\n",
    "train_env = DummyVecEnv([lambda: train_env])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jakob\\OneDrive\\Dokumenter\\Masteroppgave\\MyVenv\\Lib\\site-packages\\stable_baselines3\\common\\buffers.py:242: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 1.69GB > 0.74GB\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the SAC model\n",
    "model = SAC(\n",
    "    policy=\"MlpPolicy\",     # Policy type\n",
    "    policy_kwargs=dict(net_arch=[64, 64]),  # Smaller network\n",
    "    env=train_env,                # Environment\n",
    "    verbose=1,              # Printing\n",
    "    learning_rate=3e-4,     # Learning rate\n",
    "    buffer_size=1000000,    # Memory usage\n",
    "    batch_size=64,         # Batch size for training  (higher= stable updates and exploitation, and vice versa)\n",
    "    ent_coef='auto',        # Entropy coefficient (higher=more exploration, and vice versa)\n",
    "    gamma=0.99,             # Discount factor (time value of older rewards/observations)\n",
    "    tau=0.005,              # Target network update rate\n",
    "    train_freq=1,           # Train every step (higher=policy update frequency and exploitation, and vice versa)\n",
    "    gradient_steps=1,  # Gradient steps per update\n",
    "    seed=42  # Random seed for reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 51       |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 400      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -25.8    |\n",
      "|    critic_loss     | 0.45     |\n",
      "|    ent_coef        | 0.925    |\n",
      "|    ent_coef_loss   | -2.6     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 299      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 45       |\n",
      "|    time_elapsed    | 17       |\n",
      "|    total_timesteps | 800      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -43.3    |\n",
      "|    critic_loss     | 0.267    |\n",
      "|    ent_coef        | 0.82     |\n",
      "|    ent_coef_loss   | -6.65    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 699      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 42       |\n",
      "|    time_elapsed    | 28       |\n",
      "|    total_timesteps | 1200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -59.6    |\n",
      "|    critic_loss     | 0.271    |\n",
      "|    ent_coef        | 0.727    |\n",
      "|    ent_coef_loss   | -10.7    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1099     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 41       |\n",
      "|    time_elapsed    | 38       |\n",
      "|    total_timesteps | 1600     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -74.3    |\n",
      "|    critic_loss     | 0.158    |\n",
      "|    ent_coef        | 0.645    |\n",
      "|    ent_coef_loss   | -14.8    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1499     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 49       |\n",
      "|    total_timesteps | 2000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -87      |\n",
      "|    critic_loss     | 0.191    |\n",
      "|    ent_coef        | 0.572    |\n",
      "|    ent_coef_loss   | -18.9    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 59       |\n",
      "|    total_timesteps | 2400     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -97.7    |\n",
      "|    critic_loss     | 0.169    |\n",
      "|    ent_coef        | 0.507    |\n",
      "|    ent_coef_loss   | -22.8    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2299     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 70       |\n",
      "|    total_timesteps | 2800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -107     |\n",
      "|    critic_loss     | 0.0808   |\n",
      "|    ent_coef        | 0.45     |\n",
      "|    ent_coef_loss   | -26.9    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2699     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 81       |\n",
      "|    total_timesteps | 3200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -115     |\n",
      "|    critic_loss     | 0.095    |\n",
      "|    ent_coef        | 0.399    |\n",
      "|    ent_coef_loss   | -30.9    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3099     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 92       |\n",
      "|    total_timesteps | 3600     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -122     |\n",
      "|    critic_loss     | 0.0455   |\n",
      "|    ent_coef        | 0.354    |\n",
      "|    ent_coef_loss   | -34.9    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3499     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 102      |\n",
      "|    total_timesteps | 4000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -128     |\n",
      "|    critic_loss     | 0.0447   |\n",
      "|    ent_coef        | 0.314    |\n",
      "|    ent_coef_loss   | -39.2    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 38       |\n",
      "|    time_elapsed    | 112      |\n",
      "|    total_timesteps | 4400     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -133     |\n",
      "|    critic_loss     | 0.0499   |\n",
      "|    ent_coef        | 0.278    |\n",
      "|    ent_coef_loss   | -43      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4299     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 38       |\n",
      "|    time_elapsed    | 124      |\n",
      "|    total_timesteps | 4800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -136     |\n",
      "|    critic_loss     | 0.0581   |\n",
      "|    ent_coef        | 0.247    |\n",
      "|    ent_coef_loss   | -47      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4699     |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.learn(total_timesteps=5000)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"sac_portfolio_management\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = PorEnv(stock_data_test, esg_scores, max_steps=100, window_size=10, esg_threshold=27)\n",
    "test_env = DummyVecEnv([lambda: test_env])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the testing environment\n",
    "# obs = test_env.reset()\n",
    "\n",
    "# # Create a list to store the weights and portfolio values\n",
    "# weights_history = []\n",
    "# portfolio_values = []\n",
    "\n",
    "# # Run the testing loop\n",
    "# for _ in range(len(stock_data_test) - 1):  # Adjust for test data length\n",
    "#     # Predict the action (portfolio weights) using the trained model\n",
    "#     action, _states = model.predict(obs, deterministic=True)\n",
    "    \n",
    "#     # Normalize the action to ensure weights sum to 1\n",
    "#     normalized_action = np.clip(action, 0, 1).astype(\"float64\")  # Clip to [0, 1]\n",
    "#     normalized_action /= np.sum(normalized_action)  # Normalize to sum to 1\n",
    "    \n",
    "#     # Execute the action in the testing environment\n",
    "#     obs, rewards, dones, info = test_env.step(normalized_action)\n",
    "    \n",
    "#     # Store the normalized weights and portfolio value\n",
    "#     weights_history.append(np.squeeze(normalized_action))  # Remove the extra dimension\n",
    "#     portfolio_values.append(test_env.envs[0].cash)  # Access the cash value from the environment\n",
    "    \n",
    "#     # Render the environment (optional)\n",
    "#     test_env.render()\n",
    "    \n",
    "#     # Reset the environment if the episode is done\n",
    "#     if dones:\n",
    "#         obs = test_env.reset()\n",
    "\n",
    "# # Convert the weights history to a DataFrame\n",
    "# weights_df = pd.DataFrame(weights_history, columns=[f\"Stock_{i+1}\" for i in range(test_env.envs[0].num_stocks)])\n",
    "\n",
    "# # Add the portfolio values to the DataFrame\n",
    "# # weights_df[\"Portfolio_Value\"] = portfolio_values\n",
    "\n",
    "# # Verify that the weights sum to 1\n",
    "# # weights_df[\"Sum_of_Weights\"] = weights_df[[f\"Stock_{i+1}\" for i in range(test_env.envs[0].num_stocks)]].sum(axis=1)\n",
    "# # print(weights_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights_df.to_csv(\"../../Data/RL_weights.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
