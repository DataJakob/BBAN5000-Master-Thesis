{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Custom Environment </h1>\n",
    "<h4> -An attempt at creating it from scratch </h4>\n",
    "<i> \"You either die a hero, or live long enough to see yourself become a villain.\"</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "class\n",
    "<br>\n",
    "initializer\n",
    "<br>\n",
    "step\n",
    "<br>\n",
    "reset\n",
    "<br>\n",
    "render\n",
    "<br>\n",
    "close\n",
    "<br>\n",
    "<br>\n",
    "Validate thorugh check_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/master/5_custom_gym_env.ipynb#scrollTo=9DOpP_B0-LXm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "from RewardFunctions import (\n",
    "    sharpe_ratio,\n",
    "    sortino_ratio,\n",
    "    calculate_drawdown,\n",
    "    sterling_ratio,\n",
    "    return_ratio,\n",
    "    penalise_reward\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_usage = 30\n",
    "rolling_reward_window = 10\n",
    "\n",
    "return_data = pd.read_csv(\"../../Data/StockReturns.csv\")\n",
    "\n",
    "esg_data = np.array([36.6, 35.3, 17.9, 18, \n",
    "                    18, 21.2, 18.7, 29.2, \n",
    "                    15.7, 25.6, 25.6, 18.4, \n",
    "                    19.8, 13.8, 18.1, 19, \n",
    "                    17.2, 14, 17.2, 19.5, \n",
    "                    19.7, 21.2, 26.8, 19.3])\n",
    "\n",
    "objective = \"Sterling\"\n",
    "\n",
    "esg_compliancy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_size = 0.8\n",
    "\n",
    "train_data = return_data.iloc[:int(split_size*len(return_data))]\n",
    "test_data = return_data.iloc[int(split_size*len(return_data)):].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PortfolioEnvironment(gym.Env):\n",
    "    \"\"\"\n",
    "    doc string\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 history_usage, rolling_reward_window,\n",
    "                 return_data, esg_data,\n",
    "                 objective, esg_compliancy):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        doc  string,\n",
    "\n",
    "        Good, initialize all variables with values \n",
    "        \"\"\"\n",
    "        self.return_data = return_data.values\n",
    "        self.esg_data: np.array = esg_data\n",
    "        self.history_usage: int = history_usage\n",
    "        self.rolling_reward_window: int = rolling_reward_window\n",
    "        self.n_stocks = len(esg_data)\n",
    "\n",
    "        self.objective: str = objective\n",
    "        self.esg_compliancy: bool = esg_compliancy\n",
    "\n",
    "        self.action_space = spaces.Box(low=-1, \n",
    "                                       high=1, \n",
    "                                       shape=(self.n_stocks,),)\n",
    "        self.observation_space = spaces.Box(low=-np.inf, \n",
    "                                            high=np.inf, \n",
    "                                            shape=(self.n_stocks * self.history_usage,))\n",
    "\n",
    "        self.current_step: int = 0\n",
    "        self.weights_list: list = []\n",
    "        self.returns_list: list = []\n",
    "        \n",
    "\n",
    "\n",
    "    def reset(self, seed=42):\n",
    "        \"\"\"\n",
    "        doc string\n",
    "\n",
    "        Good, changing all non-fixed variables inside the environment\n",
    "        \"\"\"\n",
    "        super().reset(seed=None)\n",
    "\n",
    "        self.current_step = 0\n",
    "        self.weights = []\n",
    "        self.portfolio_returns = []\n",
    "\n",
    "        observation = self.get_observation()\n",
    "        additional_info = {\n",
    "            \"time_step\": self.current_step,\n",
    "            \"cumulative_geo_return\": np.cumprod(self.portfolio_returns)\n",
    "        }\n",
    "\n",
    "        return observation, additional_info\n",
    "\n",
    "\n",
    "\n",
    "    def get_observation(self):\n",
    "        \"\"\"\n",
    "        doc string\n",
    "        \"\"\"\n",
    "        start_idx = max(0, self.current_step -self.history_usage)\n",
    "        end_idx = self.current_step\n",
    "        observation_space  = self.return_data[start_idx:end_idx].T\n",
    "        if observation_space.shape[1] < self.history_usage:\n",
    "            padding = np.zeros((self.n_stocks, self.history_usage - observation_space.shape[1]))\n",
    "            observation_space = np.hstack([padding, observation_space])\n",
    "        \n",
    "        return observation_space.flatten().astype(np.float32)\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        doc string\n",
    "        \"\"\"\n",
    "        # Generate weights based on actions\n",
    "        # Forces action from in range (-1,1) to become (0,1)\n",
    "        current_weights = (action + 1) / 2                          \n",
    "        current_weights = (current_weights+1e-8) / (np.sum(current_weights)+1e-8)\n",
    "        self.weights_list.append(current_weights)\n",
    "        \n",
    "        # Find current weights and multiply with weights\n",
    "        # Variables for (early) stopping\n",
    "        terminated = self.current_step >= len(self.return_data)-1\n",
    "        truncated = False\n",
    "\n",
    "        # Add return if possible, (edge case if-statement)\n",
    "        if not terminated:\n",
    "            current_returns = self.return_data[self.current_step +1]\n",
    "            portfolio_return = 0.0\n",
    "            if self.current_step +1 < len(self.return_data):\n",
    "                portfolio_return = np.dot(current_weights, current_returns)\n",
    "            self.returns_list.append(portfolio_return)\n",
    "        else:\n",
    "            portfolio_return = 0.0\n",
    "            self.returns_list.append(portfolio_return)\n",
    "\n",
    "        #Calculate ESG score for portfolio\n",
    "        esg_score = np.dot(current_weights, self.esg_data)\n",
    "\n",
    "        # Define rolling window for reward\n",
    "        if len(self.returns_list) < self.rolling_reward_window:\n",
    "            current_reward = np.array(self.returns_list)\n",
    "        else:\n",
    "            current_reward = np.array(self.returns_list[-self.rolling_reward_window:])\n",
    "\n",
    "        # Calcualte reward based on objective\n",
    "        if self.objective == \"Return\":\n",
    "            new_reward = return_ratio(current_reward)\n",
    "        elif self.objective == \"Sharpe\":\n",
    "            new_reward = sharpe_ratio(current_reward)\n",
    "        elif self.objective == \"Sortino\":\n",
    "            new_reward = sortino_ratio(current_reward)\n",
    "        else:\n",
    "            new_reward = sterling_ratio(current_reward)\n",
    "        \n",
    "        # Add ESG penalty\n",
    "        if esg_compliancy == True:\n",
    "            new_reward = penalise_reward(new_reward, esg_score)\n",
    "    \n",
    "        # New step\n",
    "        self.current_step += 1\n",
    "            \n",
    "        # Returns the next observation space for the algo to use\n",
    "        next_window = self.get_observation()\n",
    "\n",
    "        return next_window, new_reward, terminated, truncated, {}\n",
    "        \n",
    "\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        \"\"\" \n",
    "        doc string\n",
    "        \"\"\"\n",
    "        print(f\"Current step: {self.current_step}, and geometric return: {np.cumprod(self.portfolio_returns)}\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = PortfolioEnvironment(history_usage=history_usage, \n",
    "                                 rolling_reward_window=rolling_reward_window,\n",
    "                                 return_data=train_data,\n",
    "                                 esg_data=esg_data,\n",
    "                                 objective=objective,\n",
    "                                 esg_compliancy=esg_compliancy)\n",
    "check_env(train_env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jakob\\OneDrive\\Dokumenter\\Masteroppgave\\MyVenv\\Lib\\site-packages\\stable_baselines3\\common\\buffers.py:242: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 0.59GB > 0.50GB\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = SAC(policy=\"MlpPolicy\",\n",
    "            env=train_env,\n",
    "            buffer_size=100_000,\n",
    "            verbose=1\n",
    "            ).learn(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = PortfolioEnvironment(history_usage=history_usage, \n",
    "                                rolling_reward_window=rolling_reward_window,\n",
    "                              return_data=test_data,\n",
    "                              esg_data=esg_data,\n",
    "                              objective=objective,\n",
    "                              esg_compliancy=esg_compliancy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([np.repeat([3],4)]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = test_env.reset()\n",
    "weights_history = []\n",
    "finished = False\n",
    "\n",
    "while not finished: \n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "\n",
    "    weights = (action+1) / 2\n",
    "    weights /= np.sum(weights)\n",
    "\n",
    "    obs, reward, terminated, truncated, info = test_env.step(action)\n",
    "    finished = terminated or truncated\n",
    "\n",
    "    weights_history.append(action)\n",
    "    \n",
    "df = pd.DataFrame(weights_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_reward, std_reward = evaluate_policy(model, train_env, n_eval_episodes=10)\n",
    "# print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_reward, std_reward = evaluate_policy(model, test_env, n_eval_episodes=10)\n",
    "# print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
