{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Main.ipynb </h1>\n",
    "<h2> 1. Import libraries and modules </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from src.Data_Retriever import DataRetriever as DatRet\n",
    "\n",
    "from src.Optimization.Markowitz_PT import MarkowitzPT as MPT\n",
    "\n",
    "from src.Optimization.Environment import PortfolioEnvironment as PorEnv\n",
    "from src.Optimization.RLModelCompilation import RL_Model as RLM\n",
    "# from src.Optimization.NeuralNet import CustomCNNExtractor \n",
    "\n",
    "from src.Result.Menchero_OGA import MencheroOGA as MOGA\n",
    "from src.Result.IndPortResults import GenerateResult as GR\n",
    "from src.Result.OverviewResults import ResultConveyor as RC\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 2. Define operating variables </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trading_n = 400\n",
    "history_usage = 504\n",
    "n_sectors = 6\n",
    "n_stocks_per_sector = 4\n",
    "\n",
    "# For RL algorithm\n",
    "history_usage_RL = 30\n",
    "rolling_reward_window = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3. Define stock dataframe </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Petroleum</th>\n",
       "      <th>Seafood (food)</th>\n",
       "      <th>Materials</th>\n",
       "      <th>Technologies</th>\n",
       "      <th>Financial</th>\n",
       "      <th>Shipping</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EQNR.OL</td>\n",
       "      <td>ORK.OL</td>\n",
       "      <td>NHY.OL</td>\n",
       "      <td>TEL.OL</td>\n",
       "      <td>STB.OL</td>\n",
       "      <td>WAWI.OL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AKRBP.OL</td>\n",
       "      <td>MOWI.OL</td>\n",
       "      <td>YAR.OL</td>\n",
       "      <td>NOD.OL</td>\n",
       "      <td>DNB.OL</td>\n",
       "      <td>SNI.OL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SUBC.OL</td>\n",
       "      <td>SALM.OL</td>\n",
       "      <td>RECSI.OL</td>\n",
       "      <td>ATEA.OL</td>\n",
       "      <td>GJF.OL</td>\n",
       "      <td>BELCO.OL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BWO.OL</td>\n",
       "      <td>LSG.OL</td>\n",
       "      <td>BRG.OL</td>\n",
       "      <td>BOUV.OL</td>\n",
       "      <td>AKER.OL</td>\n",
       "      <td>ODF.OL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Petroleum Seafood (food) Materials Technologies Financial  Shipping\n",
       "0   EQNR.OL         ORK.OL    NHY.OL       TEL.OL    STB.OL   WAWI.OL\n",
       "1  AKRBP.OL        MOWI.OL    YAR.OL       NOD.OL    DNB.OL    SNI.OL\n",
       "2   SUBC.OL        SALM.OL  RECSI.OL      ATEA.OL    GJF.OL  BELCO.OL\n",
       "3    BWO.OL         LSG.OL    BRG.OL      BOUV.OL   AKER.OL    ODF.OL"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker_df =  pd.DataFrame()\n",
    "ticker_df[\"Petroleum\"] = [\"EQNR.OL\", \"AKRBP.OL\", \"SUBC.OL\", \"BWO.OL\",]\n",
    "ticker_df[\"Seafood (food)\"] = [\"ORK.OL\", \"MOWI.OL\", \"SALM.OL\", \"LSG.OL\"]\n",
    "ticker_df[\"Materials\"] = [\"NHY.OL\", \"YAR.OL\", \"RECSI.OL\", \"BRG.OL\"]  #del this\n",
    "ticker_df[\"Technologies\"] = [\"TEL.OL\", \"NOD.OL\", \"ATEA.OL\", \"BOUV.OL\"]\n",
    "ticker_df[\"Financial\"] = [\"STB.OL\", \"DNB.OL\", \"GJF.OL\", \"AKER.OL\"]\n",
    "ticker_df[\"Shipping\"] = [\"WAWI.OL\", \"SNI.OL\", \"BELCO.OL\", \"ODF.OL\"]\n",
    "ticker_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 4. Define ESG array </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "esg_scores = np.array([36.6, 35.3, 17.9, 18, \n",
    "                18, 21.2, 18.7, 29.2, \n",
    "                15.7, 25.6, 25.6, 18.4, # Del this\n",
    "                19.8, 13.8, 18.1, 19, \n",
    "                17.2, 14, 17.2, 19.5, \n",
    "                19.7, 21.2, 26.8, 19.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 5.-7. (Non-necessary for reproducability) </h2>\n",
    "<h3>Data retrieval</h3>\n",
    "<h3>Benchmark optimization (MPT)</h3>\n",
    "<h3>RL optimization x4</h3>\n",
    "<br>\n",
    "All of the data has been generated an stored in csvs. Therefore, I can comment out the code lines below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve data from yf API: y-m-d\n",
    "# data = DatRet(ticker_df, \"2013-01-01\", \"2024-12-31\", history_usage_RL=history_usage_RL)\n",
    "# # In function below, set log=True to check for data availability\n",
    "# data.retrieve_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate benchmark weights thorugh MPT using Sharpe ratio\n",
    "# # benchmark = MPT(history_usage, trading_n)\n",
    "# IMPORTANT: In order to see  the effect of the weights, algo exclude last observation from optimization\n",
    "# # benchmark.frequency_optimizing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objectives = [\"Return\", \"Sharpe\", \"Sortino\", \"Sterling\", \"Return\", \"Sharpe\", \"Sortino\", \"Sterling\"]\n",
    "# esg_compliancy = [True, True, True, True, False, False, False, False]\n",
    "# # objectives = [\"Sharpe\"]\n",
    "# # esg_compliancy = [True]\n",
    "\n",
    "# for i in range(len(objectives)):\n",
    "#     reinforcement = RLM(esg_scores, \n",
    "#                         objective=objectives[i],\n",
    "#                         history_usage=history_usage_RL,\n",
    "#                         rolling_reward_window=rolling_reward_window,\n",
    "#                         total_timesteps=200,\n",
    "#                         esg_compliancy=esg_compliancy[i],\n",
    "#                         )\n",
    "#     reinforcement.train_model()\n",
    "#     reinforcement.test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 8. Attribution analysis </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths = [\"Return_esg_True\", \"Sharpe_esg_True\",\n",
    "#          \"Sortino_esg_True\",\"Sterling_esg_True\",\n",
    "#          \"Return_esg_False\", \"Sharpe_esg_False\",\n",
    "#          \"Sortino_esg_False\",\"Sterling_esg_False\",]\n",
    "\n",
    "# analysis_list = []\n",
    "# for i in range(len(paths)):\n",
    "#     att_anal = GR(paths[i],\n",
    "#             n_sectors, n_stocks_per_sector,\n",
    "#             trading_n,\n",
    "#             esg_scores, \n",
    "#             ticker_df.columns)\n",
    "#     att_anal.friple_frequency_analysis()\n",
    "#     analysis_list.append(att_anal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 9. Overview  Result Analysis </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta = RC(analysis_list, trading_n)\n",
    "# theta.convey_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 10. Ad-hoc </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def make_env():\n",
    "    env = PorEnv(\n",
    "        history_usage=history_usage,\n",
    "        rolling_reward_window=rolling_reward_window,\n",
    "        esg_data=esg_scores,\n",
    "        objective=\"Sharpe\",\n",
    "        esg_compliancy=True\n",
    "    )\n",
    "    return env\n",
    "\n",
    "train_env = make_env()\n",
    "test_env = make_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import SAC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Step: 1 , Reward:  0.0 weights:  [0.9552399  0.00384995 0.14403522 0.46327478 0.8451614  0.17907059\n",
      " 0.21369854 0.34961334 0.2280615  0.0921357  0.07911462 0.5543798\n",
      " 0.7159076  0.32888183 0.46296707 0.735986   0.39601135 0.44419986\n",
      " 0.69159526 0.39248922 0.4227418  0.8968225  0.9965699  0.6917461 ]\n",
      "Step: 2 , Reward:  0.0 weights:  [0.20091751 0.8797969  0.11750087 0.25350788 0.76681113 0.9009633\n",
      " 0.84025526 0.9859346  0.34709087 0.14263272 0.85805655 0.465418\n",
      " 0.4252402  0.46136168 0.7400216  0.96971583 0.08947447 0.34694868\n",
      " 0.87461495 0.2284618  0.2691505  0.01231354 0.61148566 0.69283307]\n",
      "Step: 3 , Reward:  1.4120364347152459 weights:  [0.6753735  0.26454827 0.25375912 0.03325877 0.21425182 0.4976236\n",
      " 0.5864686  0.2488625  0.76298994 0.5118893  0.15382281 0.8724906\n",
      " 0.90986466 0.9377899  0.50954264 0.5818036  0.8506481  0.33702755\n",
      " 0.55896086 0.56808215 0.972205   0.09838361 0.76098806 0.1710338 ]\n",
      "Step: 4 , Reward:  1.5487715244288849 weights:  [0.7546826  0.2532343  0.09135169 0.8428011  0.6501963  0.12299946\n",
      " 0.6563515  0.6124524  0.757955   0.13630289 0.58040315 0.17259294\n",
      " 0.07216269 0.814861   0.79449093 0.86780506 0.69924617 0.35244256\n",
      " 0.5554136  0.05871293 0.23692828 0.13416582 0.8644867  0.4354061 ]\n",
      "Step: 5 , Reward:  1.4742296953684488 weights:  [0.58955896 0.35035193 0.23396242 0.95319396 0.58867115 0.8710423\n",
      " 0.6635281  0.9012357  0.2593268  0.69972837 0.07481563 0.95997584\n",
      " 0.88714164 0.9881304  0.20634961 0.20683208 0.66213816 0.50099397\n",
      " 0.97072905 0.10411015 0.2069003  0.12766284 0.09379429 0.8281325 ]\n",
      "Step: 6 , Reward:  1.223929870054397 weights:  [0.5723243  0.9645188  0.787848   0.09767413 0.53754574 0.58031344\n",
      " 0.18188763 0.90966177 0.5847882  0.9132616  0.02882284 0.4640977\n",
      " 0.65381265 0.9000293  0.42022818 0.5402334  0.14972717 0.6932758\n",
      " 0.1128836  0.68452376 0.6978952  0.75171435 0.52257943 0.42288047]\n",
      "Step: 7 , Reward:  1.532562397391411 weights:  [0.9689138  0.46410465 0.21249801 0.9137971  0.0612537  0.6198082\n",
      " 0.03225768 0.33797434 0.7285678  0.82371134 0.89761215 0.97143716\n",
      " 0.46996424 0.44339022 0.26581886 0.11888471 0.415174   0.02780798\n",
      " 0.34155077 0.237578   0.12299305 0.46905622 0.3082332  0.756099  ]\n",
      "Step: 8 , Reward:  1.694054972624822 weights:  [0.64011943 0.6923081  0.8043985  0.58779794 0.6717895  0.7484958\n",
      " 0.15806413 0.3160348  0.9448032  0.7131229  0.18765607 0.9264333\n",
      " 0.60961396 0.2241565  0.4867195  0.35219735 0.48562068 0.8440229\n",
      " 0.818927   0.7846038  0.1080012  0.14394855 0.79200417 0.38829413]\n",
      "Step: 9 , Reward:  1.2635865895377605 weights:  [8.8165230e-01 6.3053292e-01 9.9863249e-01 1.0742617e-01 6.5012264e-01\n",
      " 6.8876642e-01 3.9073613e-01 7.2868699e-01 9.1619408e-01 9.2000097e-01\n",
      " 1.9671899e-01 9.1676652e-02 9.2449892e-01 8.0523151e-01 7.5499558e-01\n",
      " 6.0217029e-01 8.3889961e-01 9.8581600e-01 4.7907764e-01 7.6520765e-01\n",
      " 9.3597734e-01 5.4374671e-01 9.3436241e-04 8.1428170e-02]\n",
      "Step: 10 , Reward:  1.2551002100757656 weights:  [0.00368601 0.5344797  0.06037816 0.06142837 0.0963034  0.96849245\n",
      " 0.05444315 0.53162456 0.4236853  0.7666379  0.2534917  0.62183595\n",
      " 0.4991353  0.5627423  0.51193887 0.23221308 0.24070072 0.10387796\n",
      " 0.63577294 0.992447   0.3115897  0.2847316  0.0277431  0.27990606]\n",
      "Step: 11 , Reward:  1.2528612440765288 weights:  [0.70751506 0.61897165 0.6453328  0.33709654 0.13404083 0.51139057\n",
      " 0.95476025 0.33511034 0.8145238  0.9993785  0.97762835 0.31722024\n",
      " 0.18083721 0.7758384  0.71473145 0.48209417 0.20489222 0.73952043\n",
      " 0.17334491 0.47997054 0.8501875  0.19694048 0.66602767 0.63123935]\n",
      "Step: 12 , Reward:  1.4099995075396883 weights:  [0.44686875 0.31386793 0.6534596  0.79033023 0.09816864 0.9678818\n",
      " 0.10289145 0.93482447 0.53778917 0.9038267  0.21946734 0.19841418\n",
      " 0.9404152  0.6053388  0.538821   0.970281   0.5391336  0.68385935\n",
      " 0.22655219 0.7993327  0.54164886 0.9993198  0.197613   0.19905448]\n",
      "Step: 13 , Reward:  1.550384991382875 weights:  [0.03431398 0.23036718 0.7840372  0.9890923  0.10054177 0.00648397\n",
      " 0.70647085 0.8590304  0.699539   0.7255604  0.726016   0.71143514\n",
      " 0.40504125 0.16569585 0.7069301  0.01158318 0.6894737  0.20843074\n",
      " 0.07927489 0.71876085 0.5245317  0.24417788 0.9245545  0.21908468]\n",
      "Step: 14 , Reward:  1.634425576872965 weights:  [0.8856408  0.4120327  0.08053479 0.9832088  0.2149005  0.78515315\n",
      " 0.5515668  0.17609167 0.7464108  0.94723254 0.59328574 0.7998767\n",
      " 0.7857463  0.6427387  0.50185865 0.7711819  0.41214287 0.6441991\n",
      " 0.22226262 0.75171965 0.1339871  0.33002198 0.90392345 0.16250688]\n",
      "Step: 15 , Reward:  1.6149196077379901 weights:  [0.70943695 0.7427308  0.8461155  0.89404607 0.6528768  0.97099036\n",
      " 0.78354686 0.8062262  0.7921618  0.7523945  0.482141   0.6975749\n",
      " 0.6897138  0.43278766 0.5493725  0.15656906 0.8199857  0.685456\n",
      " 0.14480501 0.7409792  0.96049833 0.56411785 0.7432447  0.04688752]\n",
      "Step: 16 , Reward:  1.522989435969877 weights:  [0.2256633  0.26468876 0.952321   0.3197282  0.25286612 0.9823141\n",
      " 0.09717223 0.2707955  0.5461837  0.17154157 0.8478834  0.22669345\n",
      " 0.96780324 0.49238154 0.93543446 0.03674358 0.87652415 0.5647879\n",
      " 0.0094445  0.5784167  0.9468082  0.9309398  0.42183173 0.04406938]\n",
      "Step: 17 , Reward:  1.3346288246432594 weights:  [0.9849173  0.5180537  0.27434084 0.7283388  0.30040744 0.3637423\n",
      " 0.5902983  0.7001753  0.05389795 0.9832798  0.5664329  0.37994844\n",
      " 0.31814432 0.51194006 0.41491523 0.8476931  0.8591634  0.45065036\n",
      " 0.91766924 0.9776256  0.44044286 0.01408628 0.20466775 0.10966995]\n",
      "Step: 18 , Reward:  1.2934929356420288 weights:  [0.5161611  0.43029144 0.21566951 0.909534   0.48295522 0.5928452\n",
      " 0.23053837 0.87624335 0.09402502 0.9278818  0.07147026 0.43512484\n",
      " 0.2736411  0.6531784  0.71029073 0.54360765 0.18508881 0.4791642\n",
      " 0.6325689  0.5349455  0.69402945 0.5613573  0.9593109  0.79654855]\n",
      "Step: 19 , Reward:  1.2597128254747822 weights:  [0.23288614 0.89974403 0.32983017 0.9482672  0.41466025 0.83665514\n",
      " 0.93590283 0.272727   0.63101137 0.09962574 0.49464658 0.69843835\n",
      " 0.09409332 0.37276828 0.33472002 0.4643645  0.14182627 0.084461\n",
      " 0.53463453 0.3163821  0.3194689  0.8974961  0.53828925 0.30597052]\n",
      "Step: 20 , Reward:  1.1496842204803002 weights:  [0.98012793 0.33967468 0.6752797  0.33661702 0.00664189 0.9270623\n",
      " 0.1817593  0.8402788  0.73656774 0.10502142 0.75977814 0.18268472\n",
      " 0.984502   0.28845662 0.33125904 0.02508032 0.18945926 0.3580383\n",
      " 0.06938708 0.09359491 0.47723728 0.7856978  0.6290411  0.04035693]\n",
      "Step: 21 , Reward:  0.654784966201136 weights:  [0.7936616  0.6309764  0.85688156 0.5881656  0.7758933  0.3137406\n",
      " 0.83825827 0.12842315 0.6798887  0.47640288 0.23250422 0.8846352\n",
      " 0.33953092 0.43022048 0.52859926 0.14845023 0.33585888 0.2327047\n",
      " 0.889422   0.86627644 0.13640323 0.7455682  0.24792877 0.354249  ]\n",
      "Step: 22 , Reward:  0.41044056954359914 weights:  [0.2843723  0.27361944 0.66623527 0.12321997 0.918561   0.8214413\n",
      " 0.2939999  0.5632134  0.9323781  0.69996655 0.47346205 0.47488514\n",
      " 0.7590712  0.334203   0.699701   0.09207007 0.45751074 0.81517667\n",
      " 0.9459992  0.580015   0.69581497 0.23025203 0.37906379 0.29032266]\n",
      "Step: 23 , Reward:  0.3084741794140897 weights:  [0.12266555 0.954206   0.09154749 0.66982585 0.5732029  0.5171193\n",
      " 0.5388278  0.84782475 0.25400066 0.9822476  0.952131   0.5458787\n",
      " 0.47780484 0.37083974 0.9372151  0.3829564  0.12724078 0.21604156\n",
      " 0.34990588 0.5893696  0.51959354 0.8776384  0.3353653  0.15048051]\n",
      "Step: 24 , Reward:  0.4292792607319629 weights:  [0.5325299  0.5686964  0.22966689 0.2553677  0.10915297 0.12417808\n",
      " 0.11543036 0.8220817  0.54868346 0.05220711 0.37376505 0.38758576\n",
      " 0.82625765 0.3953266  0.7528574  0.25269064 0.61544067 0.72040355\n",
      " 0.8555423  0.94885606 0.33075932 0.34554547 0.08869469 0.50545573]\n",
      "Step: 25 , Reward:  0.562124342075673 weights:  [0.46271753 0.6321843  0.7777415  0.06322065 0.5556723  0.6373234\n",
      " 0.7204001  0.9945648  0.7900751  0.07199904 0.5012467  0.62963694\n",
      " 0.28822923 0.02372348 0.6284996  0.73538816 0.0695073  0.61115044\n",
      " 0.2494305  0.5463888  0.4510663  0.84951556 0.75838125 0.89001495]\n",
      "Step: 26 , Reward:  0.6610994563046393 weights:  [0.60337603 0.1227622  0.8487826  0.8521078  0.37865093 0.46961185\n",
      " 0.32203403 0.04420424 0.99469817 0.74684745 0.53478473 0.51286095\n",
      " 0.79468304 0.7128093  0.8634279  0.58282644 0.12113827 0.0427469\n",
      " 0.6089193  0.117953   0.80272204 0.63569474 0.63895303 0.8312661 ]\n",
      "Step: 27 , Reward:  0.6072744729070142 weights:  [0.40806195 0.45020118 0.8916434  0.6954321  0.08064288 0.82093453\n",
      " 0.04367024 0.27009743 0.52793044 0.884329   0.5784671  0.5382894\n",
      " 0.20703304 0.5298664  0.15202391 0.65577686 0.41754937 0.03341568\n",
      " 0.35774925 0.87828565 0.6051703  0.11592644 0.6129465  0.28743798]\n",
      "Step: 28 , Reward:  0.5649924855013452 weights:  [0.11865884 0.22901744 0.14326814 0.416075   0.96404946 0.5410883\n",
      " 0.17208159 0.03235617 0.6538778  0.00241512 0.13767737 0.2815274\n",
      " 0.70753264 0.4554015  0.06718332 0.9360125  0.8545574  0.7302546\n",
      " 0.6847079  0.1249536  0.18639296 0.13766521 0.97990996 0.39692193]\n",
      "Step: 29 , Reward:  0.5462137469488398 weights:  [0.3430587  0.39440563 0.41908643 0.8569919  0.68385774 0.46254924\n",
      " 0.24746436 0.85045826 0.29472414 0.3042602  0.25673237 0.740212\n",
      " 0.22125709 0.52831435 0.33258846 0.33196017 0.7549073  0.43604115\n",
      " 0.8592714  0.94855225 0.08584774 0.2606651  0.36421314 0.23887676]\n",
      "Step: 30 , Reward:  0.6174267695870077 weights:  [0.9168665  0.07578182 0.49352643 0.33710623 0.00845066 0.3603574\n",
      " 0.6405123  0.8181864  0.67479616 0.4391847  0.07756847 0.10056323\n",
      " 0.4903792  0.29134583 0.4570737  0.37702405 0.4672598  0.6097942\n",
      " 0.2997808  0.18560886 0.79174954 0.49449614 0.8491069  0.04778638]\n",
      "Step: 31 , Reward:  0.3438372299184378 weights:  [2.4622679e-04 5.2074188e-01 9.0850669e-01 6.5695965e-01 6.8894219e-01\n",
      " 4.8834234e-01 4.7652453e-02 3.3066195e-01 4.4574133e-01 3.5191739e-01\n",
      " 5.3902602e-01 5.5634922e-01 9.7812748e-01 4.7765404e-01 9.0852833e-01\n",
      " 4.3336201e-01 7.7116674e-01 1.9750208e-02 2.9835317e-01 3.3594018e-01\n",
      " 8.9064372e-01 2.3957613e-01 1.8759274e-01 7.9711890e-01]\n",
      "Step: 32 , Reward:  0.539716648910281 weights:  [0.5646473  0.9334817  0.31051254 0.6471556  0.76327354 0.4110549\n",
      " 0.26717624 0.6789609  0.6991624  0.70196843 0.16298941 0.80202603\n",
      " 0.8147495  0.896981   0.8970658  0.27271575 0.05490604 0.702932\n",
      " 0.34042913 0.6764195  0.5746315  0.3755792  0.9004496  0.06309259]\n",
      "Step: 33 , Reward:  0.5938639223989673 weights:  [0.05896786 0.8485242  0.11400712 0.25431278 0.4118082  0.7318844\n",
      " 0.8596846  0.39402375 0.23866194 0.89847225 0.692551   0.49961254\n",
      " 0.13533169 0.24372843 0.873906   0.7404351  0.7833294  0.9158555\n",
      " 0.16408491 0.6811086  0.6274235  0.47551554 0.11696246 0.14236021]\n",
      "Step: 34 , Reward:  0.5122471120876898 weights:  [0.4259228  0.7061759  0.16411936 0.20140094 0.49203548 0.87905496\n",
      " 0.9139042  0.36403346 0.9723043  0.9787537  0.32935998 0.6063354\n",
      " 0.6665528  0.29632846 0.29379386 0.42884466 0.48637816 0.54785204\n",
      " 0.00422621 0.51986176 0.37262955 0.73519015 0.28248578 0.24016118]\n",
      "Step: 35 , Reward:  0.659693428805118 weights:  [0.7178034  0.27693623 0.13549095 0.3616758  0.6945211  0.46567076\n",
      " 0.04775047 0.10499012 0.57810265 0.1491996  0.6866465  0.25715792\n",
      " 0.62492    0.24437761 0.18230906 0.22587222 0.43941435 0.49748966\n",
      " 0.8622184  0.50689894 0.34643295 0.80413604 0.13336259 0.0431546 ]\n",
      "Step: 36 , Reward:  1.1077602793419246 weights:  [0.337004   0.95899075 0.07621852 0.0022361  0.19180226 0.38594097\n",
      " 0.34011474 0.86973494 0.12158805 0.082789   0.38888744 0.9947939\n",
      " 0.73064643 0.6776847  0.5507027  0.9045976  0.09529316 0.39534345\n",
      " 0.24410754 0.44191194 0.3050037  0.4957295  0.35984272 0.902114  ]\n",
      "Step: 37 , Reward:  1.142811240094778 weights:  [0.76052886 0.5915386  0.2000798  0.42168573 0.7626276  0.6606016\n",
      " 0.1905187  0.6827102  0.3242368  0.8017626  0.56872165 0.06787154\n",
      " 0.2742012  0.5135023  0.28474018 0.02387008 0.31061304 0.6471991\n",
      " 0.73607767 0.0481526  0.14036834 0.22290888 0.231745   0.10304034]\n",
      "Step: 38 , Reward:  1.2060450550155444 weights:  [0.34601977 0.35791653 0.45523545 0.9535442  0.37194517 0.6623965\n",
      " 0.98080736 0.70639473 0.76112026 0.58686024 0.75953823 0.92045736\n",
      " 0.23644489 0.19053236 0.9919632  0.19246715 0.68841517 0.00929406\n",
      " 0.76482075 0.13612008 0.653922   0.4203453  0.00164416 0.0583176 ]\n",
      "Step: 39 , Reward:  0.7107599050084838 weights:  [0.8899655  0.3271646  0.20410493 0.84382296 0.08289018 0.9554619\n",
      " 0.9356664  0.7038619  0.5339861  0.73180664 0.78831816 0.60708797\n",
      " 0.36633185 0.4539935  0.02348262 0.5299268  0.60789174 0.31031126\n",
      " 0.07022977 0.7668207  0.28612256 0.14245373 0.9866178  0.8210091 ]\n",
      "Step: 40 , Reward:  0.5011566283126817 weights:  [0.4626099  0.974819   0.86645937 0.6269599  0.05052388 0.6426962\n",
      " 0.3586986  0.90866107 0.24184358 0.5213423  0.6778211  0.33236602\n",
      " 0.09651291 0.7150108  0.23003426 0.8708942  0.9854389  0.6036756\n",
      " 0.22616053 0.72451407 0.11098331 0.1718688  0.40578967 0.59812796]\n",
      "Step: 41 , Reward:  0.35706160085551786 weights:  [0.9124921  0.30535874 0.71227264 0.46629998 0.19972065 0.58332497\n",
      " 0.23604184 0.26760906 0.6064027  0.3642789  0.85799986 0.9497828\n",
      " 0.5765106  0.00107706 0.14504087 0.01941636 0.9445941  0.76530373\n",
      " 0.02124998 0.78462994 0.5765718  0.04297873 0.27687266 0.57119024]\n",
      "Step: 42 , Reward:  0.2765344577886261 weights:  [0.62979233 0.6040578  0.8180189  0.51359755 0.4572653  0.1242182\n",
      " 0.74378836 0.79759425 0.16129038 0.7351597  0.68267876 0.5548171\n",
      " 0.5301829  0.8683199  0.07153088 0.9242197  0.62535125 0.25395107\n",
      " 0.14521202 0.9564014  0.9068385  0.36537242 0.08670384 0.74828804]\n",
      "Step: 43 , Reward:  0.06789298399402681 weights:  [0.7021663  0.42726475 0.14271355 0.6536533  0.13474786 0.80650544\n",
      " 0.63944125 0.79787296 0.43836465 0.5688513  0.80476236 0.3259459\n",
      " 0.08292854 0.2782677  0.25326127 0.43653232 0.62551415 0.48901853\n",
      " 0.6575818  0.6272075  0.38656723 0.06065109 0.4511765  0.71181947]\n",
      "Step: 44 , Reward:  -0.11819971463552705 weights:  [0.85716516 0.46412373 0.41291595 0.8022798  0.9754489  0.4534951\n",
      " 0.00540063 0.20751733 0.5141431  0.9827158  0.9696962  0.46135825\n",
      " 0.78956765 0.34573781 0.16356239 0.7619214  0.40226078 0.07970753\n",
      " 0.05007836 0.62533283 0.6329603  0.4357964  0.6780658  0.2793939 ]\n",
      "Step: 45 , Reward:  -0.2540830618244659 weights:  [0.32775325 0.14769757 0.3055734  0.98868996 0.89386743 0.86615485\n",
      " 0.13529679 0.7914426  0.1780535  0.6206532  0.29657778 0.82785994\n",
      " 0.6373371  0.4209365  0.01627597 0.40947202 0.43813592 0.16433722\n",
      " 0.22845161 0.1437369  0.23341733 0.32203782 0.03518069 0.02701634]\n",
      "Step: 46 , Reward:  0.09902303159588158 weights:  [0.26368082 0.8835929  0.5191348  0.9870285  0.43853232 0.14070699\n",
      " 0.26656383 0.73195213 0.16562116 0.49039215 0.8636301  0.60337275\n",
      " 0.9191439  0.09350932 0.6708571  0.36329827 0.4438011  0.02874631\n",
      " 0.89336413 0.04766029 0.11138359 0.8069686  0.38579473 0.93745893]\n",
      "Step: 47 , Reward:  0.25354630413996004 weights:  [0.29949582 0.500646   0.58748037 0.95412946 0.17196989 0.3475761\n",
      " 0.32476875 0.47118986 0.32656902 0.27758688 0.5484464  0.17676103\n",
      " 0.9802533  0.8622322  0.6040096  0.6456662  0.05435145 0.07449886\n",
      " 0.7755635  0.86522245 0.0678345  0.6561201  0.8211107  0.63372284]\n",
      "Step: 48 , Reward:  0.1617398242165152 weights:  [0.17305803 0.9140057  0.16556352 0.31308338 0.39993644 0.3336013\n",
      " 0.03736341 0.81342113 0.02894911 0.7746038  0.23930234 0.24547434\n",
      " 0.41096094 0.8986472  0.55025196 0.04809546 0.4555405  0.8347389\n",
      " 0.6834097  0.26194957 0.30180383 0.24533868 0.38973284 0.26378882]\n",
      "Step: 49 , Reward:  0.04642669018730398 weights:  [9.6381760e-01 1.5732756e-01 6.5672815e-02 7.2374475e-01 5.8883685e-01\n",
      " 1.7712101e-01 1.3543677e-01 7.0849061e-04 9.3311292e-01 2.0185286e-01\n",
      " 3.6368567e-01 5.9776825e-01 5.2681160e-01 3.2284117e-01 7.7960575e-01\n",
      " 7.9654425e-01 7.5233352e-01 5.5672789e-01 3.6022890e-01 5.2168399e-01\n",
      " 3.0309755e-01 6.5955269e-01 2.0239204e-01 7.2667199e-01]\n",
      "Step: 50 , Reward:  -0.22815500367439737 weights:  [0.7031582  0.5104228  0.90078723 0.8745934  0.0116334  0.39389282\n",
      " 0.703249   0.23418671 0.96952194 0.93332213 0.9119359  0.9538688\n",
      " 0.48942816 0.70166475 0.3313921  0.49086025 0.25229067 0.18851566\n",
      " 0.93129617 0.19029087 0.4151937  0.51725155 0.33343926 0.53375894]\n",
      "Step: 51 , Reward:  -0.1722888560909822 weights:  [0.2709315  0.7859277  0.3291626  0.0900692  0.6030722  0.82138175\n",
      " 0.7261561  0.4970596  0.75734967 0.18090591 0.25479662 0.24585837\n",
      " 0.776002   0.28573745 0.7749118  0.4411781  0.03453636 0.8362622\n",
      " 0.26120758 0.8860775  0.9895867  0.6647757  0.18924153 0.55107534]\n",
      "Step: 52 , Reward:  -0.0046334509934763445 weights:  [0.67601794 0.89037246 0.7417288  0.65193284 0.60032874 0.47031945\n",
      " 0.7201619  0.647258   0.24426511 0.24164969 0.3339716  0.41049132\n",
      " 0.52903265 0.18988115 0.7497446  0.37775666 0.14751202 0.9526325\n",
      " 0.40383464 0.35792205 0.22924936 0.6837398  0.792587   0.8058094 ]\n",
      "Step: 53 , Reward:  -0.13980148469261286 weights:  [0.05353066 0.32835454 0.67428815 0.9442081  0.0083476  0.41861582\n",
      " 0.96749544 0.62688065 0.98916733 0.7209131  0.75018054 0.874058\n",
      " 0.8852929  0.04260588 0.10446143 0.08661014 0.58644897 0.757726\n",
      " 0.98463345 0.46446043 0.1397031  0.5321453  0.9407054  0.422018  ]\n",
      "Step: 54 , Reward:  0.495569948459144 weights:  [0.49636665 0.4706989  0.5754114  0.31738225 0.78476626 0.29834133\n",
      " 0.22704017 0.07894489 0.26068637 0.15593994 0.55368716 0.56587857\n",
      " 0.9020396  0.82885695 0.6236624  0.48929492 0.9699981  0.7219371\n",
      " 0.39869314 0.45917493 0.7205828  0.9918519  0.20059335 0.36194655]\n",
      "Step: 55 , Reward:  0.7293417386129486 weights:  [0.3957139  0.38082883 0.88737035 0.16006416 0.5573861  0.3896272\n",
      " 0.5757161  0.27160278 0.2650583  0.74068    0.4831022  0.3414065\n",
      " 0.926198   0.86869204 0.08899873 0.15503126 0.06728017 0.36049473\n",
      " 0.7946866  0.9849666  0.41559574 0.7477445  0.7692724  0.1116173 ]\n",
      "Step: 56 , Reward:  0.7879654197166963 weights:  [0.18609136 0.9036084  0.9284449  0.6729367  0.8905427  0.98665184\n",
      " 0.35209483 0.2292698  0.09054434 0.5975662  0.98226905 0.9104086\n",
      " 0.08503231 0.322573   0.44463405 0.95351416 0.84864753 0.10417783\n",
      " 0.20267585 0.19046858 0.14588314 0.33826327 0.9325496  0.5722251 ]\n",
      "Step: 57 , Reward:  0.7689051486295728 weights:  [0.9880438  0.97589374 0.1995258  0.8918512  0.7567486  0.32860735\n",
      " 0.255063   0.60823923 0.23048368 0.69673526 0.34765536 0.80513483\n",
      " 0.3322329  0.39857963 0.2870454  0.8406191  0.94743824 0.30883265\n",
      " 0.4629308  0.8561925  0.8492217  0.7776365  0.26818475 0.22477487]\n",
      "Step: 58 , Reward:  0.8196708824277148 weights:  [0.26320535 0.7631777  0.6327253  0.41841573 0.9837547  0.0756405\n",
      " 0.12864658 0.70640445 0.21338063 0.9319532  0.00672948 0.7132875\n",
      " 0.27883834 0.81718504 0.7980692  0.11786777 0.63676083 0.7094595\n",
      " 0.72595507 0.15952832 0.47989053 0.7553303  0.5011349  0.92672354]\n",
      "Step: 59 , Reward:  0.7168478747527872 weights:  [0.02026489 0.23472074 0.00773525 0.3154898  0.47936326 0.18676704\n",
      " 0.09601045 0.85954845 0.35808283 0.03440309 0.04287836 0.44135976\n",
      " 0.990873   0.23684847 0.05905995 0.19778422 0.44134986 0.49108395\n",
      " 0.5090512  0.05963331 0.17209929 0.68416923 0.21747983 0.40025792]\n",
      "Step: 60 , Reward:  0.8465642345533719 weights:  [0.41146317 0.1550262  0.6291922  0.4588896  0.12522233 0.14649653\n",
      " 0.47733125 0.96279186 0.52832466 0.18898681 0.5794721  0.98361236\n",
      " 0.7907013  0.08733475 0.5882039  0.8533005  0.2636629  0.40570793\n",
      " 0.37930673 0.9454364  0.13285851 0.08581895 0.79917115 0.7669108 ]\n",
      "Step: 61 , Reward:  0.9808886209960791 weights:  [0.15771651 0.33044636 0.22768956 0.7784918  0.44401628 0.4731249\n",
      " 0.05965593 0.9747341  0.72732353 0.6968064  0.36577493 0.62108815\n",
      " 0.31200835 0.6377813  0.5685554  0.18987316 0.16510957 0.1950633\n",
      " 0.53844494 0.4411508  0.03538898 0.9852613  0.6170145  0.0846827 ]\n",
      "Step: 62 , Reward:  0.7496374207425269 weights:  [0.43913957 0.13044614 0.6450001  0.27750906 0.2645795  0.819434\n",
      " 0.8814904  0.7335171  0.9168894  0.2512996  0.39244914 0.9608927\n",
      " 0.48916292 0.8312992  0.750926   0.08869052 0.16932803 0.5892964\n",
      " 0.8879158  0.97356594 0.01145905 0.36231503 0.12476638 0.80061215]\n",
      "Step: 63 , Reward:  0.8192588822305507 weights:  [0.3472803  0.1858426  0.5583454  0.48724404 0.8159325  0.96639836\n",
      " 0.83431906 0.4711893  0.1185351  0.9282327  0.8105382  0.372649\n",
      " 0.6386789  0.5362032  0.82392675 0.28013194 0.9466141  0.5756936\n",
      " 0.6071804  0.64496726 0.84934324 0.73796064 0.75836515 0.6301936 ]\n",
      "Step: 64 , Reward:  0.7385095332725665 weights:  [0.22450581 0.81134385 0.3286639  0.7742964  0.39749676 0.75732255\n",
      " 0.65748805 0.43159595 0.9468034  0.87362343 0.91367257 0.13671881\n",
      " 0.5410116  0.5624524  0.9360081  0.90730375 0.84048223 0.36030108\n",
      " 0.01803285 0.36091682 0.55716085 0.23241821 0.9848114  0.8512205 ]\n",
      "Step: 65 , Reward:  0.28824490778123757 weights:  [0.9797911  0.53939015 0.60414517 0.06850076 0.17221504 0.6693136\n",
      " 0.77040726 0.63620967 0.11057436 0.60869265 0.5020183  0.0357362\n",
      " 0.7220314  0.6064474  0.09962469 0.84880567 0.33643082 0.04638571\n",
      " 0.25458848 0.9840059  0.9481606  0.49244583 0.20063812 0.83787274]\n",
      "Step: 66 , Reward:  0.23667517983565495 weights:  [0.13742465 0.64481497 0.551641   0.14846188 0.9687446  0.08457834\n",
      " 0.56391203 0.3158989  0.5079874  0.5962418  0.842438   0.12062383\n",
      " 0.68671745 0.5038764  0.94970715 0.22547144 0.29166773 0.66621816\n",
      " 0.0848158  0.9794265  0.22293404 0.8904896  0.24671346 0.23563391]\n",
      "Step: 67 , Reward:  0.2676071080218768 weights:  [0.04516673 0.29548463 0.4710763  0.3219426  0.37989652 0.6902877\n",
      " 0.65369964 0.38222128 0.83691114 0.8053688  0.44828445 0.14310235\n",
      " 0.43320876 0.9677596  0.3043209  0.9849811  0.7803215  0.56340796\n",
      " 0.38021165 0.7906684  0.6225625  0.32190332 0.05554047 0.7622794 ]\n",
      "Step: 68 , Reward:  0.28987739994987605 weights:  [0.3712598  0.4482567  0.8237735  0.97423077 0.5342458  0.4297958\n",
      " 0.8318217  0.7072067  0.25405154 0.9425886  0.23929176 0.15020889\n",
      " 0.448286   0.3461984  0.59623426 0.75163156 0.7600526  0.22157574\n",
      " 0.66125864 0.05704337 0.15370786 0.41858628 0.3624914  0.3768986 ]\n",
      "Step: 69 , Reward:  0.3660385069531943 weights:  [0.70638317 0.06354505 0.29152784 0.78819394 0.26570302 0.30052468\n",
      " 0.34389666 0.62250763 0.48235735 0.04477611 0.36541694 0.81687707\n",
      " 0.05976996 0.74613214 0.7735352  0.46754357 0.7185975  0.61053663\n",
      " 0.4208293  0.4964451  0.85594994 0.8975228  0.07865486 0.36667985]\n",
      "Step: 70 , Reward:  0.5032698041291428 weights:  [0.00425661 0.22742587 0.39911458 0.9556584  0.15304852 0.18216175\n",
      " 0.17633647 0.80597246 0.11578894 0.01132986 0.44612104 0.45430467\n",
      " 0.66124606 0.2885516  0.46216688 0.00903413 0.5292456  0.25537127\n",
      " 0.98528284 0.76670927 0.96599716 0.34929535 0.3447387  0.77884203]\n",
      "Step: 71 , Reward:  0.5828623107760946 weights:  [0.03972054 0.36370096 0.23521501 0.46583992 0.38005596 0.3757113\n",
      " 0.81813323 0.23759407 0.17136016 0.8276632  0.87387604 0.7700992\n",
      " 0.8073332  0.8622825  0.48622143 0.3078313  0.6995509  0.527358\n",
      " 0.37624866 0.39170167 0.2944651  0.6297533  0.44953093 0.80718464]\n",
      "Step: 72 , Reward:  0.5668758985827823 weights:  [0.23557985 0.9957578  0.76496595 0.68861943 0.17508441 0.52992535\n",
      " 0.7126818  0.630772   0.44931793 0.87429893 0.65330356 0.6004377\n",
      " 0.35849053 0.24452454 0.05068165 0.30716565 0.94293535 0.5751502\n",
      " 0.48913205 0.6183396  0.72303015 0.00276458 0.97296137 0.83071655]\n",
      "Step: 73 , Reward:  0.6361751142305127 weights:  [0.1412201  0.9713379  0.01428008 0.13449004 0.03597373 0.3757749\n",
      " 0.00584778 0.9879217  0.82289875 0.18391919 0.32205987 0.8343567\n",
      " 0.05559161 0.6730994  0.77989787 0.29989013 0.4844336  0.27301624\n",
      " 0.55402744 0.36200705 0.4427048  0.00394297 0.4932596  0.76821667]\n",
      "Step: 74 , Reward:  0.49921389692701934 weights:  [0.75846934 0.6992396  0.834885   0.667283   0.07228643 0.4085949\n",
      " 0.48998302 0.27939087 0.47286108 0.021878   0.87876546 0.5281726\n",
      " 0.8596374  0.6029956  0.7629853  0.44099137 0.598794   0.5887797\n",
      " 0.2601723  0.9528901  0.26686475 0.1979779  0.01723832 0.4305503 ]\n",
      "Step: 75 , Reward:  0.32114065183399426 weights:  [0.47709912 0.9756496  0.16412526 0.94772905 0.17559785 0.3825764\n",
      " 0.45862186 0.5754068  0.83997184 0.14817023 0.336724   0.272239\n",
      " 0.22576648 0.48220295 0.9827161  0.7731828  0.4273201  0.7085707\n",
      " 0.999249   0.9026725  0.6054267  0.25920564 0.28961492 0.83115745]\n",
      "Step: 76 , Reward:  0.13749823719914916 weights:  [0.6698153  0.8450599  0.39520967 0.6237872  0.22517952 0.23916686\n",
      " 0.27302328 0.35628143 0.7533538  0.10435414 0.2513156  0.5575698\n",
      " 0.9754859  0.38641664 0.38869518 0.5002382  0.23323679 0.74839264\n",
      " 0.01400921 0.93311256 0.9308736  0.8929356  0.9243531  0.40430167]\n",
      "Step: 77 , Reward:  0.16060698686010125 weights:  [0.91243434 0.33558676 0.76503736 0.15593398 0.92136216 0.63569605\n",
      " 0.7486355  0.71291625 0.27468413 0.8181266  0.921019   0.27963677\n",
      " 0.47676632 0.2608077  0.53590965 0.78096604 0.42481703 0.97785294\n",
      " 0.35245344 0.3825462  0.45258284 0.9886764  0.00448629 0.27300638]\n",
      "Step: 78 , Reward:  0.22906853749907236 weights:  [0.39515737 0.9319089  0.89361614 0.27146724 0.76499206 0.03732565\n",
      " 0.09242368 0.9823454  0.5910728  0.95658094 0.20543909 0.7233061\n",
      " 0.10773841 0.522023   0.7760578  0.49202156 0.1698947  0.34686503\n",
      " 0.5437253  0.91015226 0.05879754 0.06900913 0.9787178  0.0565424 ]\n",
      "Step: 79 , Reward:  0.4807370507308141 weights:  [0.24712136 0.53375745 0.78938043 0.36292052 0.7726003  0.57129675\n",
      " 0.68485206 0.3716215  0.02155221 0.08368242 0.6948324  0.5320559\n",
      " 0.29399475 0.8434394  0.37638727 0.60959035 0.51070046 0.6291953\n",
      " 0.64307266 0.27281353 0.55571765 0.6597608  0.7283511  0.93079925]\n",
      "Step: 80 , Reward:  0.4711319905721496 weights:  [0.04523405 0.96469724 0.78549993 0.14231133 0.43536115 0.97126514\n",
      " 0.14012021 0.12505704 0.9860185  0.42252943 0.29757744 0.36430904\n",
      " 0.5874972  0.87442786 0.13697797 0.16180885 0.00865275 0.94153804\n",
      " 0.2390796  0.00565925 0.5862475  0.2890787  0.03111345 0.9152716 ]\n",
      "Step: 81 , Reward:  0.5793400077927872 weights:  [0.9481834  0.4281561  0.45348406 0.2684807  0.7508799  0.20141649\n",
      " 0.86325663 0.538696   0.70884657 0.37754723 0.22646868 0.7670656\n",
      " 0.22359633 0.2818094  0.46873537 0.3368053  0.49550596 0.79127103\n",
      " 0.01498145 0.24076879 0.5351983  0.7063592  0.51615834 0.6599419 ]\n",
      "Step: 82 , Reward:  0.5494956848161046 weights:  [0.10473624 0.9161755  0.9648791  0.49075037 0.81329834 0.18853253\n",
      " 0.49622482 0.32666254 0.53246725 0.27745998 0.71138686 0.3699923\n",
      " 0.04307711 0.26815766 0.9134514  0.1945518  0.3602047  0.27820763\n",
      " 0.25104758 0.9247026  0.63238144 0.228787   0.45463184 0.25548834]\n",
      "Step: 83 , Reward:  0.5465967940499611 weights:  [0.8870215  0.12042564 0.69139165 0.11816919 0.9043705  0.0261763\n",
      " 0.99551284 0.2119118  0.78380865 0.1699     0.00622857 0.5587788\n",
      " 0.6586072  0.53108627 0.45545527 0.02543148 0.19667816 0.7431594\n",
      " 0.39202118 0.24127978 0.5022835  0.09916225 0.5891862  0.8329549 ]\n",
      "Step: 84 , Reward:  0.4451498802974578 weights:  [0.23798096 0.03717202 0.9589159  0.05206022 0.16339386 0.79688215\n",
      " 0.85766816 0.35240093 0.9050198  0.6843991  0.85823524 0.33138347\n",
      " 0.8713316  0.17345971 0.16121387 0.7089954  0.01949295 0.29283896\n",
      " 0.7464307  0.66501886 0.07875457 0.39081526 0.8658245  0.4988364 ]\n",
      "Step: 85 , Reward:  0.23701350916792688 weights:  [0.6346471  0.413302   0.25459254 0.8427932  0.29913428 0.10748151\n",
      " 0.8304182  0.99815285 0.691647   0.04845643 0.2752511  0.21069497\n",
      " 0.32340506 0.69389546 0.0646165  0.7501357  0.8339634  0.4108564\n",
      " 0.31714958 0.8612191  0.8007135  0.6836319  0.9767375  0.89230406]\n",
      "Step: 86 , Reward:  0.019576395851663816 weights:  [0.1801801  0.6918488  0.37194952 0.8588961  0.196365   0.19065627\n",
      " 0.3413426  0.9637675  0.26643756 0.4952705  0.11822909 0.52905935\n",
      " 0.61277884 0.06177595 0.508725   0.7014191  0.01015139 0.4752092\n",
      " 0.7336182  0.03441253 0.6663598  0.9135945  0.45887557 0.1036191 ]\n",
      "Step: 87 , Reward:  0.03160641239017405 weights:  [0.7257754  0.35330078 0.543919   0.3399861  0.3226342  0.38086152\n",
      " 0.41049278 0.472949   0.63400686 0.9470581  0.54130995 0.33301532\n",
      " 0.41177237 0.68217176 0.14731938 0.9534238  0.5577545  0.02655801\n",
      " 0.6964548  0.6146361  0.15469486 0.20588678 0.5900961  0.73870504]\n",
      "Step: 88 , Reward:  -0.08521633509936155 weights:  [0.968977   0.06356698 0.53407955 0.4365674  0.35631582 0.9231498\n",
      " 0.72330797 0.939109   0.32764536 0.437428   0.32678965 0.9852562\n",
      " 0.42264867 0.9693948  0.768675   0.57446563 0.05509561 0.54315704\n",
      " 0.18915135 0.13608599 0.51117617 0.47008932 0.51597434 0.4836224 ]\n",
      "Step: 89 , Reward:  0.09393472269149498 weights:  [0.6528061  0.19815528 0.11625448 0.23079488 0.7841902  0.8141505\n",
      " 0.15324092 0.98331255 0.3639013  0.40625983 0.8575694  0.78047085\n",
      " 0.363598   0.66743034 0.7051807  0.00722918 0.09104696 0.5109602\n",
      " 0.5119584  0.34180132 0.8691188  0.17687136 0.7312377  0.67344505]\n",
      "Step: 90 , Reward:  0.32102863669712695 weights:  [0.6662369  0.7750911  0.6785232  0.77272666 0.5719496  0.5780905\n",
      " 0.37167376 0.46957386 0.0963003  0.78360456 0.3925046  0.0473974\n",
      " 0.56575894 0.21304846 0.7695616  0.6870357  0.9709867  0.020089\n",
      " 0.9181441  0.10856551 0.7775625  0.7813993  0.7212867  0.81248915]\n",
      "Step: 91 , Reward:  0.4792978525906265 weights:  [0.26404294 0.03433543 0.5077532  0.74956566 0.16871342 0.8922777\n",
      " 0.75103575 0.11481297 0.9386367  0.16534412 0.774874   0.6918416\n",
      " 0.9370706  0.8128417  0.5750293  0.415188   0.49540272 0.36037225\n",
      " 0.8350739  0.5056258  0.4330037  0.6019404  0.9524094  0.25439832]\n",
      "Step: 92 , Reward:  0.40963554754949477 weights:  [0.84052664 0.2784891  0.67173576 0.02213559 0.40315357 0.770068\n",
      " 0.877492   0.24459928 0.9451972  0.36037984 0.8604906  0.92272717\n",
      " 0.54055893 0.7384657  0.3709407  0.0918507  0.76121914 0.60987735\n",
      " 0.12068576 0.10221255 0.4666812  0.3369217  0.11855212 0.38807872]\n",
      "Step: 93 , Reward:  0.2684678158822167 weights:  [0.24398696 0.6228851  0.4947777  0.34516814 0.8221462  0.68595475\n",
      " 0.5653333  0.13800997 0.3231682  0.65279067 0.21606252 0.95788956\n",
      " 0.15879047 0.58329934 0.25603205 0.572614   0.59596634 0.5891341\n",
      " 0.3970192  0.0563966  0.8505597  0.5945415  0.6635298  0.12021363]\n",
      "Step: 94 , Reward:  0.04753356750855082 weights:  [0.42174375 0.8752732  0.57709587 0.16967866 0.10078478 0.7716208\n",
      " 0.7958053  0.8705812  0.17229915 0.31368166 0.56425035 0.70355064\n",
      " 0.97519165 0.6910557  0.04551244 0.41041127 0.2589866  0.7881339\n",
      " 0.77882636 0.06710455 0.52228254 0.74680036 0.80379295 0.47780064]\n",
      "Step: 95 , Reward:  0.8616733877888285 weights:  [0.04834491 0.10669601 0.03617087 0.28639674 0.57097644 0.15100473\n",
      " 0.4415823  0.49024555 0.3415221  0.98555744 0.78513366 0.31780627\n",
      " 0.7375328  0.5478405  0.6470912  0.7533675  0.66797024 0.7667003\n",
      " 0.7520249  0.03429008 0.84163725 0.8950589  0.94041425 0.62626404]\n",
      "Step: 96 , Reward:  0.7907961175455118 weights:  [0.76778036 0.68607825 0.6990618  0.39039072 0.81472033 0.9485105\n",
      " 0.714042   0.5039716  0.13888374 0.8870258  0.39407718 0.5489904\n",
      " 0.22128296 0.6594362  0.6887168  0.0807023  0.2678074  0.18169516\n",
      " 0.521773   0.76506376 0.69052356 0.01648197 0.25607646 0.31159234]\n",
      "Step: 97 , Reward:  0.7903922395811649 weights:  [0.81691146 0.11539963 0.83981544 0.1693756  0.91746277 0.6301161\n",
      " 0.65392816 0.67554903 0.92765033 0.9773327  0.8856169  0.76722044\n",
      " 0.55753106 0.8613573  0.48995855 0.12601823 0.5330507  0.02767974\n",
      " 0.1689001  0.3263143  0.6438966  0.849747   0.32107493 0.6444321 ]\n",
      "Step: 98 , Reward:  0.38255548335177 weights:  [0.0082095  0.48491922 0.09285569 0.94322747 0.9027719  0.5557341\n",
      " 0.10693508 0.8211266  0.6225779  0.06298155 0.8458481  0.69006205\n",
      " 0.25755715 0.5809822  0.29612637 0.72096986 0.13305894 0.04688028\n",
      " 0.9001497  0.8229405  0.04023063 0.6894932  0.30245897 0.52783906]\n",
      "Step: 99 , Reward:  0.4214464588517832 weights:  [0.5312373  0.04802293 0.9712696  0.67984706 0.08401081 0.5400635\n",
      " 0.93806064 0.7710805  0.16828406 0.98660713 0.3409075  0.750921\n",
      " 0.33630416 0.77757424 0.31609082 0.6420591  0.72845405 0.21354932\n",
      " 0.6323445  0.6644857  0.6577783  0.17597187 0.6592852  0.9292482 ]\n",
      "Step: 100 , Reward:  0.22657319825285485 weights:  [0.55001646 0.04531738 0.4090875  0.41350472 0.44771153 0.08817804\n",
      " 0.4030581  0.8452029  0.45113668 0.39332494 0.8747372  0.39388812\n",
      " 0.88249534 0.23268706 0.34194085 0.34017155 0.95161813 0.90848553\n",
      " 0.7193659  0.76579356 0.28466988 0.8860253  0.04543123 0.08240676]\n",
      "Step: 101 , Reward:  0.5248687212114762 weights:  [0.9597671  0.9529703  0.28816468 0.95269144 0.6903306  0.00687635\n",
      " 0.5791471  0.13115016 0.50526077 0.6988645  0.36874914 0.11947083\n",
      " 0.16082123 0.44263622 0.25214183 0.9915228  0.60211366 0.46960205\n",
      " 0.67410856 0.16953254 0.70718867 0.27437872 0.5174688  0.8290278 ]\n",
      "Step: 102 , Reward:  0.47414021971844367 weights:  [0.9247227  0.32041758 0.89975    0.9135761  0.64791584 0.4847267\n",
      " 0.9796405  0.22706196 0.03725016 0.05273253 0.80862427 0.8765631\n",
      " 0.10386014 0.6110534  0.45107472 0.30700588 0.17877668 0.73062354\n",
      " 0.04824957 0.5025277  0.1440652  0.1845546  0.7882217  0.5674309 ]\n",
      "Step: 103 , Reward:  0.4130126458016988 weights:  [0.9394689  0.81398183 0.6811198  0.64608276 0.65582925 0.9262656\n",
      " 0.09856012 0.35689977 0.99433994 0.20933786 0.3562572  0.9099345\n",
      " 0.8650471  0.97410166 0.59113747 0.59155625 0.9286647  0.03508443\n",
      " 0.88441885 0.99617654 0.92610335 0.9192519  0.1841144  0.65983975]\n",
      "Step: 104 , Reward:  0.47996485071756056 weights:  [0.06155732 0.6487405  0.56316787 0.01799312 0.25477874 0.63056874\n",
      " 0.39475274 0.42523494 0.06007251 0.7382432  0.10826051 0.10843572\n",
      " 0.10836577 0.30534983 0.9141845  0.28993773 0.6905984  0.3195731\n",
      " 0.3279628  0.6525774  0.9589841  0.39039478 0.6481336  0.34357944]\n",
      "Step: 105 , Reward:  0.5141376413115438 weights:  [0.49242955 0.7406861  0.45985126 0.49039847 0.22434092 0.0570935\n",
      " 0.98320246 0.36155236 0.9003025  0.7422927  0.89556    0.2212125\n",
      " 0.23710915 0.1309298  0.16659713 0.0284619  0.48181495 0.5482595\n",
      " 0.8445575  0.829914   0.98769355 0.90477103 0.29102457 0.5599669 ]\n",
      "Step: 106 , Reward:  0.32201573415887286 weights:  [0.30670065 0.06225955 0.4263327  0.99192584 0.7931303  0.36199892\n",
      " 0.7012882  0.8414643  0.20419767 0.21351618 0.9382951  0.39450216\n",
      " 0.7922934  0.40044883 0.59055114 0.07485625 0.20598292 0.46007442\n",
      " 0.11848804 0.7672528  0.5624439  0.76909703 0.5347367  0.9189989 ]\n",
      "Step: 107 , Reward:  0.253374762809198 weights:  [0.07111812 0.8992795  0.71806324 0.5456295  0.88060063 0.17191708\n",
      " 0.20772514 0.14200264 0.5459297  0.26630294 0.637003   0.516121\n",
      " 0.7618263  0.12629533 0.48232076 0.05858636 0.761748   0.8186748\n",
      " 0.7274201  0.32523954 0.5557282  0.6853293  0.85058165 0.4269698 ]\n",
      "Step: 108 , Reward:  -0.1961309804648054 weights:  [0.5884108  0.30397797 0.79633707 0.7141347  0.8883531  0.24992648\n",
      " 0.9910427  0.24685505 0.47123006 0.11790726 0.1650506  0.9608163\n",
      " 0.33310586 0.3697356  0.02820098 0.8997544  0.4830981  0.0915336\n",
      " 0.36482778 0.4683777  0.9545235  0.5790838  0.08146158 0.21239382]\n",
      "Step: 109 , Reward:  -0.12648161256787063 weights:  [0.42568272 0.01136395 0.10512295 0.7097202  0.37985596 0.7435081\n",
      " 0.4280374  0.16738558 0.30666357 0.7409999  0.16959217 0.7751111\n",
      " 0.3241676  0.50238436 0.2539769  0.47233215 0.4054606  0.19966727\n",
      " 0.7054364  0.2122561  0.7052652  0.33151004 0.5623074  0.36531967]\n",
      "Step: 110 , Reward:  -0.2156561821196253 weights:  [0.96509993 0.7084072  0.5853386  0.9468709  0.44072074 0.1431916\n",
      " 0.96534026 0.04335791 0.07472    0.28711492 0.3422013  0.6742061\n",
      " 0.01795724 0.43271562 0.93580675 0.8299233  0.12611017 0.50461394\n",
      " 0.6103255  0.25156537 0.9427638  0.5887392  0.23460627 0.4695465 ]\n",
      "Step: 111 , Reward:  -0.26380748323659314 weights:  [0.1481969  0.7331199  0.26559615 0.09581184 0.7401884  0.52840555\n",
      " 0.6981512  0.31590894 0.38758364 0.20136249 0.8863769  0.26435906\n",
      " 0.79330564 0.39715543 0.9502066  0.08177003 0.41467547 0.70045143\n",
      " 0.2680614  0.9802706  0.29973394 0.93050957 0.5867536  0.17756084]\n",
      "Step: 112 , Reward:  -0.4220058829636061 weights:  [0.9160722  0.6359729  0.47642422 0.45869908 0.11747158 0.30550945\n",
      " 0.60853726 0.07406142 0.42531517 0.8270944  0.9509909  0.14104769\n",
      " 0.35124484 0.10188311 0.5797571  0.48340803 0.6019979  0.16605452\n",
      " 0.79042614 0.51108825 0.30098873 0.8222054  0.6908893  0.41680652]\n",
      "Step: 113 , Reward:  -0.2714092259905526 weights:  [0.11908108 0.15210667 0.12981549 0.24407113 0.00392282 0.01303855\n",
      " 0.8955847  0.47501072 0.8719019  0.48180628 0.22496405 0.29347467\n",
      " 0.44619003 0.8122891  0.9748776  0.94801164 0.8012991  0.30296665\n",
      " 0.3065941  0.21629182 0.6621305  0.39402062 0.9762107  0.82003427]\n",
      "Step: 114 , Reward:  -0.28012068570937093 weights:  [0.9355992  0.12264192 0.08441988 0.74528545 0.39645585 0.11432481\n",
      " 0.03707823 0.4492818  0.94960016 0.9239316  0.8459502  0.2033152\n",
      " 0.11242935 0.770926   0.21378654 0.02346507 0.7948305  0.57767\n",
      " 0.03295276 0.9554117  0.97075975 0.36406073 0.8388064  0.31638813]\n",
      "Step: 115 , Reward:  -0.2822537516798136 weights:  [0.29878306 0.5123809  0.5033801  0.4664776  0.33108407 0.09374556\n",
      " 0.41114658 0.5437607  0.887602   0.76523757 0.11761126 0.91869426\n",
      " 0.2783169  0.969572   0.08067727 0.19358507 0.05101156 0.24648255\n",
      " 0.47552222 0.5247471  0.9886434  0.06369469 0.35445374 0.22557485]\n",
      "Step: 116 , Reward:  -0.5041701439571412 weights:  [0.7687698  0.3301003  0.8278663  0.7828982  0.76538503 0.5835054\n",
      " 0.39822066 0.6162383  0.6883549  0.7448124  0.12994564 0.9548764\n",
      " 0.6901381  0.83197653 0.24711844 0.6196188  0.75377023 0.65156513\n",
      " 0.4902413  0.97920465 0.3961287  0.09704673 0.9349513  0.19932848]\n",
      "Step: 117 , Reward:  -0.4935326212005736 weights:  [0.2173413  0.0803535  0.7885801  0.5247412  0.26307577 0.4573121\n",
      " 0.8109597  0.9559031  0.9891924  0.55804586 0.90486133 0.01471895\n",
      " 0.8799185  0.35149297 0.17503983 0.01420301 0.6379375  0.5485698\n",
      " 0.96732974 0.6266484  0.18825555 0.46378678 0.596497   0.42474747]\n",
      "Step: 118 , Reward:  -0.20193270444687844 weights:  [0.29506904 0.68341815 0.61110514 0.9173385  0.86058736 0.23631918\n",
      " 0.67185813 0.9350817  0.55873644 0.08224455 0.98685485 0.17333671\n",
      " 0.3703794  0.6291132  0.87281835 0.9473701  0.7735113  0.5763512\n",
      " 0.38548458 0.9869573  0.7516142  0.85640186 0.13051316 0.55069125]\n",
      "Step: 119 , Reward:  -0.5089910619336226 weights:  [0.869118   0.6601776  0.53638434 0.35883424 0.1302611  0.11599377\n",
      " 0.31842583 0.54578733 0.01132327 0.45748273 0.03378293 0.00790551\n",
      " 0.2578131  0.38719168 0.43548065 0.88700676 0.22583798 0.58757627\n",
      " 0.57912207 0.32331306 0.9274402  0.80273026 0.13007072 0.64142674]\n",
      "Step: 120 , Reward:  -0.5902936515860834 weights:  [0.51805866 0.6131021  0.6231131  0.7610422  0.5306619  0.1552782\n",
      " 0.05595368 0.96505344 0.06824988 0.29588258 0.6079259  0.24084997\n",
      " 0.6837574  0.27518398 0.08307838 0.01467788 0.9380786  0.68765855\n",
      " 0.80387545 0.83742106 0.16292796 0.6547803  0.76121354 0.29655015]\n",
      "Step: 121 , Reward:  -0.6715691388279315 weights:  [0.23611417 0.69343674 0.9010537  0.9649586  0.9136307  0.7426878\n",
      " 0.09951374 0.73201716 0.9593972  0.3411969  0.16083854 0.9345378\n",
      " 0.7224196  0.3140645  0.818693   0.25119624 0.34027547 0.39374968\n",
      " 0.23309788 0.76840436 0.99456775 0.5430989  0.4387176  0.88650966]\n",
      "Step: 122 , Reward:  -0.5492783155722649 weights:  [0.6042575  0.51474154 0.8139498  0.68852717 0.28289694 0.41916066\n",
      " 0.5396548  0.1890623  0.79292774 0.33898467 0.23792845 0.66243565\n",
      " 0.6525633  0.3603081  0.19261321 0.00534716 0.643107   0.5518229\n",
      " 0.06136087 0.23061204 0.51327443 0.7097345  0.2820663  0.9087815 ]\n",
      "Step: 123 , Reward:  -0.28982882912977276 weights:  [0.6236178  0.73319495 0.4472812  0.27805066 0.91630864 0.84248173\n",
      " 0.35809445 0.10564762 0.3037387  0.8698148  0.24948442 0.21341813\n",
      " 0.15884405 0.85076    0.9810888  0.5630939  0.06017441 0.6800748\n",
      " 0.44247568 0.36728477 0.52403754 0.8431879  0.01463699 0.8364635 ]\n",
      "Step: 124 , Reward:  -0.34416163584976406 weights:  [0.65851927 0.34028473 0.07978755 0.78496337 0.77086604 0.5924551\n",
      " 0.7007196  0.22240493 0.92673314 0.11033255 0.7572756  0.86185306\n",
      " 0.13546091 0.80211294 0.7627101  0.00376904 0.8824976  0.2807144\n",
      " 0.50123024 0.66748524 0.80029863 0.5168715  0.63711864 0.26387596]\n",
      "Step: 125 , Reward:  -0.4411449776529006 weights:  [0.5307007  0.04446143 0.4576014  0.75809085 0.37254906 0.1041773\n",
      " 0.97585136 0.4089266  0.4583351  0.71719074 0.33020103 0.4211575\n",
      " 0.0833126  0.98119426 0.40168238 0.01273435 0.55663943 0.84760875\n",
      " 0.77715504 0.73523045 0.80586517 0.43432808 0.44719994 0.1665627 ]\n",
      "Step: 126 , Reward:  -0.40959686335231377 weights:  [0.25292674 0.5081991  0.20917374 0.0492166  0.10673991 0.88462234\n",
      " 0.65534115 0.92249125 0.76974833 0.17245516 0.0821687  0.49398735\n",
      " 0.2329934  0.6369012  0.1354453  0.8413397  0.9252951  0.71088916\n",
      " 0.16494155 0.16211578 0.62232757 0.8248506  0.29490036 0.8853888 ]\n",
      "Step: 127 , Reward:  -0.3190720359756779 weights:  [0.44181544 0.7332601  0.38711822 0.14096415 0.16410977 0.979551\n",
      " 0.4267863  0.397448   0.08694348 0.5089997  0.35696137 0.18237397\n",
      " 0.9939402  0.5093722  0.15029761 0.3292602  0.6097802  0.25038707\n",
      " 0.3517368  0.47732294 0.63562465 0.03612986 0.04586768 0.46408588]\n",
      "Step: 128 , Reward:  -0.27760342787853537 weights:  [0.8337263  0.5823434  0.18467915 0.41450047 0.655949   0.590506\n",
      " 0.48875746 0.8417077  0.79046345 0.26892436 0.13274321 0.08502001\n",
      " 0.43253937 0.6217789  0.03977898 0.03408986 0.21522689 0.4300425\n",
      " 0.6908368  0.4306171  0.6425034  0.98117614 0.8361582  0.07840112]\n",
      "Step: 129 , Reward:  -0.41679393304699863 weights:  [0.9705448  0.23751858 0.7635733  0.04393473 0.3208171  0.06261256\n",
      " 0.25091255 0.8579081  0.7502108  0.9989196  0.20032471 0.32180876\n",
      " 0.7272573  0.0668245  0.57425046 0.41950268 0.04391873 0.90524036\n",
      " 0.50018644 0.41981673 0.19780302 0.86971295 0.52818316 0.06997123]\n",
      "Step: 130 , Reward:  -0.2672460914544636 weights:  [0.48406932 0.5891783  0.18537527 0.9361339  0.9299296  0.07676446\n",
      " 0.6834986  0.56853575 0.57692283 0.16117781 0.7481543  0.14646584\n",
      " 0.22170177 0.541877   0.01049197 0.28105938 0.24988395 0.99497473\n",
      " 0.22437572 0.12541413 0.4710288  0.90134096 0.05892286 0.20455778]\n",
      "Step: 131 , Reward:  -0.34493232759576453 weights:  [0.60844165 0.23032656 0.9263798  0.31793422 0.32100034 0.23189071\n",
      " 0.6722498  0.27619314 0.77698946 0.46311283 0.02988994 0.9244474\n",
      " 0.6795713  0.6394097  0.05514872 0.91033953 0.8929279  0.45825884\n",
      " 0.96531737 0.51625353 0.96242297 0.11195669 0.73824775 0.072909  ]\n",
      "Step: 132 , Reward:  -0.30740948690263414 weights:  [0.8536814  0.950233   0.640003   0.37637943 0.6400888  0.6483633\n",
      " 0.3716725  0.11095515 0.6714943  0.5497203  0.41910142 0.9631182\n",
      " 0.2888611  0.58069    0.5787566  0.5251572  0.52945054 0.650751\n",
      " 0.16790819 0.0387578  0.9227209  0.2591563  0.5117923  0.7000683 ]\n",
      "Step: 133 , Reward:  -0.4673162123369378 weights:  [0.20590493 0.67004466 0.72682714 0.9396292  0.35419297 0.72669375\n",
      " 0.68298125 0.51919895 0.6792402  0.34581268 0.5039717  0.11553925\n",
      " 0.88251686 0.12484828 0.9083906  0.31739745 0.11673436 0.7675569\n",
      " 0.45229918 0.45953262 0.01170844 0.6556729  0.1774689  0.78536457]\n",
      "Step: 134 , Reward:  -0.3181920497398594 weights:  [0.014072   0.48992866 0.14988938 0.7860542  0.07491553 0.47474682\n",
      " 0.7411814  0.391514   0.7066387  0.32689402 0.09941432 0.86024183\n",
      " 0.56263626 0.30376184 0.33274508 0.658372   0.26000518 0.73850024\n",
      " 0.6627972  0.604575   0.00539622 0.95469236 0.06654    0.05481312]\n",
      "Step: 135 , Reward:  -0.7002747603710561 weights:  [0.49659207 0.95011115 0.7095182  0.16321784 0.26591665 0.8927616\n",
      " 0.98984337 0.702912   0.17013422 0.45287937 0.06368321 0.16908357\n",
      " 0.14860827 0.24961874 0.10121125 0.4121735  0.08225229 0.66498035\n",
      " 0.6032001  0.7127907  0.9925569  0.7192287  0.59615725 0.31399918]\n",
      "Step: 136 , Reward:  -0.47625469088265626 weights:  [0.8286375  0.86916286 0.8397783  0.5812531  0.8369378  0.2032175\n",
      " 0.8974881  0.14511767 0.5887938  0.33821377 0.5571179  0.41590354\n",
      " 0.4854984  0.36145633 0.6289983  0.4236067  0.8134996  0.7624531\n",
      " 0.70629793 0.5471773  0.51623684 0.03178623 0.5562219  0.13608038]\n",
      "Step: 137 , Reward:  -0.783183937130385 weights:  [0.24045935 0.0105629  0.6480832  0.07060426 0.29388934 0.1607391\n",
      " 0.08620307 0.77891135 0.5008834  0.00467747 0.038488   0.76817167\n",
      " 0.21385106 0.97171867 0.20500526 0.06110454 0.84494114 0.19892383\n",
      " 0.7921077  0.54185206 0.9933781  0.7956039  0.06098858 0.88804793]\n",
      "Step: 138 , Reward:  -0.5540144011284805 weights:  [0.9768057  0.63282454 0.5313721  0.5269718  0.08429179 0.67251855\n",
      " 0.31387174 0.5573907  0.46470797 0.35072273 0.34681118 0.01588982\n",
      " 0.37090385 0.22949511 0.6433258  0.87096226 0.5141113  0.9103214\n",
      " 0.03153077 0.7406992  0.15144086 0.6015973  0.49586293 0.2576869 ]\n",
      "Step: 139 , Reward:  -0.688480359059186 weights:  [0.20569214 0.94821966 0.62795687 0.4399313  0.05516857 0.768198\n",
      " 0.34685832 0.4871361  0.36654672 0.5813538  0.93176913 0.71174264\n",
      " 0.8622068  0.8242639  0.13870645 0.39150426 0.878958   0.89752316\n",
      " 0.942847   0.06519806 0.23248208 0.17735672 0.17967871 0.52981687]\n",
      "Step: 140 , Reward:  -0.39673754466810573 weights:  [0.03284672 0.32752287 0.17044285 0.5780368  0.47897714 0.18288761\n",
      " 0.6260242  0.9244383  0.58429277 0.44934565 0.4399242  0.35497952\n",
      " 0.5725921  0.06250775 0.2934033  0.16861421 0.32077348 0.527531\n",
      " 0.15432411 0.09078872 0.6856438  0.23999932 0.5618028  0.21575418]\n",
      "Step: 141 , Reward:  -0.32574377491804757 weights:  [0.07116526 0.66182095 0.8673054  0.7215847  0.69282645 0.46772867\n",
      " 0.97666454 0.07324973 0.81416976 0.19265181 0.22171113 0.6089736\n",
      " 0.98511845 0.7350296  0.38983774 0.01328647 0.22558635 0.15135878\n",
      " 0.8942597  0.76753116 0.9367809  0.1859338  0.9112276  0.37732276]\n",
      "Step: 142 , Reward:  -0.2594360650067007 weights:  [0.03517574 0.44146234 0.41219205 0.7750548  0.41501525 0.33895105\n",
      " 0.16656244 0.6081947  0.15166241 0.9492644  0.6400597  0.9750936\n",
      " 0.84280777 0.6556624  0.15626645 0.38012856 0.7917846  0.66209257\n",
      " 0.567719   0.9276341  0.57428706 0.22455376 0.4730047  0.44291866]\n",
      "Step: 143 , Reward:  -0.11991049935311177 weights:  [0.16719839 0.6629429  0.34580207 0.62197566 0.68569505 0.6882609\n",
      " 0.38300687 0.6105642  0.74285537 0.8569815  0.52443045 0.40616128\n",
      " 0.07869145 0.36091372 0.33108813 0.589786   0.8596252  0.11001799\n",
      " 0.32550597 0.1939629  0.5618908  0.34876463 0.57776237 0.1333111 ]\n",
      "Step: 144 , Reward:  -0.008423310799052904 weights:  [0.34143028 0.9851157  0.01310566 0.97048247 0.06352311 0.39544928\n",
      " 0.58300203 0.5375233  0.7851914  0.20246324 0.7885269  0.8685826\n",
      " 0.00642732 0.71975905 0.0624797  0.5867963  0.6178413  0.04946014\n",
      " 0.4898306  0.10162154 0.25926006 0.06580049 0.09660584 0.68360406]\n",
      "Step: 145 , Reward:  -0.0035916714957023015 weights:  [0.08410743 0.05818638 0.8117543  0.16260934 0.32140058 0.03500456\n",
      " 0.6750129  0.85648835 0.47742623 0.6882608  0.8637655  0.05353352\n",
      " 0.06937182 0.9085555  0.37711138 0.23826087 0.31507087 0.11787435\n",
      " 0.13437569 0.61831903 0.4781484  0.14945236 0.8717568  0.28779972]\n",
      "Step: 146 , Reward:  0.021654486104266714 weights:  [0.10538045 0.27751142 0.42007205 0.7423018  0.02582955 0.16139108\n",
      " 0.5653476  0.92464364 0.9959398  0.77322334 0.98796517 0.06781277\n",
      " 0.937672   0.8476455  0.32174343 0.67393416 0.87557805 0.20544788\n",
      " 0.9420783  0.98899424 0.3263912  0.9955117  0.56406516 0.95479095]\n",
      "Step: 147 , Reward:  -0.08521415980738602 weights:  [0.32013148 0.3859918  0.15770113 0.35196015 0.14846057 0.11287796\n",
      " 0.75240314 0.43713105 0.9167039  0.05876699 0.9402157  0.04374987\n",
      " 0.80883205 0.6730937  0.237237   0.514135   0.98338914 0.4497399\n",
      " 0.03698203 0.56468415 0.04458392 0.41050208 0.15543497 0.6690003 ]\n",
      "Step: 148 , Reward:  -0.21971926360284202 weights:  [0.76032937 0.73979956 0.19860569 0.59283364 0.0886988  0.08601847\n",
      " 0.5173541  0.93732333 0.7118724  0.9161562  0.5138457  0.4494888\n",
      " 0.766398   0.844301   0.09009948 0.7897303  0.44438693 0.9577645\n",
      " 0.73220503 0.7600608  0.6484147  0.27923346 0.964219   0.3805232 ]\n",
      "Step: 149 , Reward:  -0.40980310858747404 weights:  [0.19440451 0.5552685  0.96877193 0.5783467  0.8530135  0.47594872\n",
      " 0.7132884  0.3430513  0.6935024  0.92053485 0.5813475  0.10291812\n",
      " 0.94739723 0.4805679  0.05262816 0.80058914 0.6484712  0.1269607\n",
      " 0.27095    0.05255902 0.26999307 0.6597359  0.46710914 0.7065857 ]\n",
      "Step: 150 , Reward:  -0.3398402662494905 weights:  [0.15283269 0.10188261 0.7462878  0.5000891  0.64684314 0.51896435\n",
      " 0.4903237  0.14536858 0.2680566  0.13387972 0.63815415 0.08813331\n",
      " 0.14858186 0.5164001  0.50527686 0.7596389  0.0388549  0.15222257\n",
      " 0.21227792 0.98590827 0.02043146 0.2591563  0.01693478 0.71496   ]\n",
      "Step: 151 , Reward:  -0.25011585495989586 weights:  [0.56882626 0.3399514  0.9213178  0.21574992 0.20032135 0.91250384\n",
      " 0.32296443 0.70282626 0.53435    0.6403587  0.1224449  0.52560157\n",
      " 0.06193525 0.97906095 0.32014465 0.5653851  0.59877706 0.13406372\n",
      " 0.93212235 0.6595602  0.08578491 0.25356472 0.16834381 0.27980042]\n",
      "Step: 152 , Reward:  0.08187836216458755 weights:  [0.01479658 0.23606822 0.7174956  0.2172564  0.861713   0.38365903\n",
      " 0.7825951  0.35148808 0.09818381 0.69476765 0.49980155 0.02032685\n",
      " 0.05755705 0.8973448  0.80325705 0.6329262  0.04852885 0.96865976\n",
      " 0.5839306  0.90039384 0.20181355 0.06483132 0.84136355 0.0368925 ]\n",
      "Step: 153 , Reward:  -0.23701309122240885 weights:  [0.26445448 0.73512435 0.47592038 0.9193251  0.8597053  0.40780675\n",
      " 0.9533644  0.6629564  0.919043   0.1585578  0.47548386 0.57225746\n",
      " 0.46218875 0.87005496 0.08039069 0.9401672  0.10459721 0.6391863\n",
      " 0.96156406 0.98252445 0.8774589  0.82345843 0.8925942  0.35129982]\n",
      "Step: 154 , Reward:  -0.16692330945037562 weights:  [0.0791522  0.689444   0.67473525 0.11398065 0.01347244 0.1912334\n",
      " 0.27037826 0.9345305  0.7705473  0.9049964  0.68382645 0.63464224\n",
      " 0.77449954 0.2427884  0.17313054 0.2827366  0.6649078  0.6896728\n",
      " 0.76064324 0.61320287 0.46817282 0.9774581  0.65230834 0.55817235]\n",
      "Step: 155 , Reward:  -0.4513646344664136 weights:  [0.79563826 0.21104711 0.8963523  0.18228456 0.9272718  0.5652227\n",
      " 0.52811074 0.14003503 0.6520256  0.53775007 0.9774438  0.34464633\n",
      " 0.37451732 0.27052045 0.67357314 0.12776667 0.903667   0.9946771\n",
      " 0.84770846 0.29698724 0.93872285 0.4448822  0.5070098  0.6071191 ]\n",
      "Step: 156 , Reward:  -0.3841107619895117 weights:  [0.05733985 0.02455956 0.36433208 0.50095165 0.8106445  0.54911166\n",
      " 0.85341907 0.6552563  0.927688   0.5844919  0.5114417  0.58028996\n",
      " 0.74822    0.00326782 0.18137103 0.06880167 0.4399424  0.6841517\n",
      " 0.38822144 0.28100637 0.29002064 0.8584409  0.04451793 0.8134466 ]\n",
      "Step: 157 , Reward:  -0.24814450095755503 weights:  [0.20288461 0.66960883 0.04165223 0.07523435 0.7239584  0.6811087\n",
      " 0.9476355  0.30490428 0.2026884  0.48842734 0.47170857 0.9860616\n",
      " 0.9500482  0.98707765 0.80777144 0.20871061 0.8826982  0.8027344\n",
      " 0.18680266 0.43740857 0.769673   0.9049995  0.48772404 0.00447252]\n",
      "Step: 158 , Reward:  -0.21302521414716472 weights:  [0.25044668 0.46460417 0.20031154 0.71781075 0.5249361  0.8413036\n",
      " 0.02926368 0.11173746 0.19066    0.03165096 0.37538362 0.18391806\n",
      " 0.31067306 0.8698534  0.951838   0.2719257  0.6475706  0.04813129\n",
      " 0.16401103 0.2513281  0.07915491 0.13435632 0.8181929  0.9518821 ]\n",
      "Step: 159 , Reward:  -0.46027709482695467 weights:  [0.04947782 0.462698   0.15832007 0.6084956  0.3917096  0.8544755\n",
      " 0.4613847  0.5220096  0.825137   0.69223166 0.83743954 0.9737594\n",
      " 0.28416854 0.1368452  0.09233668 0.8142766  0.17481172 0.33758837\n",
      " 0.06015801 0.46396062 0.80468774 0.47796795 0.38641262 0.9535279 ]\n",
      "Step: 160 , Reward:  -0.4107453023042765 weights:  [0.45721766 0.7045423  0.53950036 0.8985102  0.26270753 0.94347036\n",
      " 0.02703241 0.09387589 0.3880797  0.33037597 0.396891   0.21853498\n",
      " 0.9516784  0.5377312  0.62564856 0.6993362  0.46895295 0.5953632\n",
      " 0.40505648 0.98695123 0.5021188  0.32146895 0.7470646  0.05099803]\n",
      "Step: 161 , Reward:  -0.2516775934384813 weights:  [0.517524   0.02007657 0.82873166 0.28865415 0.8596251  0.1285775\n",
      " 0.05397424 0.46139678 0.857589   0.01567069 0.5438503  0.97314095\n",
      " 0.320929   0.44006774 0.11132184 0.5883043  0.6563926  0.44775426\n",
      " 0.8680508  0.01652402 0.77048707 0.6681981  0.4178357  0.7397548 ]\n",
      "Step: 162 , Reward:  -0.14591967400417352 weights:  [0.9712701  0.47877187 0.8979212  0.40171814 0.16400585 0.35723817\n",
      " 0.76289386 0.87776196 0.65437704 0.9164499  0.91485727 0.2832514\n",
      " 0.44174737 0.33951062 0.5592692  0.4019031  0.23479718 0.4139422\n",
      " 0.83478355 0.74308455 0.26279545 0.9868077  0.7526899  0.2536723 ]\n",
      "Step: 163 , Reward:  0.04809153674473199 weights:  [0.11985505 0.10170293 0.23516029 0.7324456  0.5333756  0.7910052\n",
      " 0.2675539  0.80251443 0.58803165 0.23190606 0.82908773 0.6754804\n",
      " 0.3135054  0.05970454 0.04572541 0.92575663 0.4602378  0.45888975\n",
      " 0.0599362  0.3051108  0.9893509  0.36826998 0.9029523  0.78248036]\n",
      "Step: 164 , Reward:  0.5080995082089845 weights:  [0.19780338 0.3513456  0.15662324 0.547441   0.5546502  0.72003865\n",
      " 0.4218107  0.9252439  0.9073205  0.08723029 0.2162717  0.5695209\n",
      " 0.19384503 0.90218735 0.17872638 0.1118215  0.9865181  0.875388\n",
      " 0.18625507 0.13057369 0.38149393 0.7846485  0.8138107  0.5752516 ]\n",
      "Step: 165 , Reward:  0.8219638298411779 weights:  [0.6779706  0.24530393 0.22817144 0.4935936  0.6963223  0.21100655\n",
      " 0.1958004  0.03430709 0.8843367  0.7115243  0.9663735  0.19866353\n",
      " 0.8737986  0.10566854 0.3737012  0.7487694  0.46247524 0.24147508\n",
      " 0.5738934  0.36011833 0.85944533 0.69937885 0.30676043 0.6064099 ]\n",
      "Step: 166 , Reward:  0.6854521859784584 weights:  [0.17748916 0.8493432  0.18484622 0.5796126  0.41652635 0.06123221\n",
      " 0.47354224 0.7738768  0.38701552 0.26118237 0.7954858  0.5987187\n",
      " 0.02604315 0.14180803 0.54261094 0.7764218  0.6100862  0.00473443\n",
      " 0.04809961 0.91636276 0.33469266 0.5816604  0.99756026 0.29029387]\n",
      "Step: 167 , Reward:  0.7023751406302332 weights:  [0.4965884  0.81468827 0.4854243  0.60812706 0.42157483 0.04922929\n",
      " 0.83854294 0.7299851  0.65411186 0.08325225 0.16848397 0.82526124\n",
      " 0.24965921 0.1949758  0.98736954 0.07318184 0.7751113  0.4448934\n",
      " 0.31502154 0.30665576 0.53307515 0.45974955 0.93055296 0.9712632 ]\n",
      "Step: 168 , Reward:  0.9587162411682465 weights:  [0.93066204 0.4479015  0.3969754  0.03728378 0.6251634  0.6713969\n",
      " 0.5935895  0.327168   0.14562917 0.58498013 0.6269185  0.07824716\n",
      " 0.955924   0.32555714 0.01022485 0.9600009  0.18751383 0.0248026\n",
      " 0.5923317  0.1415666  0.86477506 0.51005095 0.72633123 0.8765279 ]\n",
      "Step: 169 , Reward:  1.060861273890195 weights:  [0.25574246 0.25789803 0.59877026 0.68140006 0.29834682 0.7377187\n",
      " 0.631796   0.8789829  0.925053   0.45360953 0.08604637 0.9410341\n",
      " 0.18035531 0.32003814 0.576346   0.09115645 0.4886022  0.61514086\n",
      " 0.74601156 0.04321694 0.5874646  0.48279566 0.07807568 0.77530056]\n",
      "Step: 170 , Reward:  1.2355609437166963 weights:  [0.03026718 0.2944116  0.18056127 0.6284331  0.09084442 0.5192468\n",
      " 0.05742913 0.27533472 0.544336   0.7878941  0.02883887 0.5574337\n",
      " 0.8758667  0.3064257  0.1948911  0.02464321 0.7250216  0.9785137\n",
      " 0.59277457 0.86303926 0.2047132  0.6420798  0.07673544 0.80466807]\n",
      "Step: 171 , Reward:  1.1956516992266408 weights:  [0.902542   0.27304018 0.41797042 0.6639288  0.9827415  0.64405787\n",
      " 0.19884285 0.12904865 0.6245253  0.12250122 0.81938815 0.9885603\n",
      " 0.9486395  0.25741482 0.18582943 0.5543963  0.6479935  0.53958565\n",
      " 0.90125006 0.789642   0.35709623 0.6318313  0.89306605 0.16238737]\n",
      "Step: 172 , Reward:  1.055377192454024 weights:  [0.02703705 0.10454983 0.44940075 0.98367476 0.4341597  0.06637815\n",
      " 0.18153751 0.95306337 0.5798346  0.79894567 0.93141425 0.4253726\n",
      " 0.88750654 0.8387295  0.7891314  0.0382399  0.6278553  0.32495087\n",
      " 0.44044274 0.08751252 0.38270515 0.6196877  0.56861174 0.79053766]\n",
      "Step: 173 , Reward:  0.9466077245952572 weights:  [0.85898364 0.10354578 0.79204667 0.1247887  0.709948   0.04489633\n",
      " 0.7344049  0.31271243 0.5950431  0.9049791  0.5142885  0.16371602\n",
      " 0.32765982 0.515437   0.62539816 0.7674457  0.56660306 0.5597376\n",
      " 0.62002474 0.1032131  0.11592561 0.9106784  0.69042057 0.9675238 ]\n",
      "Step: 174 , Reward:  1.433043831692396 weights:  [0.8650017  0.4276393  0.19517756 0.41010943 0.08240545 0.43836546\n",
      " 0.07174659 0.53584903 0.7619016  0.89384055 0.02239746 0.66112256\n",
      " 0.08497667 0.7117772  0.04037523 0.02184519 0.83932614 0.10442498\n",
      " 0.03000623 0.6508361  0.25293204 0.3262215  0.16845357 0.6189407 ]\n",
      "Step: 175 , Reward:  1.4575573577247272 weights:  [0.56526774 0.1180906  0.70463526 0.33693406 0.88127613 0.25078064\n",
      " 0.898633   0.14370942 0.436841   0.07424197 0.5962692  0.8985431\n",
      " 0.04284698 0.8541888  0.14519575 0.2609108  0.85653293 0.5783618\n",
      " 0.70885384 0.660114   0.1876553  0.14758864 0.98394287 0.15050784]\n",
      "Step: 176 , Reward:  1.4299302275191712 weights:  [0.20610482 0.5614175  0.40763026 0.94126666 0.11814055 0.62877834\n",
      " 0.26925722 0.54434943 0.15279144 0.8610204  0.9615328  0.9290747\n",
      " 0.81581676 0.02655342 0.6980537  0.3970083  0.79862434 0.82828104\n",
      " 0.28594047 0.91547716 0.8862821  0.42621797 0.3341452  0.92544067]\n",
      "Step: 177 , Reward:  1.538695786956296 weights:  [0.23153216 0.15990046 0.47051266 0.2863319  0.44772238 0.64556223\n",
      " 0.6469139  0.17159984 0.7809155  0.43920052 0.19976452 0.33420378\n",
      " 0.31050828 0.49948192 0.07513478 0.80408585 0.7680385  0.5739944\n",
      " 0.21448362 0.19377634 0.7662008  0.27766222 0.41690257 0.24182758]\n",
      "Step: 178 , Reward:  1.5255896441252375 weights:  [0.27739665 0.5364521  0.95221233 0.99547803 0.7099171  0.97744405\n",
      " 0.77453804 0.47575936 0.11684197 0.09850693 0.05417967 0.6751745\n",
      " 0.32539165 0.64458525 0.23428717 0.4069873  0.17967826 0.31144297\n",
      " 0.61670107 0.7242612  0.8194737  0.14212978 0.04391718 0.49858752]\n",
      "Step: 179 , Reward:  1.3887410470845074 weights:  [0.8387755  0.56450117 0.9702475  0.82476634 0.9471849  0.6096302\n",
      " 0.29147154 0.25541312 0.89013785 0.573646   0.1369898  0.5278855\n",
      " 0.3769654  0.64051723 0.5056147  0.03913179 0.09103683 0.40829086\n",
      " 0.64985204 0.91191196 0.12968534 0.97518003 0.66244966 0.51684976]\n",
      "Step: 180 , Reward:  1.4494460393183501 weights:  [0.08984935 0.8852509  0.23272416 0.67229897 0.060123   0.52886987\n",
      " 0.7747586  0.53748864 0.34914124 0.45489952 0.49464563 0.9626602\n",
      " 0.4184363  0.63880074 0.834509   0.5233801  0.6596894  0.78965783\n",
      " 0.20718747 0.89035726 0.76790524 0.912528   0.21268004 0.5567829 ]\n",
      "Step: 181 , Reward:  1.356384520790716 weights:  [0.42744833 0.9155314  0.9509877  0.07394573 0.82220095 0.7975735\n",
      " 0.83762    0.9636855  0.49742103 0.2603046  0.20023361 0.42760995\n",
      " 0.67080474 0.68653303 0.3950358  0.71822715 0.8517246  0.208255\n",
      " 0.6842931  0.8759327  0.19232953 0.41796786 0.42452797 0.14447686]\n",
      "Step: 182 , Reward:  1.367073339362643 weights:  [0.49563965 0.83970594 0.45076713 0.9359968  0.69527906 0.25421745\n",
      " 0.9128749  0.5974666  0.09684965 0.770017   0.56100947 0.21431446\n",
      " 0.35193622 0.2946509  0.0593375  0.8286977  0.60416424 0.95790005\n",
      " 0.13439444 0.09313914 0.6588594  0.9435476  0.37212712 0.3147329 ]\n",
      "Step: 183 , Reward:  1.7796717228664407 weights:  [0.89569765 0.83866805 0.1928908  0.744683   0.2832132  0.34823185\n",
      " 0.2171419  0.9538331  0.9771539  0.08487868 0.40556726 0.53945637\n",
      " 0.17501107 0.28244197 0.64144504 0.6283358  0.02689075 0.2529332\n",
      " 0.55094963 0.8268044  0.63255024 0.82224107 0.06871358 0.7167841 ]\n",
      "Step: 184 , Reward:  1.641041868430357 weights:  [0.26629108 0.9583556  0.25491923 0.9708694  0.02518141 0.06575039\n",
      " 0.25434    0.6130271  0.17230994 0.35103315 0.9957497  0.38511342\n",
      " 0.5960622  0.6983991  0.23197538 0.61366755 0.8665712  0.19707805\n",
      " 0.13094884 0.8967101  0.11418921 0.8717817  0.26348102 0.36275297]\n",
      "Step: 185 , Reward:  1.8506022771951463 weights:  [0.03433004 0.38540757 0.09117508 0.9446496  0.06690228 0.68865657\n",
      " 0.3870004  0.6834518  0.56233656 0.37056178 0.39132047 0.5470708\n",
      " 0.15406293 0.47012419 0.9791279  0.17909843 0.54463255 0.17470512\n",
      " 0.35691124 0.7049474  0.19211999 0.00199175 0.62332785 0.5864504 ]\n",
      "Step: 186 , Reward:  1.7303752454696888 weights:  [0.7532226  0.9632118  0.47731167 0.9036155  0.77039266 0.7500558\n",
      " 0.03747675 0.49041986 0.09939882 0.914482   0.1463547  0.8412045\n",
      " 0.9204813  0.9337415  0.94083726 0.69768506 0.40307015 0.66370213\n",
      " 0.05012903 0.9444214  0.9811771  0.24180278 0.31361228 0.13743058]\n",
      "Step: 187 , Reward:  1.8179605557630945 weights:  [0.14754903 0.8360082  0.5801794  0.81124395 0.23817956 0.7503489\n",
      " 0.47365332 0.5378122  0.5066041  0.9140444  0.8777478  0.35415354\n",
      " 0.56195563 0.8671765  0.8547082  0.3788284  0.15713963 0.8960736\n",
      " 0.43577337 0.45305842 0.74256325 0.37539473 0.1890037  0.20292005]\n",
      "Step: 188 , Reward:  1.8049855561632655 weights:  [0.1504763  0.26992422 0.71491456 0.27507913 0.06143558 0.48096862\n",
      " 0.61844856 0.29320604 0.59155065 0.65937996 0.985449   0.69767606\n",
      " 0.10652676 0.87260646 0.19596869 0.06949762 0.4071486  0.74054766\n",
      " 0.22976765 0.04135397 0.157585   0.9070324  0.5190589  0.40806273]\n",
      "Step: 189 , Reward:  2.017822265100834 weights:  [0.11601698 0.39580864 0.8886795  0.07227907 0.8056071  0.698327\n",
      " 0.28402275 0.28311032 0.05863205 0.27123833 0.29250968 0.53617555\n",
      " 0.86709535 0.7621131  0.32408327 0.07791811 0.0139603  0.8400932\n",
      " 0.70301497 0.66778576 0.8384401  0.8701838  0.07928884 0.6269613 ]\n",
      "Step: 190 , Reward:  1.9715612351368066 weights:  [0.5586411  0.92522097 0.46434748 0.61414367 0.16310745 0.5453229\n",
      " 0.9936079  0.09142911 0.3721972  0.23073909 0.26672918 0.6259074\n",
      " 0.6485807  0.87597036 0.01288387 0.81959486 0.6166733  0.7427411\n",
      " 0.89106655 0.885552   0.15345064 0.50725216 0.88440967 0.4786573 ]\n",
      "Step: 191 , Reward:  1.792387506901972 weights:  [0.5789892  0.75302994 0.09209228 0.5966409  0.28102913 0.04275006\n",
      " 0.39247337 0.79472363 0.6840613  0.3061484  0.7812936  0.27457595\n",
      " 0.9839383  0.06618333 0.31726468 0.3750633  0.74775726 0.82102525\n",
      " 0.9565436  0.6164278  0.23601177 0.59050274 0.8231164  0.85590935]\n",
      "Step: 192 , Reward:  1.5750209221473093 weights:  [0.04348227 0.9250016  0.30841732 0.8392032  0.06374463 0.07755405\n",
      " 0.2768557  0.31568623 0.33324057 0.8062955  0.32637435 0.61187834\n",
      " 0.49680462 0.1014393  0.6689737  0.9794222  0.4323877  0.29264417\n",
      " 0.15356433 0.541351   0.92070746 0.6619708  0.9921601  0.43464065]\n",
      "Step: 193 , Reward:  1.2734846610520456 weights:  [0.5149809  0.0986014  0.84682024 0.6492673  0.3118468  0.03190008\n",
      " 0.6556844  0.8123652  0.59855723 0.7907704  0.66554457 0.92579204\n",
      " 0.7621925  0.69459367 0.91052985 0.3665092  0.6847391  0.26466143\n",
      " 0.04092905 0.4249807  0.86148643 0.26953405 0.3418153  0.2367545 ]\n",
      "Step: 194 , Reward:  1.5483745401472928 weights:  [0.34224108 0.1910645  0.6522863  0.21848911 0.40924832 0.5321054\n",
      " 0.7115073  0.3026678  0.11040875 0.89228487 0.08495757 0.9944012\n",
      " 0.86192787 0.82250535 0.30997342 0.02524486 0.43811437 0.91971374\n",
      " 0.3620233  0.09260482 0.06503829 0.23843116 0.35540986 0.8801055 ]\n",
      "Step: 195 , Reward:  1.515525867215698 weights:  [0.02663264 0.74276745 0.47588438 0.8663535  0.78035975 0.47054607\n",
      " 0.21067452 0.03704897 0.42745793 0.58818007 0.83434916 0.9199829\n",
      " 0.35708302 0.6336504  0.90211034 0.24287811 0.17173639 0.37192264\n",
      " 0.6640394  0.11711562 0.42994052 0.38163513 0.00162449 0.41280264]\n",
      "Step: 196 , Reward:  1.7083077819463746 weights:  [0.20155871 0.3568613  0.81344634 0.4363433  0.91570055 0.9448184\n",
      " 0.52786636 0.47433782 0.02314246 0.5860682  0.69225603 0.90373343\n",
      " 0.27550632 0.02930987 0.12020144 0.9378282  0.8831594  0.8406985\n",
      " 0.42529872 0.8127269  0.8692384  0.32354444 0.36520505 0.5388103 ]\n",
      "Step: 197 , Reward:  1.5597299715873243 weights:  [0.2952463  0.44637504 0.28317055 0.8319247  0.07319456 0.9941908\n",
      " 0.9859803  0.97662497 0.6695055  0.6343646  0.0093925  0.3839051\n",
      " 0.5120208  0.8922112  0.28562677 0.9806072  0.32018057 0.8150856\n",
      " 0.5885194  0.6113641  0.8621134  0.6190318  0.13020003 0.5283906 ]\n",
      "Step: 198 , Reward:  1.4271925855888534 weights:  [0.9545227  0.51028895 0.07465553 0.94598234 0.4497937  0.522106\n",
      " 0.67516303 0.5755173  0.33333617 0.6316204  0.47733346 0.06033227\n",
      " 0.44811144 0.01333335 0.9046628  0.07345736 0.2548016  0.13750169\n",
      " 0.7353307  0.39298797 0.92583156 0.8780558  0.9322834  0.54054105]\n",
      "Step: 199 , Reward:  1.2198868214095975 weights:  [0.7360471  0.6188729  0.6655011  0.09592462 0.63587326 0.39051437\n",
      " 0.4554733  0.51962644 0.36175936 0.23793691 0.81850433 0.38548726\n",
      " 0.60348046 0.18766633 0.00998428 0.16678539 0.64239305 0.8391813\n",
      " 0.48585972 0.05722895 0.59794533 0.74942195 0.310183   0.3307867 ]\n",
      "Step: 200 , Reward:  1.026412967542538 weights:  [0.29549497 0.720556   0.20943218 0.72982943 0.25638515 0.45276082\n",
      " 0.18612    0.88949937 0.7951072  0.15406224 0.16656983 0.21677086\n",
      " 0.13878468 0.14857563 0.34156567 0.11681798 0.16545624 0.11040106\n",
      " 0.80729604 0.6727319  0.8922086  0.10990006 0.525545   0.01764259]\n",
      "Step: 201 , Reward:  0.9707259329483185 weights:  [0.66935503 0.47321498 0.55738294 0.9286355  0.3991966  0.4049368\n",
      " 0.9119569  0.70146227 0.9621403  0.88678813 0.09888586 0.5451469\n",
      " 0.5271104  0.47531965 0.15131009 0.21855223 0.26863745 0.10306916\n",
      " 0.94984704 0.6230836  0.31398165 0.731016   0.05447823 0.85462147]\n",
      "Step: 202 , Reward:  0.9560063574654388 weights:  [0.07518202 0.84291327 0.3864825  0.24751976 0.8287271  0.10936159\n",
      " 0.67110157 0.8789877  0.8248502  0.01896825 0.16067243 0.1388514\n",
      " 0.3466854  0.7300708  0.6386988  0.3314123  0.4361761  0.59155977\n",
      " 0.7953049  0.6324221  0.22785899 0.23576847 0.5017046  0.62006634]\n",
      "Step: 203 , Reward:  0.950715891289206 weights:  [0.0601086  0.33250087 0.66677916 0.21987143 0.5042011  0.963742\n",
      " 0.54429746 0.44515204 0.25923264 0.17973378 0.08797011 0.45438302\n",
      " 0.6654682  0.07241017 0.6928351  0.0091396  0.16234943 0.04345471\n",
      " 0.94196725 0.4581852  0.6575222  0.14437026 0.142948   0.94859505]\n",
      "Step: 204 , Reward:  0.2516481569032793 weights:  [0.43597054 0.97501284 0.5674499  0.09767014 0.73606175 0.17819393\n",
      " 0.0342958  0.7208489  0.99287623 0.45675907 0.02331835 0.08662671\n",
      " 0.9469183  0.4788256  0.95984066 0.5488038  0.98691857 0.7882852\n",
      " 0.28989142 0.87052846 0.9375154  0.90424824 0.7796278  0.95180386]\n",
      "Step: 205 , Reward:  -0.011062786035700448 weights:  [0.15037382 0.72772974 0.8355181  0.8780018  0.94506586 0.2509904\n",
      " 0.07763046 0.84264463 0.5059464  0.834996   0.30214575 0.14989054\n",
      " 0.4403569  0.7886138  0.7848301  0.8415706  0.60843945 0.06657124\n",
      " 0.00958762 0.8577144  0.0932278  0.87856996 0.93869483 0.7493366 ]\n",
      "Step: 206 , Reward:  0.13892896349076211 weights:  [0.05610526 0.46854958 0.95565796 0.8526063  0.3360246  0.38768426\n",
      " 0.89154077 0.19942561 0.18562666 0.35480654 0.54196393 0.9295112\n",
      " 0.1108169  0.07263076 0.29388967 0.69014007 0.48458278 0.04267544\n",
      " 0.09311873 0.7635212  0.6661147  0.877635   0.1339477  0.7040306 ]\n",
      "Step: 207 , Reward:  0.07963718853752975 weights:  [0.1465523  0.71527696 0.37662596 0.07513708 0.62861145 0.85250646\n",
      " 0.42349356 0.5780853  0.07118377 0.24275863 0.21947008 0.20519045\n",
      " 0.2877242  0.98221135 0.8043288  0.66047263 0.20254767 0.07799616\n",
      " 0.02630705 0.1947225  0.36909345 0.94930553 0.8402356  0.604732  ]\n",
      "Step: 208 , Reward:  0.1671870025519188 weights:  [0.11922151 0.89119375 0.12656957 0.24542788 0.06217217 0.1788803\n",
      " 0.7659895  0.25398713 0.09028411 0.36097944 0.8577105  0.31569338\n",
      " 0.944002   0.6851145  0.8906714  0.10067558 0.24976885 0.61669147\n",
      " 0.6564029  0.80735135 0.06405041 0.23472199 0.28256214 0.51273054]\n",
      "Step: 209 , Reward:  0.30465448240438386 weights:  [0.740003   0.26001495 0.7161647  0.7297328  0.23830664 0.785681\n",
      " 0.08327475 0.43765828 0.8981171  0.22474262 0.2237733  0.18354729\n",
      " 0.64960647 0.33589768 0.06050113 0.5444012  0.06734461 0.0288831\n",
      " 0.6154487  0.88489354 0.5587141  0.97466373 0.7312667  0.5992596 ]\n",
      "Step: 210 , Reward:  0.35136022345389484 weights:  [0.14495859 0.28578135 0.35691643 0.0310488  0.26942405 0.89589405\n",
      " 0.83457494 0.863838   0.7011813  0.2942276  0.14857662 0.904507\n",
      " 0.63473344 0.35712612 0.49262992 0.8519923  0.19079462 0.08241087\n",
      " 0.11676943 0.60539037 0.4611793  0.8435439  0.31309053 0.39263153]\n",
      "Step: 211 , Reward:  0.5080450195881209 weights:  [0.88770276 0.81158316 0.89379066 0.72230583 0.21260029 0.7299543\n",
      " 0.850564   0.39778468 0.4897688  0.7983028  0.09228635 0.08153403\n",
      " 0.5417811  0.19284672 0.00241256 0.5206987  0.9459965  0.37308767\n",
      " 0.10687256 0.43140745 0.36965537 0.99249375 0.9213567  0.93142605]\n",
      "Step: 212 , Reward:  0.49973712713077945 weights:  [0.97006834 0.21987271 0.60283345 0.5802063  0.22205773 0.36158973\n",
      " 0.27805984 0.19117427 0.10573494 0.9624003  0.34715182 0.15634295\n",
      " 0.9635905  0.696093   0.09498039 0.93615735 0.84870183 0.9795505\n",
      " 0.86620367 0.824286   0.44483292 0.67717326 0.22483814 0.7377117 ]\n",
      "Step: 213 , Reward:  0.5562065105397522 weights:  [0.4260307  0.8098738  0.07612598 0.14207396 0.26552674 0.70080924\n",
      " 0.60214293 0.43638855 0.41743156 0.41750056 0.7921648  0.7728703\n",
      " 0.3016803  0.4166776  0.28268245 0.69067025 0.3255572  0.23195526\n",
      " 0.73737097 0.3822431  0.21785957 0.24110752 0.04585171 0.75386596]\n",
      "Step: 214 , Reward:  0.5329907702405098 weights:  [0.18202436 0.03344765 0.72584283 0.82301426 0.85706556 0.6558695\n",
      " 0.03176066 0.01580957 0.9321013  0.39827266 0.90815866 0.8785697\n",
      " 0.33719215 0.9368615  0.08538139 0.2476598  0.5030529  0.73526114\n",
      " 0.29255533 0.94927365 0.09988651 0.7268429  0.5317533  0.23813823]\n",
      "Step: 215 , Reward:  0.7204091081014722 weights:  [0.02669504 0.6018416  0.2100577  0.58516675 0.3990998  0.42439738\n",
      " 0.3679371  0.5113468  0.8307518  0.6443233  0.90634835 0.96683276\n",
      " 0.78297335 0.42362657 0.07266408 0.51061976 0.59531015 0.20180118\n",
      " 0.9126116  0.2947369  0.8998021  0.7500771  0.51640064 0.360213  ]\n",
      "Step: 216 , Reward:  0.44794834987215676 weights:  [0.6124154  0.95816493 0.6064679  0.00909269 0.4932631  0.44272012\n",
      " 0.9488816  0.7615181  0.4995468  0.07362708 0.43781868 0.92724013\n",
      " 0.40167186 0.9763942  0.19556415 0.40908462 0.13800928 0.21308994\n",
      " 0.5454394  0.09481519 0.6446742  0.96739066 0.14812511 0.45969754]\n",
      "Step: 217 , Reward:  0.4317392641579614 weights:  [0.31314486 0.37869993 0.89412016 0.9083795  0.04907763 0.6664166\n",
      " 0.49906832 0.38992244 0.89042103 0.9332843  0.27195197 0.3492232\n",
      " 0.30703798 0.7111919  0.5249053  0.95129234 0.20495644 0.9025506\n",
      " 0.39667413 0.5614801  0.65649784 0.07178321 0.88357466 0.8406235 ]\n",
      "Step: 218 , Reward:  0.2754695564723396 weights:  [0.08082613 0.4438641  0.49616057 0.8362616  0.44379798 0.57130563\n",
      " 0.25667733 0.3085918  0.4352203  0.22700441 0.2159628  0.762074\n",
      " 0.09198275 0.3630097  0.11944786 0.4366406  0.6822462  0.5102143\n",
      " 0.9057519  0.46425283 0.7768556  0.6842098  0.32079092 0.6613482 ]\n",
      "Step: 219 , Reward:  0.7568898777484305 weights:  [0.13502252 0.24561584 0.7958732  0.5980037  0.54900223 0.9251722\n",
      " 0.04345694 0.24547061 0.5336256  0.36763477 0.13322714 0.110751\n",
      " 0.38341418 0.6074039  0.00555441 0.93193257 0.545202   0.48323268\n",
      " 0.12661904 0.95939106 0.04961136 0.7114289  0.08358327 0.24775457]\n",
      "Step: 220 , Reward:  1.0975541858506705 weights:  [0.6186373  0.7143709  0.12412149 0.28057188 0.22039387 0.06032211\n",
      " 0.04853237 0.8468487  0.7396731  0.8910059  0.05876023 0.30194083\n",
      " 0.40694493 0.9441471  0.45507666 0.91930836 0.84811294 0.96150887\n",
      " 0.40665907 0.92700386 0.5589373  0.31025434 0.07957405 0.4279014 ]\n",
      "Step: 221 , Reward:  0.6169095805617415 weights:  [0.9534869  0.6170589  0.03999323 0.8569647  0.9249388  0.9780124\n",
      " 0.69141114 0.78835475 0.8248693  0.50451237 0.8178265  0.36791036\n",
      " 0.11433795 0.2748639  0.6699112  0.898031   0.08863595 0.51164186\n",
      " 0.00568944 0.60928446 0.36216593 0.05389261 0.18441972 0.728471  ]\n",
      "Step: 222 , Reward:  0.6651437982739573 weights:  [0.8871306  0.8957839  0.11678222 0.14118093 0.02697885 0.03027356\n",
      " 0.45658445 0.7318154  0.8056817  0.13934988 0.170291   0.04501066\n",
      " 0.8909292  0.6978782  0.9742677  0.4991815  0.04270986 0.52673566\n",
      " 0.51830953 0.5266805  0.19813609 0.06739318 0.37716562 0.41616142]\n",
      "Step: 223 , Reward:  0.6233293733451291 weights:  [0.7971957  0.96095073 0.89005923 0.4893454  0.05111417 0.9848786\n",
      " 0.9838793  0.54896146 0.8153514  0.9306549  0.96098185 0.82237744\n",
      " 0.80206597 0.80024654 0.39402723 0.48163068 0.07504609 0.6919129\n",
      " 0.13497114 0.38575765 0.378901   0.34919435 0.7617155  0.8307755 ]\n",
      "Step: 224 , Reward:  0.29001878356294525 weights:  [0.94415826 0.8397381  0.36786914 0.41700503 0.04024053 0.24789086\n",
      " 0.4081697  0.8022348  0.3447872  0.9699131  0.937616   0.7330658\n",
      " 0.9543903  0.7054254  0.8276763  0.93505466 0.02171719 0.91574514\n",
      " 0.02022091 0.723935   0.8018935  0.20179918 0.21576372 0.8761955 ]\n",
      "Step: 225 , Reward:  0.057018546646720444 weights:  [0.64260095 0.04622689 0.46331257 0.84756005 0.4708818  0.8817548\n",
      " 0.6352289  0.21215037 0.12806696 0.35643798 0.20695996 0.7633538\n",
      " 0.4222399  0.12835562 0.77975225 0.74603117 0.7009293  0.9143437\n",
      " 0.19528025 0.5147645  0.9338052  0.323097   0.31901476 0.22745919]\n",
      "Step: 226 , Reward:  -0.12062499607327057 weights:  [0.9537021  0.78594065 0.0328393  0.35124263 0.40335986 0.0976297\n",
      " 0.23997313 0.841867   0.50622785 0.02209413 0.869835   0.11892205\n",
      " 0.78747594 0.6435812  0.2548079  0.18951944 0.47475216 0.90329415\n",
      " 0.44626004 0.07889497 0.90980554 0.142486   0.2035374  0.57116735]\n",
      "Step: 227 , Reward:  -0.1766254674418211 weights:  [0.32243228 0.12478703 0.739077   0.6346115  0.41269672 0.2915665\n",
      " 0.9912704  0.41459894 0.59989506 0.2918964  0.08513993 0.27726984\n",
      " 0.257643   0.8445905  0.8374746  0.4464351  0.33774865 0.5877144\n",
      " 0.15893379 0.9645778  0.85826755 0.9416768  0.3773154  0.5537412 ]\n",
      "Step: 228 , Reward:  -0.3545456237936096 weights:  [0.05709669 0.9014678  0.83270407 0.7814508  0.8318143  0.3593281\n",
      " 0.41763294 0.6693374  0.0890829  0.1730907  0.9843583  0.3163366\n",
      " 0.5452516  0.27072862 0.34049183 0.93270093 0.61581767 0.20863059\n",
      " 0.3094392  0.14535657 0.6342809  0.84069777 0.45928985 0.4773682 ]\n",
      "Step: 229 , Reward:  -0.3290428514474792 weights:  [0.3280527  0.7383145  0.38421634 0.83761084 0.45939258 0.752321\n",
      " 0.4308938  0.7283026  0.67078674 0.65180385 0.4409525  0.08572668\n",
      " 0.00976124 0.85162246 0.09808487 0.80288804 0.61045915 0.4291026\n",
      " 0.09565654 0.34544    0.33441997 0.23211208 0.10428104 0.87106675]\n",
      "Step: 230 , Reward:  -0.9991751352719141 weights:  [0.9870328  0.1528112  0.7675195  0.16289538 0.09249735 0.8401259\n",
      " 0.5378341  0.40441644 0.06663954 0.77432024 0.1347009  0.9720205\n",
      " 0.09318954 0.4850263  0.53081036 0.8154961  0.5031902  0.15409672\n",
      " 0.11547685 0.6644783  0.5356699  0.8945737  0.2291269  0.6125938 ]\n",
      "Step: 231 , Reward:  -0.9792740950780775 weights:  [0.06880745 0.25089553 0.84749925 0.5392361  0.00505611 0.06072006\n",
      " 0.10405731 0.9556248  0.7028385  0.89515996 0.58734983 0.40067473\n",
      " 0.10876188 0.407646   0.6032553  0.05588591 0.48449993 0.60270125\n",
      " 0.54318386 0.30520606 0.566362   0.7663854  0.87883306 0.9528995 ]\n",
      "Step: 232 , Reward:  -1.044952966704105 weights:  [0.5837559  0.84538    0.14629576 0.22993907 0.72110796 0.8183969\n",
      " 0.5826739  0.69750535 0.04808003 0.22683242 0.14289838 0.4960273\n",
      " 0.5280759  0.03134817 0.6645781  0.84707075 0.66315436 0.57697266\n",
      " 0.6351391  0.41920036 0.19577685 0.47053978 0.25374776 0.924598  ]\n",
      "Step: 233 , Reward:  -0.6965849400389976 weights:  [0.42178994 0.09934005 0.92701644 0.44430095 0.6658547  0.9688228\n",
      " 0.22132266 0.5486633  0.15406927 0.5607553  0.94691277 0.7846212\n",
      " 0.9480379  0.81746787 0.05532488 0.06510329 0.469513   0.6204586\n",
      " 0.41102773 0.27186728 0.10030735 0.02525115 0.90014863 0.8993331 ]\n",
      "Step: 234 , Reward:  -1.0193469165348366 weights:  [0.39240626 0.5995963  0.294204   0.31253436 0.9172244  0.8673736\n",
      " 0.07353157 0.31419224 0.8775938  0.7063621  0.34422427 0.77185786\n",
      " 0.12186569 0.20271662 0.9521642  0.37463886 0.7158649  0.81675303\n",
      " 0.30882883 0.29020554 0.6833718  0.08237788 0.9057334  0.8627339 ]\n",
      "Step: 235 , Reward:  -0.6742760652434024 weights:  [0.11703989 0.8612276  0.40609252 0.97806174 0.6013014  0.13527194\n",
      " 0.08356151 0.16336438 0.9936821  0.78624797 0.10510811 0.41722727\n",
      " 0.22255981 0.6025658  0.05913708 0.9488144  0.7820778  0.71761346\n",
      " 0.93208736 0.38177827 0.8164638  0.27732143 0.3913316  0.58107966]\n",
      "Step: 236 , Reward:  -0.3609102121454328 weights:  [0.1578562  0.02965164 0.7726338  0.8427738  0.84974736 0.5030845\n",
      " 0.6312996  0.39515722 0.26089996 0.03582191 0.60918164 0.9299993\n",
      " 0.923846   0.0487828  0.07376662 0.8688035  0.945322   0.09778804\n",
      " 0.19453648 0.53709006 0.73885465 0.0632636  0.17912397 0.14749926]\n",
      "Step: 237 , Reward:  -0.4043235552840583 weights:  [0.74881727 0.554612   0.33393586 0.3290863  0.20094383 0.46619302\n",
      " 0.05011123 0.51839536 0.13266194 0.8171737  0.27243444 0.89722764\n",
      " 0.6388065  0.99836993 0.618183   0.8234115  0.25922167 0.24615881\n",
      " 0.26358414 0.22863701 0.7241514  0.69844043 0.18692699 0.71879303]\n",
      "Step: 238 , Reward:  -0.4770058239216498 weights:  [0.31912726 0.0906978  0.9371356  0.2184636  0.7066697  0.18941441\n",
      " 0.75476205 0.33373782 0.05403158 0.2853552  0.01075059 0.10489914\n",
      " 0.9480885  0.67521006 0.03703198 0.37749353 0.29459536 0.9504291\n",
      " 0.8868741  0.6733655  0.08695424 0.76791227 0.844176   0.7469828 ]\n",
      "Step: 239 , Reward:  -0.07266927352214245 weights:  [0.57209367 0.37632534 0.21714565 0.2895625  0.23913172 0.05190128\n",
      " 0.7068388  0.90764034 0.95936644 0.89657235 0.85386455 0.11090258\n",
      " 0.2939784  0.19430372 0.08630258 0.8095424  0.892851   0.91885746\n",
      " 0.27980247 0.6527364  0.6296046  0.17288452 0.6739186  0.25      ]\n",
      "Step: 240 , Reward:  -0.06020232876377938 weights:  [0.5632355  0.12536788 0.09445736 0.87778974 0.90506613 0.6205052\n",
      " 0.27665442 0.94100535 0.92800605 0.47250015 0.6314166  0.96519923\n",
      " 0.57279146 0.9013488  0.278908   0.1796427  0.81478643 0.03717968\n",
      " 0.18194696 0.9061643  0.06210229 0.9599254  0.42907098 0.8619832 ]\n",
      "Step: 241 , Reward:  -0.02070767091209955 weights:  [0.2407454  0.32286128 0.56183314 0.4740637  0.90920603 0.19225395\n",
      " 0.9627923  0.2416358  0.41361332 0.9073678  0.34782022 0.53318757\n",
      " 0.81472903 0.5060212  0.16314226 0.366744   0.01186454 0.25724778\n",
      " 0.2533918  0.34084123 0.3554109  0.60781866 0.03166193 0.13999763]\n",
      "Step: 242 , Reward:  0.03642087103366208 weights:  [0.30777764 0.43958506 0.8929562  0.1753616  0.1579547  0.09103504\n",
      " 0.8990514  0.21377048 0.04785275 0.3192399  0.42717874 0.10781419\n",
      " 0.92125237 0.94252014 0.28736863 0.05361155 0.368397   0.7989842\n",
      " 0.262236   0.6711501  0.26385707 0.08939278 0.10950541 0.40833542]\n",
      "Step: 243 , Reward:  0.1520342185762239 weights:  [0.5253523  0.95053744 0.05718809 0.8797127  0.12911037 0.04557186\n",
      " 0.08466655 0.07829615 0.03441304 0.8220932  0.20270899 0.12395823\n",
      " 0.6569514  0.8353392  0.41793057 0.3870775  0.47217587 0.10716522\n",
      " 0.53150946 0.69334227 0.02345377 0.9480573  0.9847504  0.36840594]\n",
      "Step: 244 , Reward:  -0.03637985183635937 weights:  [0.06534523 0.6600332  0.8398234  0.72117364 0.68833935 0.5895938\n",
      " 0.8772663  0.53370553 0.9365428  0.6522917  0.26456308 0.5094452\n",
      " 0.36757016 0.6742759  0.16040179 0.43899736 0.58136827 0.42992586\n",
      " 0.9369118  0.35139945 0.7677398  0.1379756  0.67701226 0.11788666]\n",
      "Step: 245 , Reward:  -0.07450551947542898 weights:  [0.20789704 0.10706231 0.26632586 0.14251754 0.08995143 0.8575462\n",
      " 0.71514356 0.10332623 0.815513   0.21832922 0.955935   0.59291923\n",
      " 0.7534239  0.5128454  0.14329565 0.3182234  0.59957975 0.8412036\n",
      " 0.2921244  0.7508521  0.42300528 0.63199216 0.685362   0.6603223 ]\n",
      "Step: 246 , Reward:  0.25605345832131426 weights:  [0.32733232 0.5155876  0.839278   0.7995414  0.33599743 0.03470674\n",
      " 0.7503475  0.84258366 0.6493337  0.03329381 0.3601795  0.7292312\n",
      " 0.02139261 0.5434887  0.43248674 0.6783477  0.86961114 0.672781\n",
      " 0.30281317 0.37646383 0.35715252 0.04246578 0.74068856 0.01556706]\n",
      "Step: 247 , Reward:  0.2305657387940128 weights:  [0.5133159  0.04494444 0.44527695 0.8816243  0.38777152 0.19337723\n",
      " 0.4614914  0.2048851  0.61122155 0.65223163 0.1091018  0.46579888\n",
      " 0.86816853 0.8570378  0.16110924 0.2725438  0.9912958  0.8943049\n",
      " 0.8595221  0.73567665 0.60000026 0.29259455 0.00488696 0.34347698]\n",
      "Step: 248 , Reward:  0.1131041287493152 weights:  [0.51708204 0.97083133 0.8314746  0.16129005 0.716646   0.8325635\n",
      " 0.3325131  0.68891287 0.40728486 0.94210386 0.7051722  0.10221714\n",
      " 0.64964616 0.38358867 0.9261587  0.42058197 0.6467191  0.41828734\n",
      " 0.6145361  0.14400116 0.56664574 0.8335378  0.6188942  0.88073075]\n",
      "Step: 249 , Reward:  -0.19601708089848974 weights:  [0.06629303 0.96357155 0.6624869  0.21355206 0.8091641  0.9273056\n",
      " 0.06523159 0.93479455 0.7322347  0.7251823  0.51666266 0.77170354\n",
      " 0.8070049  0.8859444  0.98389715 0.7514592  0.6347346  0.18554622\n",
      " 0.02064905 0.23139572 0.92599523 0.4450298  0.42235222 0.02529705]\n",
      "Step: 250 , Reward:  -0.2953379789880617 weights:  [0.7462731  0.26562902 0.75572777 0.3174932  0.10565335 0.22654542\n",
      " 0.07135162 0.6646448  0.7348952  0.9112162  0.26108438 0.5460411\n",
      " 0.4352626  0.10331422 0.5488195  0.90708655 0.72158825 0.9397619\n",
      " 0.04029563 0.23006514 0.6360544  0.1438548  0.73044884 0.6528897 ]\n",
      "Step: 251 , Reward:  -0.35743296848732126 weights:  [0.79036343 0.07907167 0.3389121  0.72062767 0.9688376  0.9876622\n",
      " 0.15375054 0.4241127  0.4823826  0.80767167 0.31924093 0.9032718\n",
      " 0.83582443 0.18756685 0.2106156  0.0389865  0.37442738 0.8553153\n",
      " 0.33800042 0.24595645 0.4323459  0.9947317  0.97495246 0.3037743 ]\n",
      "Step: 252 , Reward:  -0.2934194635952436 weights:  [0.22824618 0.8523823  0.95532507 0.84981596 0.11007109 0.44578516\n",
      " 0.7289992  0.9098505  0.8028338  0.42232126 0.93099844 0.7449548\n",
      " 0.906322   0.40726304 0.09727064 0.9898745  0.6286156  0.5022295\n",
      " 0.52498263 0.5903086  0.09201375 0.6062243  0.00626299 0.05624941]\n",
      "Step: 253 , Reward:  -0.15187368939533566 weights:  [0.5209931  0.39372915 0.01569128 0.3045609  0.12099031 0.28813478\n",
      " 0.26034585 0.7328192  0.9291787  0.726541   0.96057016 0.31428868\n",
      " 0.01437807 0.49580872 0.11727947 0.5584242  0.06997213 0.4451988\n",
      " 0.44168487 0.27907252 0.46405718 0.5924457  0.25761998 0.9229467 ]\n",
      "Step: 254 , Reward:  -0.37267336552757796 weights:  [0.9256507  0.12819675 0.25689092 0.40009236 0.949934   0.61751324\n",
      " 0.44188994 0.35496032 0.7891221  0.4204287  0.783549   0.56079644\n",
      " 0.06025603 0.1161975  0.7634947  0.12508386 0.48419344 0.6403091\n",
      " 0.44327474 0.08542779 0.9699874  0.7370727  0.12334237 0.13506854]\n",
      "Step: 255 , Reward:  -0.15741893116341815 weights:  [0.97142595 0.11129764 0.16203368 0.7772606  0.70679885 0.66816294\n",
      " 0.23388386 0.86338747 0.5444468  0.9968378  0.8352878  0.19518909\n",
      " 0.9419275  0.32924536 0.5303494  0.9084811  0.31540287 0.5945966\n",
      " 0.02315626 0.7595447  0.36256915 0.2960388  0.10299996 0.3493939 ]\n",
      "Step: 256 , Reward:  -0.11676339977972242 weights:  [0.6190131  0.669825   0.39186534 0.9944584  0.9260596  0.25236398\n",
      " 0.96380305 0.17350936 0.31647527 0.8977325  0.04483047 0.52057713\n",
      " 0.89567935 0.7988297  0.03601122 0.9441539  0.721299   0.24059787\n",
      " 0.28062522 0.6968259  0.8460531  0.07543752 0.60129464 0.23915702]\n",
      "Step: 257 , Reward:  -0.19861847014402556 weights:  [0.42313388 0.4222463  0.2809354  0.82652277 0.6051456  0.28143528\n",
      " 0.08688736 0.11910686 0.6419303  0.22125256 0.30220112 0.25558466\n",
      " 0.98246086 0.7943105  0.38377616 0.01360807 0.8966068  0.26652515\n",
      " 0.46304134 0.18590575 0.089167   0.88317025 0.38254362 0.6552187 ]\n",
      "Step: 258 , Reward:  -0.23354466806915877 weights:  [0.9627127  0.16752115 0.7842928  0.5483096  0.5251716  0.79924333\n",
      " 0.36388242 0.9395933  0.95060414 0.31054452 0.4738976  0.3282346\n",
      " 0.41472146 0.31147885 0.6922724  0.6296654  0.6077949  0.5816991\n",
      " 0.9420808  0.60440576 0.76060665 0.25513625 0.43538085 0.62885725]\n",
      "Step: 259 , Reward:  0.048245502293406246 weights:  [0.0583761  0.4273551  0.22234985 0.04728055 0.710654   0.77847266\n",
      " 0.4396767  0.3174153  0.6102805  0.8414345  0.20302609 0.48643798\n",
      " 0.9409471  0.01444373 0.27813947 0.6108608  0.6711566  0.10636273\n",
      " 0.47096068 0.08543506 0.4291743  0.8628006  0.5684867  0.44849017]\n",
      "Step: 260 , Reward:  0.11771587502367492 weights:  [0.4422812  0.7791092  0.9885944  0.2352413  0.51945853 0.04594263\n",
      " 0.27023783 0.91946197 0.2989173  0.3716213  0.22770673 0.661327\n",
      " 0.7879207  0.08403903 0.09623009 0.9856924  0.3238842  0.03926516\n",
      " 0.76198375 0.41296735 0.4998597  0.93012    0.864493   0.37442833]\n",
      "Step: 261 , Reward:  0.22196331986947718 weights:  [0.0279541  0.10032788 0.16703433 0.9016446  0.88642555 0.9128618\n",
      " 0.9545947  0.75593746 0.6668668  0.36848012 0.7794465  0.96170294\n",
      " 0.8420213  0.6257132  0.4481675  0.65925074 0.6357759  0.07764611\n",
      " 0.47720185 0.90507305 0.7906562  0.7380991  0.03951764 0.64803874]\n",
      "Step: 262 , Reward:  0.14796843916590546 weights:  [0.7465981  0.30192626 0.5700017  0.09941027 0.8649625  0.42282963\n",
      " 0.32186276 0.02272981 0.29294115 0.8828675  0.8516257  0.15488344\n",
      " 0.69465256 0.6054666  0.06807467 0.94195074 0.0482932  0.08239123\n",
      " 0.16186905 0.32838178 0.6244286  0.85054123 0.06893408 0.91229177]\n",
      "Step: 263 , Reward:  0.009942177084378136 weights:  [0.51831704 0.472211   0.7355497  0.9544257  0.06635696 0.09931302\n",
      " 0.97694504 0.24971116 0.9753412  0.33415323 0.8841163  0.3952716\n",
      " 0.25704783 0.7483464  0.16956815 0.3150229  0.27714303 0.09892815\n",
      " 0.2056978  0.7110837  0.8804025  0.8653246  0.3508988  0.41321447]\n",
      "Step: 264 , Reward:  0.7171542824969857 weights:  [0.34633616 0.29646158 0.31882036 0.29577312 0.54269654 0.7731613\n",
      " 0.22709385 0.04650962 0.73465574 0.6317233  0.46137822 0.9855268\n",
      " 0.9469974  0.06294635 0.57276857 0.48923483 0.4221536  0.738229\n",
      " 0.67166495 0.93466616 0.96663773 0.77026045 0.8858253  0.8027218 ]\n",
      "Step: 265 , Reward:  0.7548915887214366 weights:  [0.76435065 0.5492294  0.3524713  0.59293747 0.5837527  0.7533162\n",
      " 0.09708804 0.25016445 0.61058456 0.6045426  0.19683835 0.21426275\n",
      " 0.95072186 0.30252695 0.38741648 0.00524604 0.18439296 0.05365902\n",
      " 0.61511916 0.5734409  0.05242532 0.00477472 0.13433605 0.23560765]\n",
      "Step: 266 , Reward:  0.754059893483903 weights:  [0.57031345 0.10768026 0.13487491 0.2663633  0.04099107 0.35841674\n",
      " 0.7082886  0.38803035 0.6939722  0.37132978 0.21393993 0.49065155\n",
      " 0.2529381  0.15313435 0.7862284  0.7510112  0.02825505 0.46147284\n",
      " 0.9378242  0.42553174 0.7622644  0.26697534 0.9941528  0.20533937]\n",
      "Step: 267 , Reward:  0.5982885048758895 weights:  [0.27076924 0.06017148 0.58523893 0.63971907 0.77092373 0.08034152\n",
      " 0.49613655 0.5626164  0.8159415  0.20535707 0.00786033 0.9011354\n",
      " 0.5409409  0.80076176 0.49394923 0.49630553 0.884853   0.61389136\n",
      " 0.31520593 0.8373562  0.5334969  0.5221223  0.09041965 0.7747525 ]\n",
      "Step: 268 , Reward:  0.479578896921933 weights:  [0.02329758 0.70916605 0.22682598 0.26935667 0.33483866 0.48854953\n",
      " 0.6833739  0.63043356 0.627306   0.59097    0.5376994  0.12958714\n",
      " 0.17780924 0.85021913 0.7938571  0.13848132 0.8993242  0.15611345\n",
      " 0.35717738 0.3512342  0.7207445  0.259049   0.5754757  0.8584663 ]\n",
      "Step: 269 , Reward:  0.5387382408633065 weights:  [0.41680706 0.78983766 0.60975355 0.39673772 0.9134753  0.07043409\n",
      " 0.9546138  0.39678878 0.06466541 0.2648902  0.56725186 0.23745307\n",
      " 0.8443457  0.57592696 0.36562675 0.6551279  0.9173914  0.11782858\n",
      " 0.6162762  0.5060146  0.29727104 0.12920713 0.43065268 0.45287076]\n",
      "Step: 270 , Reward:  0.323486548643376 weights:  [0.3263117  0.35923555 0.66075295 0.16660416 0.9830785  0.95986754\n",
      " 0.37529323 0.5537904  0.9561471  0.22653666 0.12454841 0.2822113\n",
      " 0.32897848 0.709632   0.31372142 0.43040138 0.9338325  0.8069809\n",
      " 0.97135156 0.37304753 0.9252343  0.61077344 0.4035422  0.85398626]\n",
      "Step: 271 , Reward:  0.2231586839371518 weights:  [0.54056096 0.88504994 0.09342667 0.6851593  0.8087429  0.850905\n",
      " 0.7750036  0.49210542 0.93159413 0.12388337 0.6233047  0.15265924\n",
      " 0.18593475 0.3721186  0.825786   0.81607765 0.1473105  0.2130923\n",
      " 0.18876925 0.5005712  0.94385093 0.9499502  0.7277451  0.4964201 ]\n",
      "Step: 272 , Reward:  0.45238954087250566 weights:  [0.5480852  0.287489   0.96223986 0.13626552 0.8424993  0.27334678\n",
      " 0.7058168  0.8724146  0.80644447 0.74098814 0.17665115 0.7496611\n",
      " 0.30105954 0.24927682 0.12189507 0.576164   0.7668253  0.07031608\n",
      " 0.23339409 0.3372755  0.8812001  0.08393833 0.46980488 0.04849899]\n",
      "Step: 273 , Reward:  0.43101515600802465 weights:  [0.88481545 0.7193867  0.91130173 0.869755   0.67056143 0.10952121\n",
      " 0.6985817  0.10464486 0.9075835  0.9695885  0.20609325 0.06372437\n",
      " 0.05497131 0.06554523 0.56334174 0.8944713  0.9339274  0.48627687\n",
      " 0.05184847 0.15085292 0.09681466 0.9863839  0.8202783  0.4551995 ]\n",
      "Step: 274 , Reward:  0.3337444217385525 weights:  [0.56961495 0.17331523 0.42798132 0.8735931  0.24116385 0.7594057\n",
      " 0.98756874 0.03800118 0.8811623  0.8150904  0.23201379 0.9624063\n",
      " 0.5187201  0.89338034 0.291325   0.17836002 0.00872585 0.08230412\n",
      " 0.14864174 0.31035733 0.8012824  0.8723099  0.86807805 0.3900459 ]\n",
      "Step: 275 , Reward:  0.5292891434358086 weights:  [0.11335295 0.8637643  0.23896176 0.61965835 0.8210977  0.7300645\n",
      " 0.1589053  0.09562683 0.91112185 0.73790514 0.8107096  0.7531066\n",
      " 0.16706464 0.08303016 0.32129192 0.79874253 0.03635502 0.76956534\n",
      " 0.08825016 0.14196411 0.5505345  0.10649818 0.3282144  0.00879785]\n",
      "Step: 276 , Reward:  0.35577279638253473 weights:  [0.7516024  0.44707736 0.21419248 0.3245219  0.9594307  0.8467857\n",
      " 0.94143176 0.07348678 0.14140782 0.12231457 0.40077025 0.14449596\n",
      " 0.76793456 0.65076953 0.06634587 0.14383125 0.6735573  0.8549466\n",
      " 0.05944988 0.10321391 0.00955907 0.32464993 0.92348665 0.46588337]\n",
      "Step: 277 , Reward:  0.7059093840522492 weights:  [0.6538633  0.21824485 0.5808038  0.35055524 0.1425058  0.97027814\n",
      " 0.09002995 0.21003258 0.40011853 0.8557869  0.39897418 0.78223777\n",
      " 0.28817585 0.40919945 0.3248348  0.42119753 0.18983576 0.72822475\n",
      " 0.6703862  0.9554666  0.6342166  0.37915426 0.55150306 0.17529854]\n",
      "Step: 278 , Reward:  1.367273285688203 weights:  [0.9573653  0.90376425 0.6005008  0.08546498 0.45262006 0.49706092\n",
      " 0.02600089 0.88805723 0.8013049  0.32599843 0.01865089 0.67914236\n",
      " 0.12795216 0.295221   0.08636108 0.6914506  0.03613412 0.97028923\n",
      " 0.15315813 0.9620886  0.508353   0.7388867  0.52793247 0.88537884]\n",
      "Step: 279 , Reward:  1.577911785708922 weights:  [0.49761954 0.88851243 0.48113823 0.2879691  0.10272747 0.14642453\n",
      " 0.14304411 0.74011445 0.0949997  0.7627188  0.60862494 0.42449653\n",
      " 0.4735187  0.99267757 0.37227154 0.31762815 0.12570897 0.47540724\n",
      " 0.881964   0.92821264 0.26314718 0.70618665 0.07895657 0.8595655 ]\n",
      "Step: 280 , Reward:  1.3312513245030393 weights:  [0.72426903 0.2976485  0.48743358 0.54100347 0.38845092 0.23818946\n",
      " 0.76002276 0.6141342  0.29401997 0.41682675 0.15438032 0.55293316\n",
      " 0.7556789  0.43878034 0.7297182  0.8026986  0.9422409  0.40622184\n",
      " 0.00175151 0.74928856 0.6817994  0.30635634 0.67589164 0.9237804 ]\n",
      "Step: 281 , Reward:  1.0817970019957877 weights:  [0.953439   0.8813469  0.9178547  0.88200176 0.2567675  0.15145913\n",
      " 0.5341267  0.8341545  0.08232352 0.4686812  0.49914587 0.0958128\n",
      " 0.29443792 0.83643925 0.00401816 0.23481536 0.33086237 0.8955163\n",
      " 0.2706687  0.11205328 0.5826593  0.79917264 0.32340756 0.17752025]\n",
      "Step: 282 , Reward:  1.584160136341873 weights:  [0.02079582 0.58880496 0.6704651  0.7454448  0.82878435 0.3901698\n",
      " 0.156432   0.8739104  0.6053124  0.3225764  0.37665087 0.87583864\n",
      " 0.5970012  0.2605511  0.08536938 0.07894349 0.49479496 0.12706277\n",
      " 0.4073679  0.11958274 0.41189355 0.06371978 0.18485889 0.22793126]\n",
      "Step: 283 , Reward:  1.4489581944437178 weights:  [0.8790543  0.61620027 0.22130576 0.6276339  0.9139952  0.54886883\n",
      " 0.45673758 0.1194236  0.02280238 0.60381097 0.02989393 0.18548158\n",
      " 0.35584056 0.5828774  0.57512915 0.52784806 0.78874594 0.8307358\n",
      " 0.00943559 0.03883943 0.7737272  0.69789886 0.22480398 0.02811679]\n",
      "Step: 284 , Reward:  1.4355466290559673 weights:  [0.84507    0.60587895 0.03910908 0.13684282 0.2827657  0.09050903\n",
      " 0.7729093  0.950204   0.96477634 0.05817759 0.10390556 0.41969025\n",
      " 0.17978156 0.726176   0.8993945  0.41198897 0.44012287 0.4880463\n",
      " 0.258494   0.28091288 0.18314049 0.69639015 0.11985829 0.86945295]\n",
      "Step: 285 , Reward:  1.3175854426599118 weights:  [0.8876023  0.3407234  0.03482702 0.41047335 0.7370445  0.10730806\n",
      " 0.01627231 0.9244031  0.98651755 0.73037595 0.5778212  0.4004507\n",
      " 0.738358   0.60800385 0.14029464 0.6811693  0.8333615  0.5226958\n",
      " 0.6520931  0.8829769  0.7774234  0.8884759  0.30921102 0.85751534]\n",
      "Step: 286 , Reward:  1.3459979042985422 weights:  [0.17364872 0.31098557 0.78622794 0.17198047 0.33651674 0.02152544\n",
      " 0.05607438 0.14356613 0.7782711  0.2806226  0.6366668  0.5029978\n",
      " 0.4751439  0.6585595  0.42411005 0.18987873 0.4831029  0.5767748\n",
      " 0.21194541 0.02342486 0.55498135 0.9398575  0.86705625 0.5489102 ]\n",
      "Step: 287 , Reward:  1.3676009763283332 weights:  [0.39276615 0.44354016 0.894384   0.6517031  0.46778926 0.38221473\n",
      " 0.6690453  0.8120744  0.93835056 0.67610073 0.92984366 0.7333155\n",
      " 0.6213741  0.28146338 0.15419084 0.9846498  0.36536068 0.4714703\n",
      " 0.21744323 0.20656309 0.15228969 0.44226506 0.745337   0.91907537]\n",
      "Step: 288 , Reward:  1.2439626312982235 weights:  [0.6515368  0.7672892  0.90891576 0.5901579  0.7925439  0.37775934\n",
      " 0.59094906 0.19699138 0.92934227 0.83044404 0.81767756 0.20278102\n",
      " 0.74073386 0.8370216  0.72825336 0.5992113  0.3364514  0.08777744\n",
      " 0.7527566  0.4217958  0.86173856 0.6549691  0.2033548  0.8769269 ]\n",
      "Step: 289 , Reward:  1.1050790877023315 weights:  [0.38855284 0.67406946 0.7781993  0.42874283 0.5924865  0.6005061\n",
      " 0.8534409  0.80958706 0.63460386 0.41812965 0.8638828  0.19784704\n",
      " 0.8506417  0.04829517 0.7738995  0.5256427  0.75829405 0.28073776\n",
      " 0.15699789 0.3663284  0.13015705 0.11186996 0.10584471 0.6818216 ]\n",
      "Step: 290 , Reward:  1.0166420268931335 weights:  [0.05245894 0.42295837 0.5694679  0.6255621  0.3341567  0.26946434\n",
      " 0.5095228  0.79234225 0.579686   0.01758721 0.9720514  0.29455432\n",
      " 0.07322529 0.27827424 0.2725805  0.22895133 0.2986467  0.45744035\n",
      " 0.35221422 0.4539773  0.5198647  0.94152904 0.22912675 0.46220672]\n",
      "Step: 291 , Reward:  0.7641512963973041 weights:  [0.7698575  0.20524326 0.23219013 0.12112254 0.7593733  0.756984\n",
      " 0.3315576  0.49828348 0.39027417 0.9221417  0.42844155 0.877948\n",
      " 0.9974083  0.99580616 0.8201051  0.691944   0.45879427 0.3706489\n",
      " 0.7587303  0.5037696  0.02508381 0.6691862  0.01206824 0.40642428]\n",
      "Step: 292 , Reward:  0.6855467800834768 weights:  [0.27906257 0.15420601 0.4399987  0.65256685 0.3407972  0.8535839\n",
      " 0.03940994 0.630149   0.49880958 0.90672123 0.39199626 0.80604124\n",
      " 0.10687    0.8690529  0.7763581  0.83929026 0.2401697  0.49505344\n",
      " 0.10115024 0.66577363 0.67427826 0.46229813 0.40431717 0.16078049]\n",
      "Step: 293 , Reward:  0.7998684537724978 weights:  [0.7890048  0.9911782  0.5860319  0.7799955  0.61198586 0.17995459\n",
      " 0.15090263 0.9673581  0.7080433  0.86915815 0.15935746 0.5391384\n",
      " 0.1101442  0.6132115  0.5429347  0.13276693 0.13061234 0.01803127\n",
      " 0.00839058 0.57388437 0.28443772 0.81200564 0.8087486  0.02049708]\n",
      "Step: 294 , Reward:  0.6708850930519327 weights:  [0.34654135 0.5330272  0.8693604  0.9256741  0.03626075 0.46190503\n",
      " 0.83711433 0.3035853  0.8494205  0.59629226 0.73192966 0.98241603\n",
      " 0.84264255 0.02918246 0.11022082 0.17715156 0.2603832  0.88185316\n",
      " 0.64953065 0.6676645  0.10444102 0.04906824 0.8794166  0.8902942 ]\n",
      "Step: 295 , Reward:  0.5658955009218443 weights:  [0.7723305  0.39113563 0.24510074 0.934083   0.43350834 0.49586886\n",
      " 0.30649495 0.48536494 0.38029405 0.91994274 0.68187094 0.98123324\n",
      " 0.27750894 0.43504816 0.69815254 0.39835724 0.15626326 0.53637874\n",
      " 0.1461663  0.06940719 0.5116187  0.01554498 0.7311598  0.42340246]\n",
      "Step: 296 , Reward:  0.5952077174859903 weights:  [0.8967343  0.25518686 0.84655666 0.94787407 0.50227135 0.40848422\n",
      " 0.64508086 0.5816956  0.3312729  0.55476743 0.5037494  0.63785\n",
      " 0.4648562  0.8254082  0.05218986 0.33826637 0.34379613 0.30069432\n",
      " 0.6862329  0.10126385 0.7228782  0.12257159 0.48922282 0.19037294]\n",
      "Step: 297 , Reward:  0.27260704834589183 weights:  [0.17503756 0.5907806  0.34447816 0.6446174  0.10747677 0.06489369\n",
      " 0.6388382  0.27684212 0.8458649  0.2659939  0.8468732  0.354511\n",
      " 0.6974434  0.51230115 0.96546865 0.20680737 0.1629974  0.04199398\n",
      " 0.3092575  0.2110906  0.05636069 0.4126445  0.27506092 0.49006262]\n",
      "Step: 298 , Reward:  0.15178891650888868 weights:  [0.4217186  0.09711212 0.57568824 0.1215038  0.00347689 0.7856553\n",
      " 0.2408446  0.6510905  0.49037135 0.00372556 0.4802708  0.92769593\n",
      " 0.3674931  0.9388627  0.01731429 0.8413677  0.3282569  0.30419034\n",
      " 0.5521425  0.93054736 0.91751564 0.6212077  0.608165   0.5252103 ]\n",
      "Step: 299 , Reward:  -0.504971973226147 weights:  [0.4338941  0.78173673 0.6644447  0.7345846  0.09314764 0.15802607\n",
      " 0.8413973  0.6103092  0.49778065 0.61558545 0.09976023 0.3740052\n",
      " 0.87120867 0.84955734 0.16121519 0.9449562  0.7464476  0.50332075\n",
      " 0.0742254  0.4625407  0.15860623 0.9568918  0.33061886 0.3144539 ]\n",
      "Step: 300 , Reward:  -0.43314638636022834 weights:  [0.01817629 0.71571815 0.33457786 0.45947072 0.25881046 0.0622552\n",
      " 0.7709644  0.36026174 0.96875376 0.803655   0.6533631  0.677564\n",
      " 0.8193797  0.14768288 0.25750774 0.03960487 0.6916252  0.27314568\n",
      " 0.2532348  0.24428871 0.36531758 0.07923019 0.5498467  0.14466998]\n",
      "Step: 301 , Reward:  -0.5246874715355798 weights:  [0.6836914  0.16519046 0.9194819  0.89966244 0.31524122 0.60980874\n",
      " 0.9627124  0.66983527 0.24705493 0.8365756  0.08627117 0.21632075\n",
      " 0.29751658 0.6323726  0.7393245  0.95135593 0.2804131  0.7433319\n",
      " 0.5662722  0.05352423 0.46932057 0.5818937  0.04759952 0.7329427 ]\n",
      "Step: 302 , Reward:  -0.24744815089382138 weights:  [0.14637738 0.47397247 0.7401478  0.89844584 0.18968382 0.18116057\n",
      " 0.94279265 0.26980835 0.6063818  0.0305267  0.5029772  0.67029804\n",
      " 0.6166882  0.10408667 0.5197994  0.6949052  0.22840774 0.12049386\n",
      " 0.7280634  0.7454729  0.71142286 0.8082086  0.7480675  0.14188367]\n",
      "Step: 303 , Reward:  -0.15346657003416675 weights:  [0.35510486 0.22481775 0.49102798 0.43251655 0.06297413 0.07166269\n",
      " 0.8031667  0.09164995 0.65105003 0.6877828  0.10067746 0.55851626\n",
      " 0.42792445 0.2919144  0.6493413  0.6755208  0.2666807  0.6402209\n",
      " 0.27208942 0.88388056 0.1657053  0.5048134  0.2816741  0.18124747]\n",
      "Step: 304 , Reward:  -0.09589858130192455 weights:  [0.3328869  0.0272496  0.9866781  0.8430767  0.6647243  0.18548402\n",
      " 0.5869193  0.33231217 0.8621459  0.94973326 0.09649175 0.5805305\n",
      " 0.85284656 0.440312   0.3121936  0.9605069  0.00575706 0.19503662\n",
      " 0.18979833 0.905506   0.6350267  0.6700746  0.8279741  0.22355103]\n",
      "Step: 305 , Reward:  -0.09568921086685274 weights:  [0.49666667 0.20658869 0.6560371  0.54285103 0.96632147 0.1089817\n",
      " 0.34493798 0.56687313 0.94644666 0.05753872 0.9264168  0.95038474\n",
      " 0.8432996  0.14056402 0.31240016 0.06374851 0.32584321 0.7642395\n",
      " 0.29025534 0.04988718 0.6953231  0.14315268 0.8095689  0.8052206 ]\n",
      "Step: 306 , Reward:  0.11022959324336551 weights:  [0.03938359 0.01174623 0.15074694 0.9022679  0.52513975 0.2660194\n",
      " 0.55143034 0.8505415  0.92243433 0.40224153 0.51591194 0.03925508\n",
      " 0.97027016 0.42549437 0.48071948 0.52129006 0.63782275 0.3834292\n",
      " 0.24828717 0.7188139  0.27709174 0.14678884 0.56248826 0.06311411]\n",
      "Step: 307 , Reward:  0.21370537020270722 weights:  [0.11903876 0.6916903  0.8130864  0.03840441 0.37692335 0.44224334\n",
      " 0.5968126  0.6738016  0.46798128 0.961144   0.80055225 0.93919754\n",
      " 0.4932242  0.61532867 0.04463401 0.5801295  0.31019568 0.31951755\n",
      " 0.8610313  0.4244364  0.5761092  0.14496368 0.434717   0.88797945]\n",
      "Step: 308 , Reward:  0.019287361940055574 weights:  [0.7644165  0.9126315  0.41367427 0.15919757 0.6134381  0.23296523\n",
      " 0.00663796 0.4819899  0.69205225 0.5035777  0.09728929 0.13567635\n",
      " 0.75152075 0.93568516 0.5111635  0.7458238  0.5659225  0.79821706\n",
      " 0.46998736 0.4171819  0.4555068  0.04695871 0.91011864 0.81233555]\n",
      "Step: 309 , Reward:  0.12692544571628897 weights:  [0.13365215 0.63495165 0.98545516 0.5634698  0.23201138 0.2613703\n",
      " 0.9132252  0.91017056 0.46834087 0.5339345  0.3124845  0.2808273\n",
      " 0.76410973 0.9909533  0.32468927 0.4325106  0.03409213 0.41490123\n",
      " 0.67751795 0.33225018 0.9512156  0.19032231 0.63574696 0.9428129 ]\n",
      "Step: 310 , Reward:  0.17008994866395982 weights:  [0.28591335 0.65361625 0.47004864 0.83456266 0.6252653  0.3046587\n",
      " 0.1389524  0.38719636 0.9570384  0.34921205 0.9875157  0.08797586\n",
      " 0.29924107 0.41509813 0.32534534 0.5147488  0.84294945 0.16430074\n",
      " 0.99812984 0.47966635 0.4723269  0.35966223 0.77949643 0.60923123]\n",
      "Step: 311 , Reward:  0.01165920077566844 weights:  [0.03357872 0.43377143 0.6139947  0.97291744 0.15269926 0.06620809\n",
      " 0.24121222 0.58592427 0.9536704  0.19738042 0.4035499  0.04544738\n",
      " 0.9379219  0.97842073 0.18424603 0.83624417 0.0222795  0.818333\n",
      " 0.13252735 0.7925113  0.39812097 0.30552608 0.02728587 0.09484342]\n",
      "Step: 312 , Reward:  0.1513394609159948 weights:  [0.13378033 0.33778408 0.0852879  0.19704127 0.8617743  0.190698\n",
      " 0.92862153 0.6560847  0.82989216 0.18775994 0.41492352 0.5037707\n",
      " 0.47144356 0.57948744 0.9653326  0.09569508 0.22191161 0.29977077\n",
      " 0.11290473 0.22077313 0.83238035 0.45587698 0.28628308 0.20240414]\n",
      "Step: 313 , Reward:  0.2795942206142185 weights:  [0.05073726 0.14503977 0.05041525 0.13953143 0.47505417 0.32217216\n",
      " 0.817673   0.4797023  0.5185465  0.7463891  0.45334676 0.13721511\n",
      " 0.7998928  0.22148728 0.04413432 0.7500273  0.5603124  0.3687843\n",
      " 0.15601185 0.4712153  0.7911234  0.44430134 0.3184731  0.05140299]\n",
      "Step: 314 , Reward:  0.6787937994162279 weights:  [0.11313459 0.73697007 0.35096964 0.5915291  0.7938     0.18169856\n",
      " 0.7539927  0.00821504 0.48106718 0.85591465 0.27893898 0.81614304\n",
      " 0.9137893  0.91019785 0.7587972  0.9149562  0.24571538 0.96179163\n",
      " 0.82180405 0.11021143 0.13833869 0.22664103 0.57369816 0.3331917 ]\n",
      "Step: 315 , Reward:  0.8241123477462232 weights:  [0.33657777 0.22080594 0.76572466 0.2522406  0.16667888 0.78926134\n",
      " 0.2956884  0.23824355 0.7684903  0.9168073  0.5739795  0.10881555\n",
      " 0.5871388  0.07235628 0.09255192 0.24592826 0.05593234 0.12554905\n",
      " 0.633047   0.8961388  0.06848833 0.5236383  0.42158425 0.6849194 ]\n",
      "Step: 316 , Reward:  0.747606556240551 weights:  [0.6199644  0.11337319 0.8943336  0.8251717  0.01747146 0.7971778\n",
      " 0.27478418 0.7798635  0.20527378 0.7564095  0.8955112  0.49772087\n",
      " 0.6409486  0.06169152 0.5004267  0.18028298 0.79314446 0.72483647\n",
      " 0.5396824  0.90476274 0.00785768 0.9240904  0.2504995  0.77807987]\n",
      "Step: 317 , Reward:  0.5422664735954558 weights:  [0.29854876 0.82672274 0.517547   0.48260468 0.9075426  0.0493634\n",
      " 0.718116   0.25597817 0.7284225  0.03635803 0.04380089 0.07728443\n",
      " 0.4362566  0.02962241 0.11858258 0.37799376 0.91814715 0.823635\n",
      " 0.09467259 0.75640273 0.29120696 0.21193236 0.77352214 0.0353893 ]\n",
      "Step: 318 , Reward:  0.552399757291071 weights:  [0.93154156 0.57927006 0.39498636 0.34220254 0.74875224 0.7901904\n",
      " 0.88479906 0.49320066 0.5038291  0.27127862 0.45891768 0.86605465\n",
      " 0.33584023 0.8931023  0.7567674  0.54485995 0.0874325  0.7175102\n",
      " 0.26348895 0.4148041  0.98705745 0.24869028 0.41546544 0.79951286]\n",
      "Step: 319 , Reward:  0.45875437053960466 weights:  [0.1318948  0.9525827  0.7523156  0.91555685 0.7956136  0.59157777\n",
      " 0.18576157 0.08298719 0.9270786  0.06986794 0.09810188 0.3242405\n",
      " 0.44232184 0.391308   0.8865993  0.67915714 0.56150085 0.18700844\n",
      " 0.9013176  0.43349338 0.05430898 0.6128888  0.7245092  0.01406407]\n",
      "Step: 320 , Reward:  0.3621851527796884 weights:  [0.74749047 0.42634845 0.08041471 0.72222567 0.66512775 0.572792\n",
      " 0.4181174  0.2585088  0.51896125 0.86713296 0.5633512  0.94881153\n",
      " 0.9588893  0.64453864 0.06036648 0.2593549  0.7155753  0.5559066\n",
      " 0.7139771  0.2712875  0.01106986 0.28125897 0.41608188 0.41235423]\n",
      "Step: 321 , Reward:  0.040090860374193925 weights:  [0.30127558 0.9270214  0.33816648 0.90076387 0.72979355 0.44266427\n",
      " 0.5663234  0.32475623 0.5784843  0.28199703 0.83191437 0.8800415\n",
      " 0.4801815  0.75104004 0.1865387  0.15401614 0.5111025  0.6725175\n",
      " 0.31322685 0.9430839  0.98434055 0.4088256  0.26944363 0.3618742 ]\n",
      "Step: 322 , Reward:  0.03193960645477323 weights:  [0.42791542 0.54322916 0.8203695  0.42256415 0.6818112  0.82447445\n",
      " 0.94405496 0.03518045 0.9699088  0.53742206 0.8343917  0.2973165\n",
      " 0.56999826 0.9806627  0.09149644 0.01513341 0.77054334 0.57147473\n",
      " 0.00134712 0.85568523 0.03897047 0.05685514 0.9644437  0.96024084]\n",
      "Step: 323 , Reward:  0.02064420343475635 weights:  [0.62521344 0.26671582 0.25449434 0.11176842 0.09870905 0.3885693\n",
      " 0.80342996 0.8839352  0.69676095 0.36295813 0.82372963 0.5520712\n",
      " 0.88384056 0.13928646 0.0469321  0.03162849 0.6773418  0.4394336\n",
      " 0.14825475 0.5670957  0.1747087  0.9432281  0.93059003 0.64020556]\n",
      "Step: 324 , Reward:  -0.06815111844353884 weights:  [0.32750857 0.16673464 0.6507055  0.593633   0.20635667 0.88875663\n",
      " 0.28384936 0.8135492  0.4413463  0.65804976 0.2712832  0.25547358\n",
      " 0.11432931 0.898085   0.4694862  0.7749122  0.6643631  0.7169553\n",
      " 0.93351686 0.94023275 0.690743   0.5693377  0.6589903  0.9413828 ]\n",
      "Step: 325 , Reward:  0.03343543758659319 weights:  [0.14733693 0.22362912 0.79952985 0.29251552 0.15473351 0.334045\n",
      " 0.7685672  0.9986787  0.7750238  0.3948432  0.16998401 0.8282528\n",
      " 0.0506883  0.84631574 0.05884081 0.19503516 0.144449   0.53987074\n",
      " 0.893615   0.18720555 0.7271359  0.11463422 0.06558865 0.9737619 ]\n",
      "Step: 326 , Reward:  0.39090974796433453 weights:  [0.27837682 0.49532294 0.18822321 0.7807052  0.36929286 0.82854486\n",
      " 0.03962719 0.46779352 0.8975502  0.4852506  0.02701458 0.44078997\n",
      " 0.95734453 0.75222677 0.50728554 0.19432804 0.91257644 0.07277355\n",
      " 0.12068453 0.28946015 0.49325627 0.9082109  0.8050498  0.4848875 ]\n",
      "Step: 327 , Reward:  0.5845196335742494 weights:  [0.08549246 0.25020003 0.4643147  0.4321652  0.5745331  0.04398027\n",
      " 0.23128152 0.07936096 0.3717184  0.6490652  0.11293885 0.87485456\n",
      " 0.9820609  0.9612103  0.57899    0.20718107 0.05171549 0.4251699\n",
      " 0.26422173 0.9299556  0.38329008 0.48907763 0.06158596 0.02641079]\n",
      "Step: 328 , Reward:  0.5267937613297732 weights:  [0.9418078  0.02647838 0.16832674 0.18331769 0.3199222  0.04742566\n",
      " 0.07360461 0.5542249  0.5846117  0.73735416 0.86413044 0.60330296\n",
      " 0.913399   0.13622099 0.6516385  0.16000637 0.12954807 0.9647157\n",
      " 0.8076973  0.8101842  0.48418874 0.71528304 0.47783574 0.70160747]\n",
      "Step: 329 , Reward:  0.593652004631677 weights:  [0.72067523 0.49311924 0.49084392 0.01553798 0.18583536 0.5424233\n",
      " 0.56638163 0.8076477  0.9649149  0.6718481  0.6808011  0.1927431\n",
      " 0.39836264 0.6609275  0.59113485 0.23184258 0.11634383 0.01448113\n",
      " 0.97353995 0.8231752  0.09195387 0.07252654 0.84055007 0.41381863]\n",
      "Step: 330 , Reward:  0.22557376541526475 weights:  [0.14831632 0.04895449 0.07274121 0.7100105  0.49242532 0.09643739\n",
      " 0.36053723 0.6806186  0.5621848  0.96386886 0.8433938  0.89582694\n",
      " 0.6806231  0.7939061  0.6885647  0.53265256 0.7852833  0.9864838\n",
      " 0.4290533  0.16404906 0.89853513 0.09074739 0.10015246 0.37321338]\n",
      "Step: 331 , Reward:  0.04867548743730778 weights:  [0.64879966 0.3592661  0.83721507 0.29654425 0.7923726  0.71576583\n",
      " 0.47434738 0.6892416  0.3829731  0.3879817  0.54545194 0.98287356\n",
      " 0.13224456 0.60922986 0.7992891  0.6689566  0.5906535  0.97463405\n",
      " 0.8315191  0.12751913 0.33594447 0.06357518 0.10992929 0.78817725]\n",
      "Step: 332 , Reward:  0.11336643165159889 weights:  [0.78639674 0.88847154 0.27933356 0.10596895 0.60049057 0.65144247\n",
      " 0.8926059  0.21213737 0.6816788  0.951488   0.878695   0.36305076\n",
      " 0.8667058  0.7692579  0.89799935 0.73919404 0.6613621  0.6784136\n",
      " 0.3520081  0.25888193 0.37599754 0.06153843 0.481998   0.521004  ]\n",
      "Step: 333 , Reward:  0.10137317326565104 weights:  [0.2625661  0.6416398  0.20087194 0.90824854 0.34313947 0.55078024\n",
      " 0.975164   0.5712707  0.51264715 0.2306925  0.30614418 0.13609415\n",
      " 0.08918878 0.78098655 0.10656592 0.35339874 0.21263388 0.45758212\n",
      " 0.9623995  0.61927986 0.02281091 0.8205564  0.698256   0.9676552 ]\n",
      "Step: 334 , Reward:  -0.010268746501172871 weights:  [0.6694498  0.35523176 0.8833051  0.3151512  0.42655656 0.20955786\n",
      " 0.8446767  0.08800626 0.64044565 0.95579165 0.31253982 0.12696195\n",
      " 0.45649132 0.07546404 0.1607441  0.12752908 0.12012702 0.7063746\n",
      " 0.52698153 0.05027476 0.6844015  0.5034285  0.883829   0.9922699 ]\n",
      "Step: 335 , Reward:  0.3464449167342495 weights:  [0.6628754  0.47589457 0.35894972 0.89091456 0.17087927 0.02590922\n",
      " 0.92157257 0.6846174  0.28161418 0.8620327  0.19368228 0.10204124\n",
      " 0.97214913 0.1973261  0.20220476 0.03168428 0.83801615 0.56893146\n",
      " 0.90653515 0.20121327 0.6244437  0.5562195  0.8934724  0.6455351 ]\n",
      "Step: 336 , Reward:  0.5686477867454961 weights:  [0.7364021  0.18194804 0.61924934 0.661818   0.24158818 0.11277264\n",
      " 0.10715032 0.7629068  0.1661155  0.88616204 0.85280305 0.10675424\n",
      " 0.78077114 0.8960996  0.8624431  0.15288553 0.01151082 0.5626963\n",
      " 0.01064393 0.02123797 0.89971864 0.13904345 0.73762757 0.65770614]\n",
      "Step: 337 , Reward:  0.33710136009786895 weights:  [0.2378641  0.29366732 0.21497956 0.37201795 0.02551642 0.90921783\n",
      " 0.8569418  0.18430448 0.9044373  0.23320773 0.22672397 0.24065015\n",
      " 0.9767457  0.06281903 0.04964411 0.1297968  0.81692576 0.43507147\n",
      " 0.22428387 0.90521914 0.64888215 0.08281589 0.12225196 0.05448443]\n",
      "Step: 338 , Reward:  0.6647356552013326 weights:  [0.24145848 0.5314258  0.25692195 0.9556769  0.13902545 0.01230431\n",
      " 0.4646114  0.09135601 0.39713466 0.14303148 0.84348696 0.1764327\n",
      " 0.7101829  0.8027828  0.85327846 0.4697127  0.91339034 0.61201423\n",
      " 0.47916913 0.34730256 0.22914225 0.69301945 0.02508759 0.8500843 ]\n",
      "Step: 339 , Reward:  0.4167604922174736 weights:  [0.64906985 0.92366165 0.5586171  0.94669294 0.24756235 0.889417\n",
      " 0.7037902  0.65665925 0.13110137 0.22188559 0.60469884 0.15402147\n",
      " 0.6618717  0.47174817 0.15262944 0.22314423 0.6285686  0.22710782\n",
      " 0.8581344  0.7888001  0.95788336 0.01238775 0.9704808  0.895511  ]\n",
      "Step: 340 , Reward:  0.2128047499505488 weights:  [0.29774648 0.3185776  0.8114189  0.50178045 0.19245604 0.21860918\n",
      " 0.98042107 0.02769765 0.8455936  0.9124167  0.3682744  0.81633914\n",
      " 0.714661   0.3433953  0.5644526  0.49627692 0.19001389 0.62552834\n",
      " 0.10158879 0.9633745  0.37221128 0.7345586  0.2621824  0.84918666]\n",
      "Step: 341 , Reward:  -0.09486376670345269 weights:  [0.04796979 0.3126266  0.8940512  0.8512734  0.92948234 0.9005623\n",
      " 0.31297094 0.8300451  0.4979422  0.06307679 0.17976817 0.16560787\n",
      " 0.81270766 0.55323523 0.85996735 0.07605171 0.6689939  0.5765287\n",
      " 0.46250927 0.12496153 0.37659723 0.53844255 0.3626272  0.19281307]\n",
      "Step: 342 , Reward:  -0.28923606639416427 weights:  [0.15424371 0.86458385 0.02207494 0.3967622  0.44951665 0.9285742\n",
      " 0.56077826 0.6245064  0.8110657  0.19162008 0.4515658  0.6871687\n",
      " 0.29435503 0.91128314 0.50113297 0.5519518  0.4913005  0.66394484\n",
      " 0.05293941 0.41855434 0.82683194 0.32239172 0.8118404  0.8958527 ]\n",
      "Step: 343 , Reward:  -0.25473626285374124 weights:  [0.06740746 0.8856927  0.740488   0.8160093  0.44947422 0.17765588\n",
      " 0.70177585 0.12975094 0.52425367 0.1996117  0.5681518  0.63193595\n",
      " 0.6020817  0.40414008 0.86860365 0.41805765 0.96088994 0.70003974\n",
      " 0.74522465 0.97288764 0.92654455 0.823101   0.26380035 0.07984957]\n",
      "Step: 344 , Reward:  -0.22171588874014317 weights:  [0.6394739  0.5756975  0.17910334 0.23074767 0.14230847 0.935387\n",
      " 0.7407882  0.9971051  0.3057384  0.79061025 0.04642698 0.23797408\n",
      " 0.502929   0.8067045  0.24615881 0.77736497 0.7109584  0.12261799\n",
      " 0.3255946  0.11920732 0.66682154 0.5943623  0.4203945  0.9214751 ]\n",
      "Step: 345 , Reward:  -0.08234127796189485 weights:  [0.38752878 0.8622661  0.07536158 0.9576133  0.5954137  0.17861727\n",
      " 0.2784481  0.48873857 0.18803644 0.51026523 0.40606248 0.46610513\n",
      " 0.86688656 0.90793383 0.155177   0.942365   0.94235283 0.25278562\n",
      " 0.25480455 0.9029359  0.27994427 0.28228197 0.96961206 0.24613991]\n",
      "Step: 346 , Reward:  0.13630555619376175 weights:  [0.97059226 0.835055   0.25670987 0.01546213 0.8924724  0.01395816\n",
      " 0.35142434 0.29046464 0.19740057 0.9169687  0.27081236 0.20614576\n",
      " 0.7706436  0.10890782 0.43279883 0.87713933 0.09573135 0.39910695\n",
      " 0.90510213 0.08274728 0.4919086  0.769717   0.58907235 0.76441956]\n",
      "Step: 347 , Reward:  -0.20960205651493952 weights:  [0.08559853 0.9035729  0.12906331 0.800286   0.7617327  0.56554276\n",
      " 0.5500974  0.25416934 0.5584486  0.36786178 0.62042654 0.48307937\n",
      " 0.158396   0.55942214 0.2971699  0.21342742 0.16469404 0.9500604\n",
      " 0.18707418 0.9454131  0.39283383 0.60395706 0.3729363  0.27213907]\n",
      "Step: 348 , Reward:  -0.10362314905037208 weights:  [0.22008806 0.02247888 0.33235866 0.3145046  0.3951129  0.2075325\n",
      " 0.72252357 0.01655734 0.69602764 0.98383915 0.01695448 0.902364\n",
      " 0.987599   0.04717356 0.6373383  0.01182288 0.5364939  0.00953653\n",
      " 0.59296405 0.34497213 0.18472442 0.94682926 0.26297462 0.041374  ]\n",
      "Step: 349 , Reward:  0.18014036292834262 weights:  [0.32206357 0.46536443 0.12152204 0.62024224 0.71007353 0.10478106\n",
      " 0.6708902  0.33061993 0.06122553 0.9687152  0.16891643 0.65739554\n",
      " 0.06988883 0.5004199  0.19157419 0.5675609  0.5901247  0.9663302\n",
      " 0.241382   0.75629246 0.7659929  0.2775947  0.13042197 0.12263989]\n",
      "Step: 350 , Reward:  0.04218316834013185 weights:  [0.620813   0.8206686  0.06368741 0.13534006 0.01335922 0.05583778\n",
      " 0.42328796 0.43643123 0.10662043 0.7574711  0.09768245 0.7674282\n",
      " 0.68958825 0.86592484 0.21177167 0.42315447 0.5986251  0.11290035\n",
      " 0.5499854  0.5610351  0.03619644 0.04163167 0.3161403  0.65078986]\n",
      "Step: 351 , Reward:  -0.09292799996872335 weights:  [0.98023885 0.83084893 0.19703633 0.5864722  0.12493384 0.35383898\n",
      " 0.94326067 0.5511111  0.9922569  0.7619679  0.89868605 0.05052954\n",
      " 0.40339836 0.9552752  0.8599177  0.37399065 0.49699622 0.79121006\n",
      " 0.7116564  0.5134293  0.55087537 0.4058978  0.02664044 0.49634942]\n",
      "Step: 352 , Reward:  0.02241223644893783 weights:  [0.23491153 0.0677411  0.21065801 0.26608127 0.88835645 0.809492\n",
      " 0.16074446 0.8743771  0.14973319 0.93857527 0.18316758 0.05056128\n",
      " 0.597274   0.10004696 0.04593083 0.5861773  0.5204802  0.653841\n",
      " 0.7546371  0.45243317 0.51389813 0.37378478 0.49232233 0.45113057]\n",
      "Step: 353 , Reward:  0.020946913615353613 weights:  [0.09591559 0.28209263 0.32774585 0.41340703 0.98689246 0.3178287\n",
      " 0.93107545 0.80743146 0.9353357  0.7092702  0.0501568  0.9643214\n",
      " 0.83858883 0.7119914  0.86294043 0.15661067 0.15815261 0.87947106\n",
      " 0.5541195  0.37186167 0.9614206  0.19101587 0.05901787 0.4663358 ]\n",
      "Step: 354 , Reward:  0.21147358237661767 weights:  [0.12654111 0.74123347 0.06556079 0.19908345 0.09911984 0.10023621\n",
      " 0.976128   0.6958456  0.9227404  0.52441454 0.9370962  0.03282675\n",
      " 0.78603554 0.00934365 0.9318909  0.13422972 0.21152794 0.97259873\n",
      " 0.24243024 0.6221502  0.84823316 0.88849324 0.10265097 0.6640989 ]\n",
      "Step: 355 , Reward:  0.41921413290535736 weights:  [0.5818024  0.6157682  0.80474186 0.96561897 0.61599386 0.71801233\n",
      " 0.82145697 0.02455088 0.6670864  0.21883324 0.9334439  0.59488875\n",
      " 0.43664688 0.31087065 0.19747025 0.62144226 0.34621567 0.03472644\n",
      " 0.26623672 0.00701487 0.49839443 0.1964792  0.34226137 0.7910944 ]\n",
      "Step: 356 , Reward:  0.7190338367346488 weights:  [0.5179612  0.8775492  0.1398747  0.08233035 0.9811063  0.58450425\n",
      " 0.74679255 0.48918852 0.96648884 0.6118327  0.29487127 0.7016371\n",
      " 0.6114843  0.04203746 0.5649215  0.35621887 0.7566635  0.62078947\n",
      " 0.5428201  0.5989025  0.05060905 0.4311619  0.48563883 0.9118118 ]\n",
      "Step: 357 , Reward:  0.8400365344684404 weights:  [0.25308943 0.30797178 0.75653625 0.20656195 0.82516897 0.91422075\n",
      " 0.25860876 0.42125311 0.9263532  0.66905606 0.31728497 0.18072799\n",
      " 0.72906524 0.4205966  0.53053755 0.71910965 0.9046936  0.9067885\n",
      " 0.4328184  0.44238204 0.2048125  0.7868576  0.00659454 0.08348623]\n",
      "Step: 358 , Reward:  0.9480049095620108 weights:  [0.9319836  0.7737407  0.14279878 0.9584187  0.28860337 0.7806879\n",
      " 0.11265308 0.0540092  0.15413699 0.255652   0.8497206  0.41999304\n",
      " 0.75079536 0.8870672  0.20025471 0.9180863  0.23604894 0.61442107\n",
      " 0.28647807 0.54735696 0.34324336 0.63771045 0.48688486 0.11018705]\n",
      "Step: 359 , Reward:  0.7849727587381485 weights:  [0.23518002 0.4854809  0.43038955 0.37002137 0.6668834  0.45838767\n",
      " 0.97789335 0.560873   0.37894768 0.8501588  0.53588444 0.68496865\n",
      " 0.05203432 0.64168185 0.5120831  0.41980243 0.90294194 0.41983926\n",
      " 0.06574193 0.90708184 0.53964156 0.20291013 0.8100151  0.06483585]\n",
      "Step: 360 , Reward:  0.8846460045624109 weights:  [0.5100015  0.7546862  0.59432423 0.25227118 0.55590516 0.9752563\n",
      " 0.27100837 0.78950596 0.43017977 0.9639276  0.03857324 0.03437522\n",
      " 0.4870422  0.09346956 0.7382109  0.45552424 0.41730523 0.84466815\n",
      " 0.8558674  0.09798384 0.42945188 0.38439393 0.591186   0.61023146]\n",
      "Step: 361 , Reward:  0.9227291686624088 weights:  [0.21305218 0.20956397 0.6869768  0.2531378  0.64793545 0.93018854\n",
      " 0.3839047  0.33039296 0.342075   0.4259041  0.4127431  0.21766835\n",
      " 0.8564569  0.525516   0.47435275 0.49454668 0.52179384 0.15597048\n",
      " 0.9032155  0.5621509  0.14664164 0.2992594  0.29305524 0.04694843]\n",
      "Step: 362 , Reward:  1.926989154334028 weights:  [1.8404400e-01 3.1071210e-01 3.5546291e-01 1.9642502e-01 4.6477652e-01\n",
      " 7.1137941e-01 7.8237295e-01 8.0850017e-01 1.7453989e-01 2.8790197e-01\n",
      " 1.1849281e-01 5.2987635e-02 8.2155144e-01 5.0007540e-01 4.6390027e-02\n",
      " 4.1608098e-01 7.0609486e-01 7.7710456e-01 3.9857626e-04 6.5547109e-01\n",
      " 3.9592940e-01 9.7115272e-01 2.0275265e-02 4.6525428e-01]\n",
      "Step: 363 , Reward:  1.9344169437475505 weights:  [0.66519964 0.18148407 0.037595   0.06352881 0.50846094 0.5069678\n",
      " 0.7452942  0.63724875 0.16165558 0.23981759 0.4305995  0.6664429\n",
      " 0.9539983  0.66802007 0.10056445 0.825425   0.7015817  0.87967914\n",
      " 0.5410839  0.7489353  0.19649374 0.0144541  0.9174596  0.35313195]\n",
      "Step: 364 , Reward:  1.566278762997107 weights:  [0.8565381  0.8133979  0.01253289 0.26801395 0.50032395 0.02295411\n",
      " 0.9767761  0.15027672 0.6306746  0.9260938  0.29328442 0.25877362\n",
      " 0.72184765 0.8237179  0.9370692  0.2647966  0.71486056 0.5410371\n",
      " 0.4213246  0.12006199 0.02684525 0.65162075 0.22632831 0.18808421]\n",
      "Step: 365 , Reward:  1.0215908682242338 weights:  [0.57418656 0.7278085  0.96623    0.35993224 0.5510025  0.5268234\n",
      " 0.7456057  0.24279335 0.01825956 0.90198123 0.97270817 0.8880453\n",
      " 0.08226368 0.9623171  0.04355255 0.47326934 0.42465073 0.6911777\n",
      " 0.85770583 0.6927416  0.40885815 0.4306389  0.0223051  0.04624239]\n",
      "Step: 366 , Reward:  1.3137201842918305 weights:  [0.5619139  0.17665675 0.33114615 0.22007543 0.22496566 0.1560829\n",
      " 0.46193716 0.82947254 0.7140473  0.825969   0.14770985 0.31245226\n",
      " 0.7911647  0.24371222 0.5428759  0.7381957  0.800766   0.9951831\n",
      " 0.00979456 0.8067315  0.9529125  0.36960405 0.6439296  0.22798085]\n",
      "Step: 367 , Reward:  1.0687124214209498 weights:  [0.42764857 0.52199936 0.7490223  0.7433144  0.09636521 0.9507268\n",
      " 0.0261218  0.48614994 0.6343454  0.85156626 0.43232846 0.02893552\n",
      " 0.20939109 0.10847223 0.78026587 0.8224344  0.47419825 0.19826218\n",
      " 0.18735483 0.8083545  0.36269456 0.12151006 0.42191365 0.4687967 ]\n",
      "Step: 368 , Reward:  1.0920087837915466 weights:  [0.54379076 0.34524918 0.21885774 0.77208596 0.43980187 0.8197886\n",
      " 0.9141641  0.04287025 0.92947096 0.8048829  0.24247742 0.24894795\n",
      " 0.6722971  0.79465115 0.49465883 0.46267277 0.16081214 0.8844208\n",
      " 0.84639025 0.5344882  0.7785615  0.17082852 0.11003047 0.538781  ]\n",
      "Step: 369 , Reward:  0.982816051353748 weights:  [0.88389957 0.6968199  0.38811725 0.32858628 0.5984145  0.32822096\n",
      " 0.47298262 0.5327989  0.4068609  0.9626727  0.54339606 0.82131165\n",
      " 0.45436913 0.9572202  0.2368705  0.13254902 0.28583503 0.89988685\n",
      " 0.74556303 0.8332896  0.42804456 0.32862544 0.6629454  0.38649875]\n",
      "Step: 370 , Reward:  0.7435309697455388 weights:  [0.02296981 0.83142304 0.8082873  0.47683728 0.03101346 0.53030473\n",
      " 0.94361997 0.98319924 0.59834015 0.44664443 0.44894078 0.56296855\n",
      " 0.98217535 0.9776124  0.4100044  0.12412825 0.20307422 0.6784937\n",
      " 0.7755643  0.56734085 0.30780175 0.96252173 0.30486995 0.495604  ]\n",
      "Step: 371 , Reward:  0.7378472146346258 weights:  [0.65986884 0.1532526  0.83876824 0.3993588  0.7087014  0.37333298\n",
      " 0.44873855 0.8130748  0.8725601  0.7380065  0.34901655 0.28221613\n",
      " 0.12682092 0.8152864  0.30357045 0.13056186 0.68670714 0.05501127\n",
      " 0.6209731  0.01688737 0.9012844  0.91661155 0.08552247 0.54343444]\n",
      "Step: 372 , Reward:  0.7988447104583514 weights:  [0.14834106 0.29321855 0.914985   0.121961   0.96703696 0.80161965\n",
      " 0.7911489  0.16994151 0.58298033 0.13085717 0.3833353  0.5790153\n",
      " 0.415963   0.48657465 0.85490084 0.33067316 0.32421064 0.11549225\n",
      " 0.07243696 0.32520694 0.930405   0.17557094 0.27106273 0.89048445]\n",
      "Step: 373 , Reward:  0.6337098900805563 weights:  [0.06517413 0.61931694 0.47370964 0.5813835  0.973827   0.08562481\n",
      " 0.2491019  0.7881086  0.07844961 0.2952932  0.23925254 0.4454474\n",
      " 0.10820457 0.21182713 0.3080978  0.8639803  0.24440563 0.30917498\n",
      " 0.9315049  0.21663523 0.8187933  0.15510753 0.9228909  0.5951441 ]\n",
      "Step: 374 , Reward:  0.86953471622364 weights:  [0.06446019 0.02676907 0.9498509  0.05499893 0.9888489  0.09663939\n",
      " 0.23856702 0.9255539  0.8125454  0.59073645 0.33096308 0.04218805\n",
      " 0.62731254 0.69325995 0.5521066  0.3531927  0.7486933  0.14896339\n",
      " 0.03534043 0.54252493 0.7915143  0.33894232 0.675477   0.64289397]\n",
      "Step: 375 , Reward:  0.7115884625422932 weights:  [0.12738186 0.03103012 0.7625017  0.68397933 0.10280561 0.28565794\n",
      " 0.33959526 0.280797   0.7709739  0.7819102  0.3373819  0.37237084\n",
      " 0.9873949  0.947271   0.6998725  0.5471431  0.805734   0.8344681\n",
      " 0.07910013 0.2797907  0.13907161 0.50224286 0.13447371 0.9919874 ]\n",
      "Step: 376 , Reward:  0.5891400682941155 weights:  [0.6995296  0.5012112  0.39281344 0.63958913 0.11553881 0.34836578\n",
      " 0.06421956 0.87876135 0.9637964  0.8855301  0.16467759 0.74957484\n",
      " 0.8149467  0.10451844 0.826282   0.13291737 0.06246048 0.05411884\n",
      " 0.777338   0.5255416  0.09836262 0.60967344 0.6258855  0.9309634 ]\n",
      "Step: 377 , Reward:  0.5085483103614247 weights:  [0.48435792 0.13255534 0.89395046 0.44263852 0.86711025 0.8411752\n",
      " 0.24114886 0.1026547  0.03294414 0.7380501  0.3312083  0.2797047\n",
      " 0.80973434 0.20670983 0.22699985 0.63806987 0.33622605 0.26983052\n",
      " 0.6162631  0.07375225 0.58254886 0.3183331  0.3309008  0.81810194]\n",
      "Step: 378 , Reward:  0.39670566356332004 weights:  [0.0267885  0.933859   0.3284303  0.64911383 0.705924   0.80411285\n",
      " 0.95562553 0.4492126  0.49904835 0.31633732 0.6454687  0.7791172\n",
      " 0.41397262 0.07178593 0.27315295 0.4716257  0.6967316  0.34647042\n",
      " 0.2552175  0.30027014 0.50942653 0.01812997 0.7205616  0.4857121 ]\n",
      "Step: 379 , Reward:  0.26288622754969143 weights:  [0.15411016 0.13459465 0.9050094  0.70241416 0.7602936  0.39318386\n",
      " 0.08097261 0.18372065 0.74902016 0.6205823  0.43143636 0.80684674\n",
      " 0.79391605 0.6066988  0.31644344 0.37158668 0.51856583 0.35907555\n",
      " 0.88919026 0.24549192 0.30768198 0.8489624  0.87372524 0.9047545 ]\n",
      "Step: 380 , Reward:  0.1400739471322201 weights:  [0.25358772 0.17940271 0.1353639  0.03487924 0.11630929 0.32835886\n",
      " 0.23479041 0.8165635  0.8026682  0.5227096  0.07405168 0.8829609\n",
      " 0.80992675 0.92144126 0.94045496 0.0356918  0.6631504  0.3025163\n",
      " 0.3460011  0.58047205 0.2769994  0.04632142 0.8451002  0.8619487 ]\n",
      "Step: 381 , Reward:  -0.0680669158259776 weights:  [0.4182089  0.3499571  0.2346808  0.0243524  0.1969296  0.1544916\n",
      " 0.59445703 0.42463082 0.34454885 0.9019978  0.10096407 0.05221161\n",
      " 0.2358045  0.09164447 0.19977275 0.25419194 0.89397204 0.1986798\n",
      " 0.7843873  0.8598054  0.13285673 0.5120419  0.40628248 0.5974952 ]\n",
      "Step: 382 , Reward:  0.05705080623095814 weights:  [0.4379111  0.28348613 0.21293828 0.5615947  0.72445834 0.1429475\n",
      " 0.28542626 0.24079931 0.7061334  0.5541142  0.6531935  0.6376599\n",
      " 0.51336694 0.7914003  0.28242007 0.57707006 0.15066129 0.12685212\n",
      " 0.7339885  0.267052   0.02268177 0.764802   0.8789812  0.29391718]\n",
      "Step: 383 , Reward:  -0.08239734760169938 weights:  [0.9209875  0.6724502  0.14293948 0.73555684 0.6831601  0.6597799\n",
      " 0.22077551 0.47120667 0.9468431  0.26554787 0.8673635  0.8633485\n",
      " 0.23250392 0.49532813 0.6314249  0.31234297 0.67726153 0.5226526\n",
      " 0.6439121  0.4840173  0.17520645 0.52241176 0.67523646 0.5953353 ]\n",
      "Step: 384 , Reward:  -0.06533129631064812 weights:  [0.32115972 0.5454962  0.49623644 0.8622049  0.13542205 0.16434383\n",
      " 0.92303574 0.50554174 0.07559457 0.1299887  0.04586565 0.06282118\n",
      " 0.49360698 0.7475084  0.74572116 0.13599181 0.42214757 0.23622897\n",
      " 0.6331237  0.6641234  0.6539451  0.5809088  0.14507607 0.8532075 ]\n",
      "Step: 385 , Reward:  -0.10757711998190854 weights:  [0.34227887 0.00608379 0.16245264 0.8661052  0.94805336 0.9084405\n",
      " 0.7480236  0.04256374 0.98772496 0.60635805 0.38056388 0.28226236\n",
      " 0.16589874 0.03860983 0.7897506  0.4977167  0.05277374 0.6563332\n",
      " 0.5331496  0.28486207 0.13501099 0.2077066  0.9350566  0.08058554]\n",
      "Step: 386 , Reward:  0.027847813742405227 weights:  [0.98020846 0.89229125 0.20266512 0.6573957  0.90922517 0.5166404\n",
      " 0.4307519  0.17373234 0.7910416  0.23800635 0.6521377  0.23409972\n",
      " 0.76854074 0.0847815  0.90718746 0.13229108 0.97382057 0.6707357\n",
      " 0.5318911  0.38801396 0.91232383 0.45797127 0.4931042  0.8561436 ]\n",
      "Step: 387 , Reward:  -0.13960376825358975 weights:  [0.5384763  0.74575007 0.03843886 0.09619951 0.3356654  0.07196087\n",
      " 0.9106935  0.50919956 0.12701574 0.2993098  0.31752673 0.26575512\n",
      " 0.7575587  0.55606216 0.7229466  0.12480035 0.02914682 0.9923662\n",
      " 0.89205    0.46967268 0.2738862  0.5165418  0.16566622 0.4899094 ]\n",
      "Step: 388 , Reward:  -0.17635365021942168 weights:  [0.9537971  0.9768224  0.05660525 0.0904195  0.35873604 0.0825136\n",
      " 0.5012518  0.33599126 0.75823724 0.40218264 0.50736487 0.07867819\n",
      " 0.9065541  0.1804817  0.75634897 0.48541385 0.04114202 0.28652048\n",
      " 0.40081617 0.89637774 0.64264935 0.5231955  0.8012722  0.16084102]\n",
      "Step: 389 , Reward:  -0.6978844405863862 weights:  [0.5131086  0.01807564 0.6343869  0.82860374 0.09886131 0.521777\n",
      " 0.6628027  0.8703053  0.8708087  0.22247446 0.50058126 0.7496399\n",
      " 0.988218   0.09006989 0.2755279  0.03914538 0.42337725 0.8864486\n",
      " 0.57779557 0.45887166 0.09554362 0.2795405  0.63542247 0.6941935 ]\n",
      "Step: 390 , Reward:  -0.6337029500295285 weights:  [0.01860264 0.06352073 0.31519556 0.23118943 0.4181968  0.65927756\n",
      " 0.16980276 0.14729434 0.42880452 0.53442174 0.3126055  0.6521661\n",
      " 0.4457947  0.7001641  0.2056798  0.06398275 0.7008052  0.05250314\n",
      " 0.49421263 0.287497   0.647509   0.535629   0.1660485  0.5123166 ]\n",
      "Step: 391 , Reward:  -0.5093215981933705 weights:  [0.40200678 0.40136978 0.6722466  0.55309546 0.8247989  0.57854915\n",
      " 0.9122977  0.40755686 0.12740746 0.9753558  0.35122216 0.81661826\n",
      " 0.9817877  0.03940174 0.1195913  0.5755576  0.22469619 0.57251626\n",
      " 0.6539128  0.35294276 0.12901711 0.11209679 0.51376945 0.9626702 ]\n",
      "Step: 392 , Reward:  -0.5933623631350357 weights:  [0.9493439  0.46633208 0.78004164 0.9189316  0.16676798 0.5092269\n",
      " 0.0303497  0.9614309  0.6034399  0.0726513  0.3969249  0.03622895\n",
      " 0.86704206 0.77173734 0.9489261  0.09071383 0.5781604  0.65250957\n",
      " 0.40040526 0.18650451 0.2435393  0.13298628 0.4376793  0.9476999 ]\n",
      "Step: 393 , Reward:  -0.5974648651038517 weights:  [0.04629049 0.9454743  0.2687779  0.02908459 0.93868387 0.0521141\n",
      " 0.8384278  0.51080513 0.693653   0.06684172 0.81164986 0.9523928\n",
      " 0.58451474 0.5296749  0.28066695 0.8025861  0.24320143 0.09860858\n",
      " 0.9316524  0.49165693 0.38661322 0.06588259 0.6069488  0.21595258]\n",
      "Step: 394 , Reward:  -0.40624191628627326 weights:  [0.6918927  0.7121234  0.00778979 0.6031772  0.54980373 0.5377558\n",
      " 0.9936637  0.7611327  0.47487664 0.6823921  0.44746026 0.12138996\n",
      " 0.57863194 0.75805056 0.40607995 0.39537227 0.44021553 0.02456942\n",
      " 0.14828348 0.99480104 0.46529368 0.04215324 0.40436292 0.963122  ]\n",
      "Step: 395 , Reward:  -0.03641592948267095 weights:  [0.7629498  0.42373767 0.6404252  0.6283519  0.7616627  0.4635299\n",
      " 0.94661415 0.21480247 0.5690705  0.36371613 0.19629604 0.41611516\n",
      " 0.7864214  0.5108126  0.5233337  0.2672434  0.06005821 0.88274777\n",
      " 0.05829886 0.8687654  0.43772507 0.5925142  0.31051034 0.5121602 ]\n",
      "Step: 396 , Reward:  0.1145817105793769 weights:  [0.59945357 0.8646304  0.08745748 0.01909801 0.3226543  0.07851291\n",
      " 0.67430186 0.56436795 0.7567001  0.65477204 0.91787255 0.14191511\n",
      " 0.14564294 0.7466866  0.08258942 0.7462777  0.33829248 0.25332355\n",
      " 0.08755672 0.57156193 0.6903105  0.23630357 0.6873878  0.7438292 ]\n",
      "Step: 397 , Reward:  0.164082371463761 weights:  [0.22634912 0.5856289  0.3617524  0.36679807 0.8662033  0.33718815\n",
      " 0.11092862 0.7354268  0.32645154 0.55685985 0.4310592  0.3525079\n",
      " 0.0378128  0.90085584 0.9662552  0.0110141  0.5340015  0.66853\n",
      " 0.14570263 0.25627938 0.41613734 0.2921238  0.46314472 0.42501843]\n",
      "Step: 398 , Reward:  0.08393816178388354 weights:  [0.21013084 0.7858956  0.3640486  0.9758159  0.42405617 0.5810489\n",
      " 0.9178455  0.1461539  0.45086405 0.37820223 0.21149147 0.60063064\n",
      " 0.07047537 0.23594218 0.68854773 0.44648987 0.48744917 0.1444425\n",
      " 0.11778322 0.23133013 0.83091396 0.3824557  0.09578487 0.9641764 ]\n",
      "Step: 399 , Reward:  -0.19004486625050845 weights:  [0.9833867  0.97617054 0.14989969 0.7884098  0.46720305 0.19287959\n",
      " 0.87708366 0.87819743 0.8549244  0.24432528 0.1414009  0.977427\n",
      " 0.46160567 0.6845056  0.1068953  0.42173237 0.44958782 0.7032987\n",
      " 0.5262561  0.98914516 0.11826289 0.75043154 0.22163022 0.32069474]\n",
      "Step: 400 , Reward:  -0.1011611341485136 weights:  [0.02349764 0.44735458 0.20438167 0.55762315 0.63858795 0.1393531\n",
      " 0.14489263 0.6682018  0.921108   0.06270394 0.72312844 0.861616\n",
      " 0.38066375 0.49209794 0.5127782  0.2835271  0.30908674 0.30507684\n",
      " 0.39877385 0.9709048  0.21881512 0.15922707 0.1796104  0.57933795]\n"
     ]
    }
   ],
   "source": [
    "model = SAC(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=train_env,\n",
    "    learning_rate=3e-4,\n",
    "    buffer_size=100_000,\n",
    "    batch_size=256,\n",
    "    ent_coef=\"auto\",\n",
    "    target_entropy=\"auto\",\n",
    "    verbose=1,\n",
    ").learn(total_timesteps=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49189955 0.5135144  0.4625722  0.48429513 0.5006337  0.49360344\n",
      " 0.5392966  0.50351894 0.54371846 0.5640096  0.4732304  0.5113648\n",
      " 0.5395701  0.5172388  0.46664128 0.5218054  0.5012927  0.55177593\n",
      " 0.4724523  0.45760816 0.40061602 0.4478844  0.47480032 0.5205927 ]\n"
     ]
    }
   ],
   "source": [
    "obs, _ = train_env.reset()\n",
    "actions, _ = model.predict(obs, deterministic=True)\n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04114959 0.04295777 0.03869623 0.04051344 0.04188024 0.04129213\n",
      " 0.04511457 0.0421216  0.04548447 0.04718192 0.03958783 0.04277795\n",
      " 0.04513744 0.04326933 0.03903662 0.04365135 0.04193537 0.04615851\n",
      " 0.03952274 0.03828096 0.03351332 0.03746753 0.03971916 0.0435499 ]\n"
     ]
    }
   ],
   "source": [
    "norm = actions / np.sum(actions)\n",
    "print(norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1 , Reward:  0.0 weights:  [0.04114959 0.04295777 0.03869623 0.04051344 0.04188024 0.04129213\n",
      " 0.04511457 0.0421216  0.04548447 0.04718192 0.03958783 0.04277795\n",
      " 0.04513744 0.04326933 0.03903662 0.04365135 0.04193537 0.04615851\n",
      " 0.03952274 0.03828096 0.03351332 0.03746753 0.03971916 0.0435499 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        , -0.14875537,\n",
       "        -0.20019796, -0.15338792, -0.13869327, -0.17764835, -0.16650148,\n",
       "        -0.13914135, -0.2101108 , -0.12466612, -0.1701478 , -0.22064877,\n",
       "        -0.14484504, -0.09933816, -0.26717907, -0.08440344, -0.20676866,\n",
       "        -0.24074557, -0.18713956, -0.06541503, -0.18698472, -0.25438488,\n",
       "        -0.40900496, -0.13383524, -0.20548418, -2.1626632 , -2.5300395 ,\n",
       "        -2.7232282 , -2.9154546 , -2.2206202 , -2.1776798 , -2.0686636 ,\n",
       "        -2.7118142 , -2.9217448 , -2.8446975 , -1.7116281 , -2.7521422 ,\n",
       "        -1.915301  , -2.702168  , -2.2993932 , -2.6029973 , -2.5585344 ,\n",
       "        -2.1853278 , -1.8066901 , -2.9620786 , -3.3031223 , -2.7580304 ,\n",
       "        -2.3031826 , -3.1330724 , -0.5533272 ,  0.21602972, -0.15833962,\n",
       "        -0.23312077,  0.08176294, -0.2285238 , -0.6759831 , -0.52709734,\n",
       "         0.4234749 , -0.5764201 , -0.21975905, -0.6222222 , -0.0674949 ,\n",
       "        -1.0217829 , -0.07844349,  0.12485982,  1.4708238 , -0.3236627 ,\n",
       "        -0.48177046, -1.0420464 ,  3.3803985 , -0.12891518, -0.34967077,\n",
       "         0.07469232,  0.04114959,  0.04295777,  0.03869623,  0.04051344,\n",
       "         0.04188024,  0.04129213,  0.04511457,  0.0421216 ,  0.04548447,\n",
       "         0.04718192,  0.03958783,  0.04277795,  0.04513744,  0.04326933,\n",
       "         0.03903662,  0.04365135,  0.04193537,  0.04615851,  0.03952274,\n",
       "         0.03828096,  0.03351332,  0.03746753,  0.03971916,  0.0435499 ],\n",
       "       dtype=float32),\n",
       " np.float64(0.0),\n",
       " False,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 3358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_env.step(norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.97150946 0.56987303 0.0306825  0.11110437 0.52139443 0.75964534\n",
      " 0.13051689 0.66843003 0.44473985 0.75057244 0.7459935  0.91527474\n",
      " 0.39351064 0.6792741  0.09107763 0.8653203  0.94605327 0.12145942\n",
      " 0.56719285 0.10465443 0.6602806  0.5610862  0.2229901  0.43685794]\n"
     ]
    }
   ],
   "source": [
    "actions, _ = model.predict(obs, deterministic=False)\n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.07918089 0.04644633 0.00250071 0.00905533 0.04249519 0.06191334\n",
      " 0.01063751 0.05447902 0.03624761 0.06117387 0.06080067 0.0745976\n",
      " 0.03207228 0.05536284 0.0074231  0.07052616 0.07710613 0.0098993\n",
      " 0.04622789 0.00852965 0.05381482 0.04573018 0.01817435 0.03560521]\n"
     ]
    }
   ],
   "source": [
    "norm = actions / np.sum(actions)\n",
    "print(norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 227 , Reward:  0.17515288000831494 weights:  [0.07918089 0.04644633 0.00250071 0.00905533 0.04249519 0.06191334\n",
      " 0.01063751 0.05447902 0.03624761 0.06117387 0.06080067 0.0745976\n",
      " 0.03207228 0.05536284 0.0074231  0.07052616 0.07710613 0.0098993\n",
      " 0.04622789 0.00852965 0.05381482 0.04573018 0.01817435 0.03560521]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([-3.0839932e-03, -2.0031747e-03,  7.4677187e-04,  0.0000000e+00,\n",
       "         7.5269174e-03,  1.3071865e-02,  2.4900958e-02, -8.3540916e-04,\n",
       "        -2.4725286e-02,  1.5789434e-02,  1.8151831e-02,  3.7735850e-03,\n",
       "         1.5576324e-02, -2.1097047e-02,  3.2921780e-03,  8.4745763e-03,\n",
       "         1.7888002e-02,  0.0000000e+00, -7.9364432e-03, -1.6077170e-02,\n",
       "        -2.1505391e-02,  0.0000000e+00, -1.1111111e-02, -2.9508185e-02,\n",
       "        -4.5030439e-01, -8.0497861e-01, -1.2996925e-01, -8.3653027e-01,\n",
       "         4.9206063e-01, -2.6468271e-02,  6.2180907e-01, -6.4603347e-01,\n",
       "        -6.6155171e-01,  7.3009521e-01, -7.9411703e-01,  3.0352041e-01,\n",
       "        -9.3837790e-03, -8.3645827e-01,  2.1069255e+00,  1.6749818e-02,\n",
       "        -7.0310152e-01, -1.1653206e+00,  1.4086002e+00, -1.6458750e+00,\n",
       "         9.1809922e-01,  1.8464211e+00, -6.9617577e-02,  3.6842531e-01,\n",
       "        -1.1970291e+00, -7.9331511e-01, -1.2791957e+00, -1.0912576e+00,\n",
       "        -4.8952281e-01, -6.0211456e-01, -5.6244129e-01, -6.9754112e-01,\n",
       "        -1.1329175e+00, -7.4386895e-01, -1.5347894e-02, -4.8739877e-01,\n",
       "        -5.5408204e-01,  2.8801417e-01, -4.1950893e-01, -6.2706971e-01,\n",
       "        -4.9596182e-01, -7.1751577e-01, -2.2288439e-01, -4.6037251e-01,\n",
       "        -1.4562558e-01,  1.2506180e+00, -9.5718515e-01,  2.4035351e-01,\n",
       "         1.7699978e-01, -1.9753182e-01, -1.0130227e+00, -4.5383239e-01,\n",
       "        -1.1236341e-01, -7.2524273e-01, -7.5408566e-01,  1.7021434e-01,\n",
       "         1.7720375e+00, -1.0152390e+00, -2.5485745e-01,  4.2593682e-01,\n",
       "        -4.1057509e-01, -3.2548177e-01, -2.2625233e-01,  3.0932283e-01,\n",
       "         6.6173691e-01, -3.2014269e-01, -7.7086020e-01, -9.5141649e-01,\n",
       "         1.1053932e+00, -3.2505140e-01, -7.6022077e-01, -6.6361934e-02,\n",
       "         7.9180896e-02,  4.6446338e-02,  2.5007150e-03,  9.0553351e-03,\n",
       "         4.2495191e-02,  6.1913341e-02,  1.0637512e-02,  5.4479025e-02,\n",
       "         3.6247615e-02,  6.1173875e-02,  6.0800675e-02,  7.4597605e-02,\n",
       "         3.2072283e-02,  5.5362847e-02,  7.4230959e-03,  7.0526168e-02,\n",
       "         7.7106141e-02,  9.8993024e-03,  4.6227895e-02,  8.5296463e-03,\n",
       "         5.3814821e-02,  4.5730185e-02,  1.8174354e-02,  3.5605215e-02],\n",
       "       dtype=float32),\n",
       " np.float64(0.17515288000831494),\n",
       " False,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 5262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_env.step(norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5263,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NewData:\n",
    "\n",
    "#     def __init__(self,\n",
    "#                  start_date:str,\n",
    "#                  end_date:str,\n",
    "#                  ticker_df: pd.DataFrame):\n",
    "#         self.start_date = start_date\n",
    "#         self.end_date = end_date\n",
    "#         self.ticker_df = ticker_df\n",
    "#         self.tickers = np.array(ticker_df.values.tolist()).flatten()\n",
    "#         self.master_date_range = pd.Series(np.repeat(pd.date_range(start=start_date, end=end_date, freq=\"B\"), 2), name=\"Date\")\n",
    "\n",
    "#         self.return_df: pd.DataFrame = None\n",
    "#         self.volume_df: pd.DataFrame = None\n",
    "#         self.rolling_vol_df: pd.DataFrame = None\n",
    "#         self.rolling_ret_df: pd.DataFrame = None\n",
    "    \n",
    "\n",
    "#     def z_score(self, data: pd.DataFrame):\n",
    "\n",
    "#         score_df = (data-data.mean()) / data.std()\n",
    "#         return score_df\n",
    "\n",
    "#     def download_data(self):\n",
    "#         ret_df = pd.DataFrame()\n",
    "#         vol_df = pd.DataFrame()\n",
    "\n",
    "#         for i in range(0,len(self.tickers),1):\n",
    "#             data = yf.download(self.tickers[i], start=self.start_date, end=self.end_date)\n",
    "#             open_merge_df = pd.merge(self.master_date_range, data[\"Open\"].copy(), on=\"Date\", how=\"left\")\n",
    "#             open_merge_df.loc[1::2, self.tickers[i]] = np.nan\n",
    "#             close_merge_df = pd.merge(self.master_date_range, data[\"Close\"].copy(), on=\"Date\", how=\"left\")\n",
    "#             close_merge_df.loc[::2, self.tickers[i]] = np.nan\n",
    "#             open_merge_df.loc[1::2, self.tickers[i]] = close_merge_df\n",
    "#             individual_df = open_merge_df.interpolate().ffill().bfill()\n",
    "#             vol_series = pd.merge(self.master_date_range, data[\"Volume\"], on=\"Date\", how=\"left\").interpolate().ffill().bfill()\n",
    "\n",
    "#             vol_df[self.tickers[i]] = vol_series[self.tickers[i]]\n",
    "#             ret_df[self.tickers[i]] = individual_df[self.tickers[i]]\n",
    "        \n",
    "#         self.return_df = ret_df.pct_change(1).bfill()\n",
    "#         self.volume_df = self.z_score(vol_df)\n",
    "#         self.rolling_ret_df = self.z_score(self.return_df.rolling(42).sum().fillna(0))\n",
    "#         self.rolling_vol_df = self.z_score(self.return_df.rolling(42).std().fillna(0))\n",
    "\n",
    "#         self.return_df.to_csv(\"Data/Input/StockReturns.csv\", index=False)\n",
    "#         self.volume_df.to_csv(\"Data/Input/Volume.csv\", index=False)\n",
    "#         self.rolling_ret_df.to_csv(\"Data/Input/RollingRet.csv\", index=False)\n",
    "#         self.rolling_vol_df.to_csv(\"Data/Input/RollingVol.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_data = NewData(\"2013-01-01\", \"2024-12-31\", ticker_df)\n",
    "# new_data.download_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5266,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, VecCheckNan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5267,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PorEnv(history_usage=80, rolling_reward_window=80, esg_data=esg_scores, objective=\"Sharpe\",  esg_compliancy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space shape: (120,)\n",
      "Actual obs shape: (120,)\n"
     ]
    }
   ],
   "source": [
    "myenv = DummyVecEnv([lambda: env])\n",
    "\n",
    "# Verify shapes\n",
    "print(\"Observation space shape:\", env.observation_space.shape)\n",
    "sample_obs, _ = env.reset()\n",
    "print(\"Actual obs shape:\", sample_obs.shape)  # Should match"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
